\chapter{Tools} \label{ch:tools}

Throughout this thesis, we will use a common collection of tools to help solve problems of interest.
Broadly speaking, we will make use of techniques from the fields of machine learning and signal processing.
In this chapter, we give a high-level overview of these fields and a more specific definition of the various techniques we will utilize.

\section{Machine Learning}

Machine learning is broadly defined as a suite of methods for enabling computers to carry out particular tasks without being explicitly programmed to do so.
Such methods make use of a collection of data, from which patterns or the desired behaviors are automatically determined.
In this sense, machine learning techniques allow computers to ``learn'' the correct procedure based on the data provided.
In addition to data, machine learning algorithms also often require an objective which quantifies the extent to which they are successfully carrying out a task.
This objective function allows them to be optimized in order to maximize their performance.

Traditionally, machine learning techniques are divided into three types: Supervised, unsupervised, and reinforcement learning.
Supervised learning requires a collection of training data that specifies both the input to the algorithm and the desired output.
Unsupervised learning, on the other hand, does not utilize target output data, and thus is limited to finding structure in the input data.
Finally, reinforcement learning refers to the broader problem of determining a policy to respond to input data in a dynamic ``environment'' based on a potentially rare signal which tells the algorithm whether it is successful or not.
All of the problems in this thesis fall into the category of supervised learing, so we will not discuss unsupervised or reinforcement learning further.

\subsection{Supervised Learning}

In the supervised setting, our machine learning algorithm produces a function $f$ (called a {\em model}) which, given input $x$ and the function's parameters $\theta$, produces an output $\hat{y}$:

\begin{equation}
\hat{y} = f(x, \theta)
\end{equation}

Supervised learning requires a training set $\mathcal{S}$ of input-target pairs $(x_1, y_1), (x_2, y_2), \ldots (x_{|\mathcal{S}|}, y_{|\mathcal{S}|})$, where $x_n \in \mathcal{X}$ and $y_n \in \mathcal{Y}$.
We assume that these pairs are drawn from an unknown joint probability distribution $p(x, y)$, of which the only information we explicitly know is the training set $\mathcal{S}$.
The goal of a supervised learning algorithm is to capture the probability distribution $p(x, y)$ with $f$ by adjusting the parameters $\theta$ based solely on $\mathcal{S}$.

One way to formulate this goal is to state that for a sample from $p(x, y)$ which is not in $\mathcal{S}$, the function $f$ should produce the correct output.
Of course, because we are only given the samples in $\mathcal{S}$, the parameters $\theta$ must be optimized solely according the training pairs in $\mathcal{S}$.
This approach, where $f$ is used directly to produce the predicted value for $y$, is referred to as discriminant training.
An alternative approach, called probabilistic training, is to use $f$ to model $p(x, y)$ by estimating $p(y | x)$ and then choosing the most probable $y$.
In either case, choosing $f$ typically involves using a task-specific ``loss function'' $L$ which computes a nonnegative scalar value which measures the extent to which $\hat{y}$ agress with $y$.
Ideally, minimizing this loss over the training set by adjusting $\theta$ will yield an $f$ which closely approximates $p(x, y)$.

The choice of a loss function will depend on the characteristics of the problem, in particular the target space $\mathcal{Y}$.
For example, in {\em classification} problems, the targets $y$ may take one of $K$ discrete values.
A natural loss function in this case is the zero-one loss

\begin{equation}
L(y, \hat{y}) = \begin{cases}
1,& \hat{y} = y\\
0,& \hat{y} \ne y
\end{cases}
\end{equation}

In {\em regression} problems, the targets are continuously valued, and a common loss function is the squared error

\begin{equation}
L(y, \hat{y}) = (y - \hat{y})^2
\end{equation}

Note that in both cases when $y = \hat{y}$, $L = 0$; this is a common property of loss functions.

In general, a suitable goal for training an unsupervised learning algorithm is to adjust $\theta$ to minimize the average loss over the training set:

\begin{equation}
\frac{1}{|\mathcal{S}|} \sum_{(x, y) \in \mathcal{S}} L(y, f(x, \theta))
\end{equation}

Minimizing this average loss is equivalent to minimizing the empirical risk of $f$.
A common paradigm is to compute the derivative of the loss function $L$ with respect to the model parameters $\theta$ and use this derivative information to incrementally change $\theta$ to make $L$ smaller.
In this case, it is beneficial to have a loss function which is continuous and smooth, which precludes the use of loss functions like the zero-one loss.
A common solution to this problem is to use a surrogate loss function which has the desired properties (e.g.\ smoothness) and whose minimization will also minimize $L$.

There are supervised learning methods which do not follow the above framework.
For example, the k-nearest neighbors algorithm is a classification algorithm which labels its input by assigning it the most common $y$ among the $k$ points in $\mathcal{S}$ which are closest to the input.
This classifier has no loss function or indeed any parameters other than $k$ and $\mathcal{S}$ itself (i.e.\ it is nonparametric).
However, all of the methods we will utilize in this thesis follow the recipe of defining a model with parameters $\theta$ which are adjusted by miniimzing the average loss over $\mathcal{S}$.

\subsection{Sequential Data}

A common scenario is that the data space $\mathcal{X}$ is fixed-dimensional, e.g.\ $\mathcal{X} \in \mathbb{R}^{D_1 \times D_2 \times \cdots D_N}$.
However, in some cases, one (or more) dimensions may vary in size.
An important example of this is sequential data, where a ``temporal'' dimension can differ between examples.
In addition, for real-world data the temporal dimension often contains strong dependencies and correlations.
For some examples of common types of data typically represented as sequences, see \cref{fig:example_sequences}.

An illustrative example is text data where each datapoint is a sentence.
Clearly, the length of each sequence will vary (e.g.\ ``I am happy.'' and ``This sentence is a few words longer.'' differ in length).
In addition, there may be a strong dependency between a given word in a sentence and the words that precede it.
For example, ``I am hungry, please give me'' is much more likely to be followed by the word ``food'' than, say, ``dirt''.

Different possible tasks involving sequential data can be broadly divided into three categories: Sequence classification, embedding, and transduction.
Sequence classification involves applying a single label to entire sequences; for example, classifying which of a finite number of people was speaking in an audio recording of an utterance.
Sequence embedding involves representing sequences as a single point in a fixed-dimensional space with some desired properties.
For example, instead of classifying which person was speaking, we might instead embed audio recordings as vectors in a Euclidean space such that recordings of the same speaker have a small embedded distance and recordings of different speakers have a large distance.
Finally, sequence transduction involves converting an input sequence to a new sequence, potentially of different length and whose elements may lie in a different space.
Speech recognition is an exemplary case, where a sequence of discrete words must be generated based on an audio recording.

\subsection{Neural Networks}

Most of the machine learning algorithms used in this thesis will come from a class of models known as ``neural netorks''.
Neural networks can be considered learnable functions which consist of a sequence of nonlinear processing stages.
First proposed in the XXXXs, these models were designed as an artificial model of the way the brain was understood to work at that time.
The modern understanding of the brain has diverged substantially from this model; nevertheless, the name ``neural network'' has endured.

A predecessor to modern neural network models was the perceptron algorithm.
The perceptron is a simple linear model which can perform binary classification of its input.
Given an input feature vector $x \in \mathbb{R}^D$, the perceptron computes

\begin{equation}
f(x) = \begin{cases}
1,& w^\top x + b > 0\\
0,& \mathrm{otherwise}
\end{cases}
\end{equation}

where $w \in \mathbb{R}^D$ and $b \in \mathbb{R}$ are the parameters of the model.
These parameters are adjusted so that the desired label ($1$ or $0$) is produced given different inputs.

The perceptron algorithm can only correctly classify its input when the data which falls into the two classes is linearly separable by class.
To remedy this, the ``multilayer perceptron'' was proposed, which can be viewed as a sequence of perceptrons organized in ``layers'', each taking its input from the previous perceptron.
In the multilayer perceptron, each perceptron can output a vector of values rather a single value.
Furthermore, rather than producing a binary 1 or 0 depending on the sign of $w^\top x + b$, the layers in a multilayer perceptron may use any of a variety of nonlinear functions.
In summary, a given layer's output is computed as
\begin{equation}
f(x) = \sigma(W x + b)
\end{equation}
where $\sigma$ is a nonlinear ``activation'' function, $W \in \mathbb{R}^{M \times D}$ is the weight matrix, $b \in \mathbb{R}^M$ is the bias vector, and $M$ is the output dimensionality of the layer.
The original perceptron is recovered when $\sigma$ is the Heaviside step function and $M = 1$.
Under this generic definition, multilayer perceptrons are, in fact, equivalent to what are now called feedforward networks; we will use this terminology for the remainder of this thesis.

In a seminal paper, it was shown that feedforward networks with at least two layers are {\em universal function approximators} when the activation function of each layer is a ``squashing'' function (i.e.\ is monotonically increasing, $\lim_{x \rightarrow \infty} \sigma(x) = 1$ and $\lim_{x \rightarrow -\infty} \sigma(x) = 0$).
This implies that such models can approximate any function to arbitrary precision.
Similar results were later demonstrated for other classes of activation functions.

Of course, the ability of a model to approximate any function is only valuable when there is a reliable method to adjust its parameters so that the desired function is suitably approximated.
Currently, the most pervasive method to achieve this is ``backpropagation'', which computes the gradient of an error function which measures the deviation between the network's output and the target output with respect to the model parameters.
We give a derivation of backpropagation for feedforward networks in the following section.
In the supervised learning context, the error function computes the deviation between the network's output and the target for input-target pairs in the training set $\mathcal{S}$.
As a result, for supervised learning, universal approximation only implies that the multilayer feedforward network can ``memorize'' the target output for each input in $\mathcal{S}$, not that it will accurately approximate the data's underlying joint probability distribution $p(x, y)$.
This is referred to as ``overfitting'', and is an undesirable property of machine learning models.

Despite this issue, feedforward networks have proven to be extremely effective models in a variety of machine learning tasks.
This success is thanks to a suite of methods and model designs which allow these models to be trained effectively, over potentially very large datasets, while avoiding overfitting.
It has also led to the development of variants which are more appropriate for sequential data (recurrent networks) and data with local regularities (convolutional networks).
In the following sections, we give an overview of these different developments and approaches.

\subsubsection{Backpropagation}

\subsection{Tricks}

\subsubsection{Recurrent Networks}

\subsubsection{Convolutional Networks}

\subsubsection{Attention}

\subsection{Stochastic Optimization}

\subsection{Bayesian Optimization}

\section{Signal Processing}

\subsection{Audio Signals and Psychoacoustics}

proposal1

\subsection{Time-Frequency Analysis}

\subsection{Dynamic Time Warping}

ismir2015large2, icassp2016optimizing2

\subsubsection{Pruning Methods}

icassp2016pruning1
