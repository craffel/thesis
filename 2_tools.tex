\chapter{Tools} \label{ch:tools}

Throughout this thesis, we will use a common collection of tools to help solve problems of interest.
Broadly speaking, we will make use of techniques from the fields of machine learning and signal processing.
In this chapter, we give a high-level overview of these fields and a more specific definition of the various techniques we will utilize.

\section{Machine Learning}

Machine learning is broadly defined as a suite of methods for enabling computers to carry out particular tasks without being explicitly programmed to do so.
Such methods make use of a collection of data, from which patterns or the desired behaviors are automatically determined.
In this sense, machine learning techniques allow computers to ``learn'' the correct procedure based on the data provided.
In addition to data, machine learning algorithms also often require an objective which quantifies the extent to which they are successfully carrying out a task.
This objective function allows them to be optimized in order to maximize their performance.

Traditionally, machine learning techniques are divided into three types: Supervised, unsupervised, and reinforcement learning.
Supervised learning requires a collection of training data that specifies both the input to the algorithm and the desired output.
Unsupervised learning, on the other hand, does not utilize target output data, and thus is limited to finding structure in the input data.
Finally, reinforcement learning refers to the broader problem of determining a policy to respond to input data in a dynamic ``environment'' based on a potentially rare signal which tells the algorithm whether it is successful or not.
All of the problems in this thesis fall into the category of supervised learing, so we will not discuss unsupervised or reinforcement learning further.

\subsection{Supervised Learning}

In the supervised setting, our machine learning algorithm produces a function $f$ (called a {\em model}) which, given input $x$ and the function's parameters $\theta$, produces an output $\hat{y}$:

\begin{equation}
\hat{y} = f(x, \theta)
\end{equation}

Supervised learning requires a training set $\mathcal{S}$ of input-target pairs $(x_1, y_1), (x_2, y_2), \ldots (x_{|\mathcal{S}|}, y_{|\mathcal{S}|})$, where $x_n \in \mathcal{X}$ and $y_n \in \mathcal{Y}$.
We assume that these pairs are drawn from an unknown joint probability distribution $p(x, y)$, of which the only information we explicitly know is the training set $\mathcal{S}$.
The goal of a supervised learning algorithm is to capture the probability distribution $p(x, y)$ with $f$ by adjusting the parameters $\theta$ based solely on $\mathcal{S}$.

One way to formulate this goal is to state that for a sample from $p(x, y)$ which is not in $\mathcal{S}$, the function $f$ should produce the correct output.
Of course, because we are only given the samples in $\mathcal{S}$, the parameters $\theta$ must be optimized solely according the training pairs in $\mathcal{S}$.
This approach, where $f$ is used directly to produce the predicted value for $y$, is referred to as discriminant training.
An alternative approach, called probabilistic training, is to use $f$ to model $p(x, y)$ by estimating $p(y | x)$ and then choosing the most probable $y$.
In either case, choosing $f$ typically involves using a task-specific ``loss function'' $\mathcal{L}$ which computes a nonnegative scalar value which measures the extent to which $\hat{y}$ agress with $y$.
Ideally, minimizing this loss over the training set by adjusting $\theta$ will yield an $f$ which closely approximates $p(x, y)$.

The choice of a loss function will depend on the characteristics of the problem, in particular the target space $\mathcal{Y}$.
For example, in {\em classification} problems, the targets $y$ may take one of $K$ discrete values.
A natural loss function in this case is the zero-one loss

\begin{equation}
\mathcal{L}(y, \hat{y}) = \begin{cases}
1,& \hat{y} = y\\
0,& \hat{y} \ne y
\end{cases}
\end{equation}

In {\em regression} problems, the targets are continuously valued, and a common loss function is the squared error

\begin{equation}
\mathcal{L}(y, \hat{y}) = (y - \hat{y})^2
\end{equation}

Note that in both cases when $y = \hat{y}$, $\mathcal{L} = 0$; this is a common property of loss functions.

In general, a suitable goal for training an unsupervised learning algorithm is to adjust $\theta$ to minimize the average loss over the training set:

\begin{equation}
\frac{1}{|\mathcal{S}|} \sum_{(x, y) \in \mathcal{S}} \mathcal{L}(y, f(x, \theta))
\end{equation}

Minimizing this average loss is equivalent to minimizing the empirical risk of $f$.
A common paradigm is to compute the derivative of the loss function $\mathcal{L}$ with respect to the model parameters $\theta$ and use this derivative information to incrementally change $\theta$ to make $\mathcal{L}$ smaller.
In this case, it is beneficial to have a loss function which is continuous and smooth, which precludes the use of loss functions like the zero-one loss.
A common solution to this problem is to use a surrogate loss function which has the desired properties (e.g.\ smoothness) and whose minimization will also minimize $\mathcal{L}$.

There are supervised learning methods which do not follow the above framework.
For example, k-nearest neighbors \cite{cover1967nearest} is a classification algorithm which labels its input by assigning it the most common $y$ among the $k$ points in $\mathcal{S}$ which are closest to the input.
This classifier has no loss function or indeed any parameters other than $k$ and $\mathcal{S}$ itself (i.e.\ it is nonparametric).
However, all of the methods we will utilize in this thesis follow the recipe of defining a model with parameters $\theta$ which are adjusted by minimizing the average loss over $\mathcal{S}$.

\subsection{Sequential Data}

A common scenario is that the data space $\mathcal{X}$ is fixed-dimensional, e.g.\ $\mathcal{X} \in \mathbb{R}^{D_1 \times D_2 \times \cdots D_N}$.
However, in some cases, one (or more) dimensions may vary in size.
An important example of this is sequential data, where a ``temporal'' dimension can differ between examples.
In addition, for real-world data the temporal dimension often contains strong dependencies and correlations.
For some examples of common types of data typically represented as sequences, see \cref{fig:example_sequences}.

An illustrative example is text data where each datapoint is a sentence.
Clearly, the length of each sequence will vary (e.g.\ ``I am happy.'' and ``This sentence is a few words longer.'' differ in length).
In addition, there may be a strong dependency between a given word in a sentence and the words that precede it.
For example, ``I am hungry, please give me'' is much more likely to be followed by the word ``food'' than, say, ``dirt''.

Different possible tasks involving sequential data can be broadly divided into three categories: Sequence classification, embedding, and transduction.
Sequence classification involves applying a single label to entire sequences; for example, classifying which of a finite number of people was speaking in an audio recording of an utterance.
Sequence embedding involves representing sequences as a single point in a fixed-dimensional space with some desired properties.
For example, instead of classifying which person was speaking, we might instead embed audio recordings as vectors in a Euclidean space such that recordings of the same speaker have a small embedded distance and recordings of different speakers have a large distance.
Finally, sequence transduction involves converting an input sequence to a new sequence, potentially of different length and whose elements may lie in a different space.
Speech recognition is an exemplary case, where a sequence of discrete words must be generated based on an audio recording.

\subsection{Neural Networks}

Most of the machine learning algorithms used in this thesis will come from a class of models known as ``neural netorks''.
Neural networks can be considered learnable functions which consist of a sequence of nonlinear processing stages.
First proposed in the 1940s \cite{mcculloch1943logical,hebb1949organization}, these models were designed as an artificial model of the way the brain was understood to work at that time.
The modern understanding of the brain has diverged substantially from this model; nevertheless, the name ``neural network'' has endured.

A predecessor to modern neural network models was the perceptron algorithm \cite{rosenblatt1958perceptron}.
The perceptron is a simple linear model which can perform binary classification of its input.
Given an input feature vector $x \in \mathbb{R}^D$, the perceptron computes
\begin{equation}
f(x) = \begin{cases}
1,& w^\top x + b > 0\\
0,& \mathrm{otherwise}
\end{cases}
\label{eq:perceptron}
\end{equation}
where $w \in \mathbb{R}^D$ and $b \in \mathbb{R}$ are the parameters of the model.
These parameters are adjusted so that the desired label ($1$ or $0$) is produced given different inputs.
A common visualization of the perceptron is shown in \cref{fig:perceptron}.

\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{figures/2-perceptron.pdf}
  \caption[Visualization of a perceptron]{Diagram visualizing the perceptron of \cref{eq:perceptron}.
  Each entry of $x \in \mathbb{R}^3$ is multiplied by an entry of the weight vector $w$, and the result is summed along with the bias $b$ before being passed into a Heaviside step function.}
  \label{fig:perceptron}
\end{figure}

The perceptron algorithm can only correctly classify its input when the data is linearly separable by class.
To remedy this, the ``multilayer perceptron'' was proposed, which can be viewed as a sequence of perceptrons organized in ``layers'', each taking its input from the previous perceptron.
In the multilayer perceptron, each perceptron can output a vector of values rather a single value.
Furthermore, rather than producing a binary 1 or 0 depending on the sign of $w^\top x + b$, the layers in a multilayer perceptron may use any of a variety of nonlinear functions.
In summary, a given layer's output is computed as
\begin{equation}
        a = f(x) = \sigma(W x + b)
\end{equation}
where $\sigma$ is a nonlinear ``activation'' function, $x \in \mathbb{D}$ is the layer's input, $W \in \mathbb{R}^{M \times D}$ is the weight matrix, $b \in \mathbb{R}^M$ is the bias vector, $a \in \mathbb{R}^M$ is the layer's output, and $M$ is the output dimensionality of the layer.
The individual entries of $a$ are referred to as the layer's ``units''.
The original perceptron is recovered when $\sigma$ is the Heaviside step function and $M = 1$.
Under this generic definition, multilayer perceptrons are, in fact, equivalent to what are now called feedforward networks; we will use this terminology for the remainder of this thesis.
\Cref{fig:network} shows a common schematic used to represent feedforward networks.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-network.pdf}
  \caption[Schematic of a feedforward network]{Schematic of a feedforward network (realized as a belt buckle).
  Each column of nodes represents the number of units in each layer; this particular diagram shows two input units, a single hidden layer with three units, and a single output unit.
  Lines indicate connections between units in each layer, i.e.\ entries of each weight matrix.
  Aside from those in the input layer, each node indicates summing of its input, adding a bias, and applying a nonlinearity.}
  \label{fig:network}
\end{figure}

In parallel seminal works \cite{cybenko1989approximation, hornik1989multilayer}, it was shown that feedforward networks with at least two layers are {\em universal function approximators} when the activation function of each layer is a ``squashing'' function (i.e.\ is monotonically increasing, $\lim_{x \rightarrow \infty} \sigma(x) = 1$ and $\lim_{x \rightarrow -\infty} \sigma(x) = 0$).
This implies that such models can approximate any function to arbitrary precision.
Similar results were later demonstrated for other classes of activation functions, typically utilizing the Stone-Weierstrass theorem.

Of course, the ability of a model to approximate any function is only valuable when there is a reliable method to adjust its parameters so that the desired function is suitably approximated.
Currently, the most pervasive method to achieve this is ``backpropagation'', which computes the gradient of an error function which measures the deviation between the network's output and the target output with respect to the model parameters \cite{rumelhart1986learning}.
We give a derivation of backpropagation for feedforward networks in the following section.
In the supervised learning context, the error function computes the deviation between the network's output and the target for input-target pairs in the training set $\mathcal{S}$.
As a result, for supervised learning, universal approximation only implies that the multilayer feedforward network can ``memorize'' the target output for each input in $\mathcal{S}$, not that it will accurately approximate the data's underlying joint probability distribution $p(x, y)$.
This is referred to as ``overfitting'', and is an undesirable property of machine learning models.

Despite this issue, feedforward networks have proven to be extremely effective models in a variety of machine learning tasks.
This success is thanks to a suite of methods and model designs which allow these models to be trained effectively, over potentially very large datasets, while avoiding overfitting.
It has also led to the development of variants which are more appropriate for sequential data (recurrent networks) and data with local regularities (convolutional networks).
In the following sections, we give an overview of these different developments and approaches.

\subsubsection{Backpropagation}

Backpropagation is a method for efficiently computing the gradient of a loss function applied to a neural network with respect to its parameters.
These partial derivatives can then be used to update the network's parameters using gradient descent methods.
In truth, backpropagation is a special case of the more generic ``reverse-mode automatic differentiation'' method.
Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.

While originally introduced earlier, the wide-spread adoption of backpropagation for training neural networks is largely thanks to Rumelhart et.\ al's seminal paper \cite{rumelhart1986learning}.
Backpropagation is so successful and widely applied that in some sense the term ``neural network'' has become genericized to mean ``a machine learning model for which backpropagation is applicable''.
Because it is fundamental in many of the approaches proposed in this thesis, we give a full derivation of backpropagation for a simple multilayer feedforward network in this section.
A similar, higher-level and more verbose derivation can be found at \cite{nielsen2015neural}.

By way of review, the chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.
If $\mathcal{L}$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial \mathcal{L}}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
For scalar-valued functions of more than one variable (e.g.\ a scalar-valued function of a vector), the chain rule essentially becomes additive.
In other words, if $L$ is a scalar-valued function of a vector $z \in \mathbb{R}^N$, whose entries are each a function of some variable $w$, the chain rule states that
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w} = \sum_{i = 1}^N \frac{\partial \mathcal{L}}{\partial z[i]}\frac{\partial z[i]}{\partial w}
\end{equation}

In the following, we'll discuss backpropagation through an $L$-layer feedforward network, where $N_n$ is dimensionality of layer $n \in \{0, \ldots, L\}$.
$N_0$ is the dimensionality of the input; $N_L$ is the dimensionality of the output.
The $m^{th}$ layer, for $m \in \{1, \ldots, L\}$ has parameters $W_m \in \mathbb{R}^{N_m \times N_{m - 1}}$, the weight matrix, and $b_m \in \mathbb{R}^{N_m}$, the bias vector.
$W_m[i, j]$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.
Each layer computes $z_m \in \mathbb{R}^{N_m}$, a linear mix of its inputs, by $z_m = W_m a_{m - 1} + b_m$, and the activation $a_m = \sigma_m(z_m)$ where $\sigma_m$ is the layer's nonlinear activation function.
$a_L$ is the output of the network.
We define the special case $a_0$ as the input of the network.
We denote $y \in \mathbb{R}^{N_L}$ as the target output, treated as a constant.
The loss function $\mathcal{L}(a_L, y)$ is used to measure the error of the network's output compared to the target.

In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the loss function $\mathcal{L}$; that is, we need to know $\partial \mathcal{L}/\partial W_m$ and $\partial \mathcal{L}/\partial b_m$.
It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture: $\partial \mathcal{L}/\partial a_L$, the derivative of the loss function with respect to the output of the network, and $\partial a_m/\partial z_m$, the derivative of the nonlinearity used in layer $m$ with respect to its argument $z_m$.

To compute the gradient of our loss function $\mathcal{L}$ with respect to $W_m[i, j]$ (a single entry in the weight matrix of the layer $m$), we can first note that $\mathcal{L}$ is a function of $a_L$, which is itself a function of the linear mix output entries $z_m[k]$, which are themselves functions of the weight matrices $W_m$ and biases $b_m$.
With this in mind, we can use the chain rule as follows:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m[i, j]} = \sum_{k = 1}^{N_m} \frac{\partial \mathcal{L}}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial W_m[i, j]}
\label{eq:L_wrt_W_m}
\end{equation}
Note that by definition
\begin{equation}
z_m[k] = \sum_{l = 1}^{N_m} W_m[k, l] a_{m - 1}[l] + b_m[k]
\end{equation}
It follows that $\partial z_m[k]/\partial W_m[i, j]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any elements in $W_m$ except for those in the $k^{th}$ row, and we are only considering the entry $W_m[i, j]$.
When $k = i$, we have
\begin{align}
\frac{\partial z_m[i]}{\partial W_m[i, j]} &= \frac{\partial}{\partial W_m[i, j]}\left(\sum_{l = 1}^{N_m} W_m[i, l] a_{m - 1}[l] + b_m[i]\right)\\
                                           &= a_{m - 1}[j]\\
\rightarrow \frac{\partial z_m[k]}{\partial W_m[i, j]} &= \begin{cases}
0 & k \ne i\\
a_{m - 1}[j] & k = i
\end{cases}
\label{eq:z_m_wrt_W_m}
\end{align}

Combining \cref{eq:z_m_wrt_W_m} with \cref{eq:L_wrt_W_m} causes the summation to collapse, giving
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m[i, j]} = \frac{\partial \mathcal{L}}{\partial z_m[i]} a_{m - 1}[j]
\end{equation}
or in vector form
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m} = \frac{\partial \mathcal{L}}{\partial z_m} a_{m - 1}^\top
\end{equation}

Similarly for the bias variables $b_m$, we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m[i]} = \sum_{k = 1}^{N_m} \frac{\partial \mathcal{L}}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial b_m[i]}
\label{eq:L_wrt_b_m}
\end{equation}

As above, it follows that $\partial z_m[k]/\partial b_m[i]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any element in $b_m$ except $b_m[k]$.  When $k = i$, we have

\begin{align}
\frac{\partial z_m[i]}{\partial b_m[i]} &= \frac{\partial}{\partial b_m[i]}\left(\sum_{l = 1}^{N_m} W_m[il] a_{m - 1}[l] + b_m[i]\right)\\
                                        &= 1\\
\rightarrow \frac{\partial z_m[k]}{\partial b_m[i]} &= \begin{cases}
0 & k \ne i\\
1 & k = i
\end{cases}
\label{eq:z_m_wrt_b_m}
\end{align}

Combining \cref{eq:z_m_wrt_b_m} and \cref{eq:L_wrt_b_m} again causes the summation to collapse to give
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m[i]} = \frac{\partial \mathcal{L}}{\partial z_m[i]}
\end{equation}

or in vector form
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m} = \frac{\partial \mathcal{L}}{\partial z_m}
\end{equation}

Now, we must compute $\partial \mathcal{L}/\partial z_m[k]$.
For the final layer ($m = L$), this term is straightforward to compute using the chain rule:

\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_L[k]} = \frac{\partial \mathcal{L}}{\partial a_L[k]} \frac{\partial a_L[k]}{\partial z_L[k]}
\end{equation}

or, in vector form

\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_L} = \frac{\partial \mathcal{L}}{\partial a_L} \frac{\partial a_L}{\partial z_L}
\end{equation}

The first term $\partial \mathcal{L}/\partial a_L$ is just the derivative of the loss function with respect to its argument, whose form depends on the loss function chosen.
Similarly, $\partial a_m/\partial z_m$ (for any layer $m$ includling $L$) is the derivative of layer $m$'s nonlinearity with respect to its argument and will depend on the choice of nonlinearity.
For other layers, we again invoke the chain rule:

\begin{align}
        \frac{\partial \mathcal{L}}{\partial z_m[k]} &= \frac{\partial \mathcal{L}}{\partial a_m[k]} \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\frac{\partial z_{m + 1}[l]}{\partial a_m[k]}\right)\frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\frac{\partial}{\partial a_m[k]} \left(\sum_{h = 1}^{N_m} W_{m + 1}[l, h] a_m[h] + b_{m + 1}[l]\right)\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]} W_{m + 1}[l, k]\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}W_{m + 1}^\top[k, l] \frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
\end{align}

where the last simplification was made because by convention $\partial \mathcal{L}/\partial z_{m + 1}$ is a column vector, allowing us to write the following vector form:
\begin{align}
        \frac{\partial \mathcal{L}}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial \mathcal{L}}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\label{eq:L_wrt_z_m}
\end{align}
where $\circ$ is the element-wise (Hadamard) product.

We now have the ingredients to efficiently compute the gradient of the loss function with respect to the network's parameters: First, we compute $\partial \mathcal{L}/\partial z_L$ based on the choice of loss function and $\partial a_m/\partial z_m$ based on each layer's nonlinearity.
Then, we recursively can compute $\partial \mathcal{L}/\partial z_m$ layer-by-layer based on the term $\partial \mathcal{L}/\partial z_{m + 1}$ computed from the previous layer (this is called the ``backward pass'').
Finally, the resulting $\partial \mathcal{L}/\partial z_m$ terms are then used to compute $\partial \mathcal{L}/\partial W_m$ and $\partial \mathcal{L}/\partial b_m$ using \cref{eq:L_wrt_W_m} and \cref{eq:L_wrt_b_m} respectively.
Using this recipe requires $\partial \mathcal{L}/\partial z_L$ and $\partial a_m/\partial z_m$ to be known ahead of time.
By way of example, \cref{tab:nonlinearities} and \cref{tab:loss_functions} list some common loss functions and nonlinearities and their appropriate derivatives.

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Nonlinearity & $a_m$                                            & $\frac{\partial a_m}{\partial z_m}$                   \\
  \midrule
  Logistic     & $\dfrac{1}{1 + e^{z_m}}$                         & $a_m(1 - a_m)$                                        \\[1.2ex]
  $\tanh$      & $\dfrac{e^{z_m} - e^{-z_m}}{e^{z_m} + e^{-z_m}}$ & $1 - (a_m)^2$                                         \\[1.2ex]
  Rectifier    & $\max(0, z_m)$                                   & $\begin{cases}0,& z_m < 0\\ 1,& z_m \ge 0\end{cases}$ \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common nonlinearities used in neural networks]{Common nonlinearities $\sigma_m(z_m)$ used in neural networks and their derivatives with respect to their input $z_m$.}
  \label{tab:nonlinearities}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Loss Function & $\mathcal{L}(a_L, y)$                           & $\frac{\partial \mathcal{L}}{\partial a^L}$ \\
  \midrule
  Squared Error & $\dfrac{1}{2}(y - a^L)^\top(y - a^L)$ & $y - a^L$                         \\
  Cross-Entropy & $(y - 1)\log(1 - a^L) - y\log(a^L)$   & $\dfrac{a^L - y}{a^L(1 - a^L)}$   \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common loss functions used in neural networks]{Common loss (loss) functions $\mathcal{L}(a_L, y)$ used in neural networks and their derivatives with respect to their input $a_L$.}
  \label{tab:loss_functions}
\end{table}

In summary, backpropagation proceeds in the following manner for each training sample: For the ``forward pass'', given the network input $a_0$, compute $a_m$ for $m \in \{1, \ldots, L\}$ recursively by $a_m = \sigma_m(W_m a_{m - 1} + b_m)$.
For the ''backward pass'', compute
\begin{equation}
\frac{\partial \mathcal{L}}{\partial z_L} = \frac{\partial \mathcal{L}}{\partial a_L} \frac{\partial a_L}{\partial z_L}
\end{equation}
whose terms are all known a priori, then recursively compute
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial \mathcal{L}}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\end{equation}
and use the resulting values to compute
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial W_m} = \frac{\partial \mathcal{L}}{\partial z_m} a_{m - 1}^\top
\end{equation}
and
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial b_m} = \frac{\partial \mathcal{L}}{\partial z_m}
\end{equation}

From this procedure, we can see why backpropagation is an efficient way to compute these partial derivatives: By reusing terms, all of the partial derivatives can be computed by a single forward and backward pass.
For a feedforward network, the computational loss of each of these passes is effectively $L$ matrix multiplications.
A simple alternative would be to measure the change in $\mathcal{L}$ by adjusting each of the parameters (i.e.\ individual elements of the weight matrices and bias vectors) by a small amount.
However, this would require a separate forward pass for each parameter.
Modern neural networks contain millions of parameters, so clearly backpropagation is an efficient choice.

\subsubsection{Tricks}
\label{sec:tricks}

Combined with a gradient descent technique, backpropagation provides an effective method for optimizing the parameters of a neural network (``training'') to minimize a given loss function.
However, training neural networks is made difficult in practice by a variety of factors.
First, the composition of the many nonlinear processing layers in a neural network make the resulting loss function highly non-convex.
Nonconvexity means that, when minimizing the network's loss function with gradient descent methods, there is no guarantee that a given stationary point is a global minimum.
It also implies that both the way the parameters are initialized prior to optimization and the gradient descent technique used may have a very strong effect on the best solution found during optimization.

As mentioned above, the universal approximation property of neural networks can also have negative implications for their utility.
The fact that a possible outcome of training a neural network is the simple memorization of correspondences in the training set means that the resulting model may be of little use in practice.
As a result, in many cases it is necessary to regularize the network to prevent it from overfitting to the training set.

Finally, feedforward networks with many layers are also prone to the problem of ``vanishing and exploding gradients''.
This issue is caused by the fact that as gradients are backpropagated through layers, they are multiplied by each successive weight matrix and the derivative of each nonlinearity (as shown in \cref{eq:L_wrt_z_m}).
When the spectral norm of the weight matrices and/or the gradients of the nonlinearities are substantially greater than or less than one, the resulting backpropagated gradients can become exponentially smaller or larger as they approach ``early'' layers in the network.
This can effectively prevent the parameters of the first few layers from being adjusted, which can greatly inhibit learning.

Fortunately, there has been a great deal of development into methods for mitigating these issues.
Collectively, these techniques have been dubbed ``deep learning'' thanks to their applicability to networks with many layers.
In this section, we motivate and describe those techniques which are used in this thesis.
Practical and broader overviews can be found in \cite{bengio2012practical, lecun2012efficient, lecun2015deep}.

\paragraph{Number and Size of Layers}

When designing a neural network model, one of the first choices which must be made is the number of layers and their sizes (output dimensionality).
Because early theory proved that networks with a single layer could approximate any function arbitrarily well given that the layer was sufficiently ``wide'' (having many units) \cite{cybenko1989approximation,hornik1989multilayer}, and because networks with many layers can be more difficult to train due to the vanishing and exploding gradients problem \cite{glorot2010understanding,erhan2009difficulty}, early applications of neural networks typically only used a single hidden layer.
However, recent results have shown that using many layers can help networks learn a hierarchical transformation to more and more abstract representations which, for real-world problems, can be highly beneficial \cite{bengio2009learning}.
Coupled with recent advances in training very deep networks \cite{glorot2011deep,glorot2010understanding,bengio2007greedy,hinton2006fast,hinton2012improving,hinton2006reducing,sutskever2013importance,ioffe2015batch,he2015delving,ciresan2010deep}, the paradigm of ``deep'' networks (having many layers) has come to dominate.

Regardless of depth, the dimensionality of each intermediate representation (the ``layer sizes'') must be chosen.
While increasing the dimensionality may increase modeling power, it also exacerbates overfitting and incurs a greater computational loss.
However, Graphics Processing Units (GPUs) have recently been used to accelerate neural network models thanks to their ability to quickly perform large matrix multiplications.
As a result, in modern practice ``wide'' layers are used in combination with regularization techniques \cite{bengio2012practical}.

\paragraph{Nonlinearities}

A fundamental ingredient of neural networks is the elementwise nonlinearity applied at each layer; without them, the network would only be able to learn linear relationships between its input and output.
Early work typically used sigmoidal nonlinearities such as the logistic or hyperbolic tangent functions (see \cref{tab:nonlinearities}).
However, these nonlinearities have a very small gradient for much of their domain which can cause vanishing gradient problems particularly for networks with many layers.
Recently, the rectifier nonlinearity has become the standard for deep networks due to its computational efficiency and tendency to produce sparse representations \cite{jarrett2009best,glorot2011deep,nair2010rectified}.
The rectifier nonlinearity has zero gradient for half of its domain, which can hinder training in some cases; as a result a number of variants have recently been proposed which have been shown empirically to produce better results in some cases \cite{maas2013rectifier,he2015delving}.

\paragraph{Regularization}

Due to their tendency to overfit, some form of regularization is typically necessary to ensure that the optimized neural network's variance is not too high.
A common regularizer which is used in many machine learning models is to include in the loss function a penalty on the norm of the model's parameters.
In neural networks, adding the $L^2$ penalty $\lambda \sum_i \theta_i^2$ where $\lambda$ is a hyperparameter and $\theta_i$ are the model's parameters (e.g. individual entries of the weight matrices and bias vectors) is referred to as ``weight decay'' \cite{hanson1989comparing}.
This can encourge the weight matrix of a given layer from focusing too heavily (via a very large weight value) on a single unit of its input.
A related term is $\lambda \sum_i |\theta_i|$ which effectively encourages parameter values to be zero \cite{bengio2012practical}.

A completely different regularization method which has recently proven popular in neural networks is ``dropout'' \cite{hinton2012improving}.
In dropout, at each training iteration each unit of each layer is randomly set to zero with probability $p$.
After training, the weights in each layer are scaled by $1/p$.
Dropout intends to prevent the units of the output of a given layer from being too heavily dependent (correlated) with one of the inputs by randomly artificially removing connections.
More simply, it provides a source of noise which prevents memorization of correspondences in the training set and has repeatedly been shown empirically to be an effective regularizer.

In practice, the technique of ``early stopping'' is almost always used to avoid overfitting in neural network models \cite{prechelt2012early}.
To utilize early stopping, during training the performance on a held-out ``validation set'' (over which the parameters of the network are not optimized) is computed.
Overfitting is indicated by performance degrading on the validation set, which simulates real-world performance.
As a result, early stopping effectively prevents overfitting by simply stopping training once the performance begins to degrade.
The measure of performance and criteria for stopping my vary widely from task to task (and practitioner to practitioner) but the straightforwardness and effectiveness of this approach has led it to be nearly universally applied.

Of course, regularization can be rendered unnecessary when a huge amount of training data is available.
This is commonly simulated by applying hand-designed realistic deformations to the training data, which is referred to as ``data augmentation''.
For example, applying elastic distortions to images can allow simple, unregularized neural network models to match the performance of more complex architectures \cite{ciresan2010deep}.
Data augmentation has also been explored in the domain of audio and music signals \cite{mcfee2015software,schluter2015exploring}.

\paragraph{Parameter Initialization}

Because the objective functions used for training neural networks tend to be highly nonconvex, the way parameters are initialized prior to training can have a substantial effect on the solutions found after optimization \cite{sutskever2013importance}.
In fact, the recent sucess of neural network models was prompted by a new method for setting parameter values prior to supervised training called ``unsupervised pretraining''.
Unsupervised pretraining starts by training an unsupervised model (e.g.\ a Restricted Boltzmann Machine \cite{hinton2006reducing,hinton2006fast} or an autoencoder \cite{bengio2007greedy,erhan2009difficulty}) and then using the parameters from that model to initialize a supervised model.
Ideally, the unsupervised model will take care of disentangling factors of variation and producing a more efficient representation of the input space.
After initialization with values from the unsupervised model, training of the model proceeds in the normal supervised fashion.
This can greatly accelerate learning, and is of particular use when there is a great deal more unlabeled data available than labeled data.

It has more recently been shown that unsupervised pretraining can be skipped (provided that there is a sufficient amount of labeled data) by carefully initializing the network's parameters.
This typically involves sampling the values of the weight matrices from zero-mean Gaussian distributions whose variances are set so that the representation at each layer's output has unit variance.
The actual variances used for the random variables depends on the input and output dimensionality as well as the nonlinearity; \cite{glorot2010understanding} provides a derivation for linear networks, \cite{he2015delving} for rectifier networks and \cite{mishkin2015all} gives a generic empirical procedure for any network architecture.
An alternative is to use orthogonal matrices \cite{saxe2013exact} or to use a sparse initialization so that a fixed number of entries are non-zero \cite{sutskever2013importance}.


\paragraph{Input Normalization}

Most of the analysis performed on neural networks in order to determine how to facilitate training rely on the assumption that the feature dimensions of the input to the network have zero mean and unit variance.
A necessary step in order to take advantage of many of these tricks is then to standardize the feature dimensions of the input, typically using statistics computed on the training set.
An alternative which can also facilitate convergence is to whiten data using Pricipal Component Analysis before presenting it to the network \cite{lecun2012efficient}.

\paragraph{Model Architecture}

Beyond the simple feedforward network model we have focused on thus far, a variety of more specialized models have been proposed which intend to exploit different types of structure present in real-world data.
For example, presenting sequential data one step at a time to a feedforward network assumes temporal independence, which is invalid for most real-world temporal data.
Other types of data, such as images, exhibit local dependencies which are invariant to translation.
Modeling this type of structure can be greatly facilitated by the important choice of model architecture.
We give an overview of the two most relevant architectures for sequential data, recurrent and convolutional networks, in the next two sections.

\subsubsection{Recurrent Networks}

A natural extension to feedforward networks are {\em recurrent} connections, which feed the output of the model back into its input.
The resulting model can be considered a learnable function which at each time step produces an output based on its current input and previous output.
This class of models, called a recurrent networks, are well-suited for sequential data due to their ability to model temporal dependencies.

Training of recurrent networks is typically accomplished using ``backpropagation through time'' (BPTT) \cite{werbos1990backpropagation}.
BPTT is a simple extension of backpropagation where, given a sequence of length $T$, the recurrent network is ``unrolled'' over time and is treated as a feedforward network with $T$ layers, each of which corresponds to a single time step.
While BPTT provides a straightforward way to train recurrent networks, this approach can suffer dramatically from vanishing and exploding gradients for large values of $T$ for the same reasons that networks with many layers do \cite{bengio1994learning,hochreiter1997long,pascanu2013difficulty}.
The use of gating architectures \cite{hochreiter1997long,cho2014learning}, sophisticated optimization techniques \cite{martens2011learning,sutskever2013importance}, gradient clipping \cite{pascanu2013difficulty,graves2013generating}, and/or careful initialization \cite{sutskever2013importance,jaegar2012long,mikolov2014learning,le2015simple} can help mitigate this issue and has facilitated the success of RNNs in a variety of fields (see e.g.\ \cite{graves2012supervised} for an overview).
However, these approaches don't {\em solve} the problem of vanishing and exploding gradients for recurrent networks, and as a result these models are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps.
Because the representations used for audio data are typically at least thousands of time steps in length, we will not use recurrent networks in this thesis, although we will use them as a point of comparison and motivation in some places.

\subsubsection{Convolutional Networks}
\label{sec:convolutional_networks}

For many types of data (represented as multi-dimensional arrays), one or more of the dimensions may have an explicit ordering.
Most relevant to this thesis is sequential data, which always has a temporal dimension.
Traversing this dimension corresponds to the passing of time; if one event occurred before another, it will appear first.
Similarly, images have ``width'' and ``height'' dimensions, which correspond to left-right and top-bottom traversal.
In many cases, the correlations along these dimensions are highly localized.
For example, in a text document the probability that a given word appears in a certain place depends much more strongly on the words appearing around it than it does on words which appeared hundreds of words ago.

The ``fully-connected'' feedforward networks we have discussed so far cause the output of each layer to depend on each unit of the layer's input due to the fact that the weight matrix has an entry which connects each input unit to each output unit.
This ignores the structure of data with an ordered dimension and may be inefficient when most of the dependencies are local.
The {\em convolutional network} architecture is formulated specifically to take advantage of this common structure.
As their name suggests, rather than utilizing an affine transformation using a fully-connected matrix, layers in these models convolve small filter ``kernels'' across their input.
Networks with this structure were first proposed by Fukushima \cite{fukushima1980neocognitron} and training them with backpropagation was popularized by LeCun et.\ al \cite{lecun1989backpropagation}.
Recently, convolutional networks have provided breakthroughs in a variety of domains, including image classification \cite{krizhevsky2012imagenet,he2015delving,szegedy2015going}, speech recognition \cite{abdel2012applying,sainath2013deep}, music information retrieval \cite{humphrey2012rethinking,van2013deep,schluter2014improved,ullrich2014boundary,mcfee2015software,schluter2015exploring}, and natural language processing \cite{kim2014convolutional,zhang2015text}.

In the two-dimensional case (as used in this thesis), a convolutional layer takes as input $x \in \mathbb{R}^{M \times I_x \times I_y}$ and outputs $y \in \mathbb{R}^{N \times J_x \times J_y}$ by computing
\begin{equation}
        y[n, j_x, j_y] = \sigma\left(\sum_{m = 1}^M \sum_{k_x = 1}^{K_x} \sum_{k_y = 1}^{K_y} w[n, m, k_x, k_y] x[m, j_x + k_x, j_y + k_y] + b[n]\right)
\label{eq:convolution}
\end{equation}
where $w \in \mathbb{R}^{N \times M \times K_x \times K_y}$ are the layer's filters (also referred to as kernels), $\sigma$ is a nonlinearity, and $b \in \mathbb{R}^N$ are the layer's biases.
This operation is visualized in \cref{fig:convolution}.
The first dimension of $x$ and $y$ is referred to as the ``channels'' and can be considered different views or representations of the data (e.g.\ R, G, and B channels in images or left and right channels in a stereo audio recording).
The remaining dimensions are feature dimensions each of which has an inherent ordering, which is exploited by the translation inherent in the convolution operation.
The exact form of \cref{eq:convolution} depends on a variety of design choices, such as the convolution's stride, whether the convolution is zero-centered, whether biases are shared across input channels, whether a correlation or convolution is computed, and whether the input will be zero-padded.
We forgo a complete enumeration of these differences here and instead refer to \cite{dumoulin2016guide}, which provides a thorough discussion of the arithmetic involved in convolutional networks.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-convolution.pdf}
  \caption[Convolution operation]{Visualization of the convolution operation defined in \cref{eq:convolution}.
  The input and output are shown as $6 \times 6$ and $4 \times 4$ grids of units respectively.
  The output unit highlighted in green depends on the green input units via a $3 \times 3$ filter, represented as horizontal lines.
  Convolution involves evaluating this filtering operation at all possible positions in the input.}
  \label{fig:convolution}
\end{figure}

From this definition, we can observe a number of beneficial properties of using convolutional networks.
First, the number of parameters per layer is potentially dramatically decreased: Following the common practice of ``flattening'' dimensions, a fully-connected layer with the same input and output dimensionality would have a weight matrix of shape $NJ_xJ_y \times I_xI_y$, compared with $N \times M \times K_x \times K_y$ with $K_x < J_x \le I_x$ and $K_y < J_y \le I_x$.
This is commonly referred to as ``parameter sharing'', because the same set of weights are applied at different locations in the input rather than using a unique weight for each position.
This property also gives the convolutional layer translational invariance, i.e.\ the ability to detect a certain pattern regardless of where it appears in the image.
A final advantage to using a convolution operation is that the layer can naturally handle inputs whose feature dimensions vary in size - the kernels simply slide over the input regardless of its shape.
This makes convolutional networks well-suited for sequential data whose length varies.

An additional ingredient commonly found in convolutional networks are {\em pooling} layers, which provide additional translational invariance by aggregating their input over small regions.
The most common pooling layer, and the only one used in this thesis, is the max-pooling layer, which outputs the maximum value within each input region.
Given input $x \in \mathbb{R}^{M \times I_x \times I_y}$, a max-pooling layer with stride $p$ produces $y \in \mathbb{R}^{M \times \frac{I_x}{p} \times \frac{I_y}{p}}$ by
\begin{equation}
y[m, j_x, j_y] = \max(x[m, j_x:j_x + p, j_y:j_y + p])
\end{equation}
for $j_x \in 1, \ldots, \frac{I_x}{p}$ and $j_y \in 1, \ldots, \frac{I_y}{p}$.
The value of $p = 2$ is almost uniformly used.
The max-pooling layer can thus be seen as indicating whether a given feature is present or absent over a region of the input.
Because pooling layers effectively downsample their input, they can be used to increase the ``receptive field'' (region of the input affecting a single unit of the output) of layers in networks consisting of many convolutional and pooling layers.

Finally, in convolutional networks it is common for the last few layers to be standard fully-connected layers.
This gives the output of the network access to the entire input region.
In this framework, the series of convolutional and pooling layers transform and downsample the input, providing appropriate invariances and often learning a hierarchy of higher and higher-level representations \cite{zeiler2014visualizing}.
In domains where none of the dimensions of the input vary from sample to sample, all of the channels and dimensions at the output of the convolutional portion of the network are flattened prior to being passed to the fully-connected layers.
When one or more dimensions are variable in size, only the fixed-size dimensions are processed by the fully-connected layers.
For example, in sequences where the sequence length varies, the output of the convolutional layersfor each time step are passed to the fully-connected layers independently.

\subsection{Stochastic Optimization}

In modern practice, the datasets used for training neural networks are typically huge, so optimizing their parameters over the entire training set is often inefficient and sometimes infeasible.
The prevailing solution is to train over a tiny subset of the training set (called a ``minibatch'') which is sampled randomly at each iteration.
This causes the objective to change stochastically at each iteration of optimization.
Beyond practical limitations, using a stochastic objective has also been shown to help avoid stationary points because the objective surface is constantly changing \cite{lecun2012efficient}.

So far, we have only alluded to the fact that the gradient information produced by backpropagation is used to train a neural network.
This is achieved using gradient descent methods, which specify a formula to adjust the network's parameters to minimize an objective.
In many problems, it is beneficial to leverage second-order gradient information to estimate the local curvature of the objective function during optimization.
However, the number of entries in the Hessian (matrix of second-order derivatives) is quadratic in the number of parameters, which makes computing it infeasible for modern neural network models which typically have millions of parameters.
As a result, neural networks are typically trained using gradient descent methods which only utilize first-order gradient information.

A final difficulty in training neural networks is that changing different parameters can have substantially different effects on the objective value.
For example, even in simple feedforward networks, adjusting a value in the final layer's bias vector may induce a considerable change in typical outputs of the network while changing a value in the first layer's weight matrix may only produce a small effect in a limited number of cases.
This problem can be exacerbated when different structures are used in different parts of the network, e.g.\ convolutional or recurrent layers.
As a result, it can be beneficial to update different parameters by different amounts according to the extent to which they effect the objective value.

The stochastic objective arising from training on minibatches and the requirement of only utilizing first-order gradients prompts the use of a special class of gradient descent methods called stochastic optimization.
Effective stochastic optimization techniques typically include mechanisms for smoothing the updates, which helps mitigate noise from stochasticity, for estimating second-order information, which improves convergence, and for adaptively updating different parameters by different amounts.
In addition, most optimization methods have at least one hyperparameter; good optimization techniques are able to be effective without being particularly sensitive to their hyperparameter values.
In this section, we discuss some of the more popular stochastic optimization methods, particularly those which are used or referenced to in this thesis.
Similar overviews are available in \cite{ruder2016overview,bayer2015climin}, and \cite{schaul2013unit} provides a comparison of many of these techniques on a variety of toy optimization problems.

In the following, we will use $\mathcal{L}$ to refer to the loss function being minimized.
$\mathcal{L}$ is typically a function of the target output, the input, and the parameters being optimized; because all but the latter can be treated as a constant, we omit those terms in this section.
In addition, because the parameters change from iteration to iteration we will use $\theta_t$ to denote the parameters of the model at iteration $t$.
Finally, $\nabla \mathcal{L}(\theta_t)$ denotes the gradient of the loss function with respect to the parameters at iteration $t$.

\subsubsection{Stochastic Gradient Descent}
\label{sec:sgd}

Stochastic gradient descent (SGD) simply updates each parameter by subtracting the gradient of the loss with respect to the parameter, scaled by the ``learning rate'' hyperparameter $\eta$:
\begin{equation}
\theta_{t + 1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
\end{equation}
SGD provides the fundamental ingredient on which other stochastic optimization methods are built.
Provided that $\eta$ is set correctly, SGD will decrease the loss function at each iteration because it is adjusting the parameters in the negative gradient direction.
However, if $\eta$ is too large, SGD can diverge (increase the loss function or oscillate between increasing and decreasing it); if it's too small, it will decrease the objective value very slowly \cite{lecun2012efficient}.
This sensitivity to the setting of $\eta$ is visualized in \cref{fig:learning_rate}.
In addition, when the loss surface is ill-conditioned (i.e.\ the curvature of the loss surface is dramatically different along different parameter axes), convergence can also be extremely slow because $\eta$ must be set small enough so that it does not diverge along any parameter axis.
Nevertheless, SGD can be effective when $\eta$ is chosen correctly; a common heuristic is to choose the largest value of $\eta$ which does not cause divergence by starting with a large $\eta$ and decreasing it by a factor of $3$ until optimization no lonver diverges \cite{bengio2012practical}.

\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{figures/2-learning_rate.pdf}
  \caption[Sensitivity of gradient descent to the learning rate]{Visualization of the sensitivity of gradient descent to the learning rate $\eta$.
  The function being optimized is a simple quadratic function represented as a blue line, and each iteration of optimization is represented as a black dot.
  In the top left, the learning rate is set too high and the optimization will diverge.
  In the top right, the learning rate is small enough that optimization will eventually converge, albeit slowly due to oscillations around the minimum.
  In the bottom left, the learning rate is set near-optimally and the minimum has effectively been reached in two iterations.
  In the bottom right, the learning rate is set too small and progress towards the minimum is slow.}
  \label{fig:learning_rate}
\end{figure}

\subsubsection{Momentum}

In SGD, the gradient $\nabla \mathcal{L}(\theta_t)$ often changes rapidly at each iteration due to the fact that the loss is being computed over different data.
It can therefore be beneficial to smooth the updates at each iteration.
This can be achieved by re-using the gradient value from the previous iteration, scaled by a ``momentum'' hyperparameter $\mu$, as follows \cite{polyak1964some}:

\begin{align}
v_{t + 1} &= \mu v_t - \eta \nabla \mathcal{L}(\theta_t) \\
\theta_{t + 1} &= \theta_t + v_{t+1}
\end{align}

In addition, it has been argued that including the previous gradient step has the effect of approximating some second-order information about the gradient because directions with low curvature will tend to persist across iterations and accumulate, which can mitigate conditioning issues \cite{sutskever2013importance}.
In practice, $\mu$ is often set close to $1$; this will cause parameters to change slowly from iteration to iteration.
While early theory and intuition suggested that some of the benefits of momentum are lost in the stochastic setting \cite{lecun2012efficient}, it can be particularly beneficial in the early stages of learning before the loss function is close to any minima.

\subsubsection{Nesterov's Accelerated Gradient}

In Nesterov's Accelerated Gradient (NAG) \cite{nesterov1983method}, the gradient of the loss at each step is computed at $\theta_t + \mu v_t$ instead of $\theta_t$.  The update rules are then as follows:

\begin{align}
v_{t + 1} &= \mu v_t - \eta \nabla\mathcal{L}(\theta_t + \mu v_t) \\
\theta_{t + 1} &= \theta_t + v_{t+1}
\end{align}

In momentum, the parameter update could be written $\theta_{t + 1} = \theta_t + \mu v_t - \eta \nabla \mathcal{L}(\theta_t)$, so NAG effectively computes the gradient at the new parameter location but without considering the gradient term (i.e.\ using the first two terms of this update only).
In practice, this causes NAG to behave more stably than regular momentum in many situations, and so it is often chosen over classical momentum.
A more thorough analysis can be found in \cite{sutskever2013importance}.

\subsubsection{Adagrad}

Adagrad \cite{duchi2011adaptive} effectively rescales the learning rate for each parameter according to the history of the gradients for that parameter.
This is done by dividing each term in $\nabla \mathcal{L}$ by the square root of the sum of squares of its historical gradient:
\begin{align}
g_{t + 1} &= g_t + \nabla \mathcal{L}(\theta_t)^2 \\
\theta_{t + 1} &= \theta_t - \frac{\eta\nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t + 1}} + \epsilon}
\end{align}
where division is elementwise and $\epsilon$ is a small constant included for numerical stability.
Rescaling in this way effectively lowers the learning rate for parameters which consistently have large gradient values, which can be particularly useful in ill-conditioned settings.
It also effectively decreases the learning rate over time, because the sum of squares will continue to grow with the iteration.
This can prove beneficial as the loss function becomes close to a minimum, where large changes in the loss function would be detrimental.

\subsubsection{RMSProp}

In its originally proposed form \cite{tieleman2012lecture}, RMSProp is very similar to Adagrad.
The only difference is that the $g_t$ term is computed as an exponentially decaying average instead of an accumulated sum.
This makes $g_t$ an estimate of the second moment of $\nabla \mathcal{L}$ and prevents the learning rate from effectively shrinking over time.
The name "RMSProp" comes from the fact that the update step is normalized by a decaying approximate RMS of recent gradients.
The update is as follows:

\begin{align}
g_{t + 1} &= \gamma g_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t)^2 \\
\theta_{t + 1} &= \theta_t - \frac{\eta\nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t + 1}} + \epsilon}
\end{align}

In the original lecture slides where it was proposed, $\gamma$ is set to $.9$, but it is most commonly treated as a variable hyperparameter.
In \cite{dauphin2015rmsprop}, it is shown that the $\sqrt{g_{t + 1}}$ term approximates (in expectation) the diagonal of the absolute value of the Hessian matrix (assuming the update steps are $\mathcal{N}(0, 1)$ distributed).
It is also argued that the absolute value of the Hessian is better to use for non-convex problems which may have many saddle points.

Alternatively, \cite{graves2013generating} defines RMSProp with an additional first moment approximator $m_t$ and a momentum term $v_t$:
\begin{align}
m_{t + 1} &= \gamma m_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t) \\
g_{t + 1} &= \gamma g_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t)^2 \\
v_{t + 1} &= \mu v_t - \frac{\eta \nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t+1} - m_{t+1}^2 + \epsilon}}  \\
\theta_{t + 1} &= \theta_t + v_{t + 1}
\end{align}
Including the first-order moment estimate in the denominator in this way causes the learning rate to be effectively normalized by the standard deviation of $\nabla \mathcal{L}$ in each direction.


\subsubsection{Adam}

Adam \cite{kingma2015adam} is somewhat similar to Adagrad and RMSProp in that it computes a decayed moving average of the gradient and squared gradient (first and second moment estimates) at each time step.
It differs mainly in two ways: First, the separate moving average coefficients $\gamma_1$ and $\gamma_2$ are used for first and second moments respectively (\cref{eq:adam_m} and \cref{eq:adam_g}).
Second, because the first and second moment estimates are initialized to zero, some bias-correction is used to counteract the resulting bias towards zero (\cref{eq:adam_mhat} and \cref{eq:adam_ghat}).
Given hyperparameters $\gamma_1$, $\gamma_2$, $\lambda$, and $\eta$, and setting $m_0 = 0$ and $g_0 = 0$, the update rule is as follows:

\begin{align}
m_{t + 1} &= \gamma_1 m_t + (1 - \gamma_1) \nabla \mathcal{L}(\theta_t) \\
\label{eq:adam_m}
g_{t + 1} &= \gamma_2 g_t + (1 - \gamma_2) \nabla \mathcal{L}(\theta_t)^2 \\
\label{eq:adam_g}
\hat{m}_{t + 1} &= \frac{m_{t + 1}}{1 - \gamma_1^{t + 1}} \\
\label{eq:adam_mhat}
\hat{g}_{t + 1} &= \frac{g_{t + 1}}{1 - \gamma_2^{t + 1}} \\
\label{eq:adam_ghat}
\theta_{t + 1} &= \theta_t - \frac{\eta \hat{m}_{t + 1}}{\sqrt{\hat{g}_{t + 1}} + \epsilon}
\end{align}

The use of the estimated first and second moments ensure that, in most cases, the step size is $\approx \pm \eta$ and that in magnitude it is less than $\eta$.
However, as $\theta_t$ approaches a true minimum, the uncertainty of the gradient will increase and the step size will decrease.
It is also invariant to the scale of the gradients.
These properties make it well-suited for neural network architectures, and as a result has seen rapid adoption.

\subsection{Bayesian Optimization}
\label{sec:bayesian_optimization}

From the discussion above, it is clear that beyond the parameters of neural network models which are optimized by gradient descent (weight matrices, bias vectors, etc.), there are additional ``hyperparameters'' which govern specifics of the model architecture, loss function, and training.
These hyperparameters are so called because they are not optimized through gradient descent with the rest of the parameters.
Nevertheless, the success or failure of training a given model will often heavily depend on these hyperparamters being set appropriately.
This problem fits into the general framework of black-box optimization, where we would like to maximize an objective function with tunable parameters which produces different values based on different parameter settings.
In this framework, we assume that we do not have a way to compute an analytical solution or access to gradient information about the function and thus can only make decisions about what parameter settings to try based on the history of function evaluation results for different settings.
For example, the function may be a mapping between hyperparameters governing the structure of a neural network model (number and width of layers, weight initialization scheme, etc.), its loss function (which loss function to use, whether to include regularization), and training procedure (which stochastic optimization technique to use, each of which has its own hyperparamwters such as the learning rate) and the objective value could be the trained model's accuracy on a held-out validation set.
Alternatively, one could consider the process of baking a cake in this framework, where the parameters are the amount and quantity of each ingredient as well as the baking temperature and time and the objective value is how good the cake tastes.

In practice, this type of problem is often tackled by adjusting parameters by hand based on a series of trials of different settings.
This approach can be extremely time consuming and expensive and may produce suboptimal results, so various approaches for automating this process have been proposed.
A popular choice is ``grid search'', which first quantizes any continuously-valued parameters and then exhaustively tries all combinations of all settings.
This approach is only feasible when there are few parameters and possible settings due to the exponential number of trials which must be run.
In addition, it has been shown that grid search is often less efficient than simply randomly trying different parameter values, particularly when the setting of one or more parameters does not have a strong effect on the objective value \cite{bergstra2012random}.
While grid search and random search can be effective, neither utilize the results of previous trials to inform subsequent parameter settings.

A valuable alternative is Bayesian Optimization, which utilizes all of the information available in previous trials to produce a probabilistic model of the black-box function being optimized.
This requires more computation than methods which only use local approximations of the function, but can produce good solutions to highly non-convex functions after making only a small number of function evaluations.
This is particularly valuable when the function itself is expensive to evaluate, as is the case when training machine learning models which take hours, days, or weeks (or when baking cakes, due to the price of ingredients).
The use of a probabilistic model of the black-box function being optimized both allows for candidate parameter settings to be chosen in a principaled way and for prior information to be integrated into the optimization process.
We utilize Bayesian Optimization techniques throughout this thesis for black-box optimization of different algorithms and systems.
This section provides a brief overview of Bayesian Optimization; for more thorough treatments see \cite{brochu2010tutorial,snoek2012practical}.
A visualization of Bayesian Optimization is shown in \cref{fig:bayesian_optimization}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/2-bayesian_optimization.pdf}
  \caption[Bayesian optimization of a synthetic function]{Visualization of Bayesian optimization of a synthetic one-dimensional objective function, shown as a blue line.
  Points at which the objective has been evaluated are shown as black dots.
  The gray shaded area shows the posterior estimate of the function, modeled by a Gaussian process with a squared exponential kernel.
  The acquisition function, calculated using expected improvement, is shown in green.
  Due to the locations of the objective evaluations, the posterior estimate has failed to account for the true maximum at the right of the plot.}
  \label{fig:bayesian_optimization}
\end{figure}

\subsubsection{Gaussian Processes}

Utilizing Bayesian Optimization requires first choosing a prior over functions which expresses beliefs about the behavior of the function being optimized.
A common choice are Gaussian processes, which associate every point in the (continuous) parameter space with a normally distributed random variable.
An alternative definition is that every finite collection of points in the parameter space induces a multivariate Gaussian distribution.
In the setting of Bayesian Optimization, this corresponds to assuming that the objective values are characterized by Gaussian distributions.

More formally, given parameter settings $\theta \in \Theta$ where $\Theta$ is the parameter space, a Gaussian process is characterized by a mean function $m : \Theta \rightarrow \mathbb{R}$ and covariance function $k : \Theta \times \Theta \rightarrow \mathbb{R}$.
The mean function is (without loss of generality) typically assumed to be zero, while the choice of the covariance function allows strong assumptions to be made about the function being estimated.
A common choice is the squared exponential kernel
\begin{equation}
        k(\theta_i, \theta_j) = \mathrm{exp}\left(-\frac{1}{2\omega^2}\|\theta_i - \theta_j\|^2\right)
\end{equation}
where $\omega$ is a hyperparameter controlling the width of the kernel.
\cite{snoek2012practical} argues that this kernel is too smooth for practical optimization problems and recommends the use of the Mat\'ern 5/2 kernel instead.
In general, the covariance function should be defined so that it approaches $1$ as its arguments get closer together and $0$ as they get farther apart.
The hyperparameters of the covariance function function can either be set according to prior beliefs about the function being modeled, or can be estimated by optimizing the marginal likelihood of the observed function evaluations under the Gaussian process.

Once mean and covariance functions have been chosen, the Gaussian process can be fit to training data and a posterior can be obtained by the following process:
Denoting the $t$ parameter settings and observed objective values as $\theta_T = \{\theta_1, \ldots, \theta_t\}$ and $y_T = \{y_1, \ldots, y_t\}$ respectively and an unseen evaluation of the objective function $y_n$, the joint marginal likelihood of $y_n$ and the observed objective values is given by
\begin{equation}
        p(y_T, y_n) = \mathcal{N}\left(0,
                \begin{bmatrix}
                        K_{T \times T} & K_{n \times T}^\top\\
                        K_{n \times T} & k(\theta_n, \theta_n)\\
        \end{bmatrix}\right)
\end{equation}
where
\begin{align}
        K_{T \times T} &= \begin{bmatrix}
                k(\theta_1, \theta_1) & \cdots & k(\theta_1, \theta_t) \\
                \vdots & \ddots & \vdots \\
                k(\theta_t, \theta_1) & \cdots & k(\theta_t, \theta_t) \\
        \end{bmatrix} \in \mathbb{R}^{t \times t} \\
        K_{n \times T} &= \begin{bmatrix}
                k(\theta_n, \theta_1) & \cdots & k(\theta_n, \theta_t)
        \end{bmatrix} \in \mathbb{R}^{1 \times t}
\end{align}
\begin{equation}
\end{equation}
This can be used to obtain the predictive distribution (see \cite{rasmussen2006gaussian} for a derivation)
\begin{equation}
        p(y_{n} | y_T, \theta_T, \theta_n) = \mathcal{N}(\mu_T(\theta_n), \sigma_T^2(\theta_n))
\end{equation}
where
\begin{align}
        \mu_T(\theta_n) &= K_{n \times T} K_{T \times T}^{-1} \begin{bmatrix}
                y_1 \\
                \vdots \\
                y_t
        \end{bmatrix} \\
        \sigma_T^2(\theta_n) &= k(\theta_n, \theta_n) - K_{n \times T} K_{T \times T}^{-1} K_{n \times T}^\top
\end{align}

Note that the predictive distribution's covariance matrix will have 1 all along its diagonal because any point is perfectly correlated with itself.
This is an invalid assumption in the presence of noise, so a common modification to include a term $\sigma_{noise}^2$ for the variance of the noise in the covariance, which yields the predictive distribution
\begin{equation}
p(y_{n} | y_T, \theta_T, \theta_n) = \mathcal{N}(\mu_T(\theta_n), \sigma_T^2(\theta_n) + \sigma_{noise}^2)
\end{equation}

\subsubsection{Acquisition Functions}

We now have a recipe for predicting the mean and variance of the objective function's value at any point based on observed values.
In order to perform optimization, we also need a principaled approach for selecting new parameter settings to evaluate the function at, both in hopes of achieving higher objective values and for obtaining a better estimate of the objective function.
This is achieved through acquisition functions, which utilize the predictive distribution to construct a function whose maximum corresponds to the point at which the objective function should be evaluated at next.
Acquisition functions can be considered a proxy function which can be optimized to estimate the objective function's maximum value under certain assumptions.
Importantly, sampling values from acquisition functions is potentially much cheaper than evaluating the objective function at new parameter values, which is one of the core reasons that Bayesian optimization is beneficial.
To facilitate optimization, acquisition functions should appropriately balance exploration and exploitation, i.e.\ the trade-off between reducing uncertainty and finding the absolute best value.

\paragraph{Probability of Improvement}

An intuitive choice is to maximize the probability that a given point is larger than the best observed value, which we denote $y_{best} = \max(y_T)$.
This probability can be computed as
\begin{equation}
        \mathrm{PI}(\theta) = \Phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right)
        \label{eq:probability_of_improvement}
\end{equation}
where $\Phi$ is the normal cumulative distribution function.
In the case that $\sigma_T(\theta) = 0$, $\mathrm{PI}(\theta)$ is set to $0$.
This strategy will always choose the $\theta$ which has the highest probability of yielding a larger objective value regardless of whether this value will provide more information about the function, i.e.\ it is completely explotation-based.
A simple way to mitigate this is to add an exploration trade-off parameter $\xi$ to instead compute
\begin{equation}
        \mathrm{PI}(\theta) = \Phi\left(\frac{\mu_T(\theta) - y_{best} + \xi}{\sigma_T(\theta)}\right)
\end{equation}

\paragraph{Expected Improvement}

Instead of simply choosing the point which is most likely to yield an improvement, we can also consider the expected magnitude of the improvement.
This can be encapsulated using an ``improvement function'' which is positive when the objective value for the predicted value of $\theta$ is larger than $y_{best}$.
Maximizing the expectation of this improvement function has a closed-form solution, yielding
\begin{equation}
        \mathrm{EI}(\theta) = (\mu_T(\theta) - y_{best})\Phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right) + \sigma(\theta)\phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right)
\end{equation}
where $\phi$ is the normal probability density function.
\cite{snoek2012practical} claims that Expected Improvement yielded the best performance among acquisition functions for hyperparameter optimization of machine learning systems.

\paragraph{Upper Confidence Bound}

An alternative approach, which is also conceptually simple, is to maximize the upper confidence bound
\begin{equation}
        \mathrm{UCB}(\theta) = \mu_T(\theta) + \kappa \sigma_T(\theta)
\end{equation}
$\kappa \ge 0$ is a hyperparameter which scales the confidence limits and therefore balances exploration and exploitation.

\section{Digital Signal Processing}

The field of signal processing encompasses the analysis, modification, and generation of ``signals''.
Signals are broadly defined as sequential data, i.e.\ data where at least one dimension has an implicit ordering (for some examples, see \cref{fig:example_sequences}).
A digital signal is a sequence of numbers or vectors where each number represents the amplitude of the signal in a given dimension at a given point in time.
In this setting, signals can be processed computationally, which allows for the development of algorithms to automatically manipulate, generate, and extract information from signals.

\subsection{Audio Signals and Psychoacoustics}
\label{sec:psychoacoustics}

In this thesis, we will mainly deal with audio signals.
An example audio signal, consisting of a series of notes played on an acoustic guitar, can be seen in \cref{fig:audio_signal}.
In the physical world, an audio signal is a time-varying change in air pressure called a sound wave.
We ``hear'' sounds when our ear converts these sound waves into neural stimuli.
The exact manner in which we perceive a sound wave is influenced by a wide variety of factors, including our sensory system and the brain's psychological response.
The field of psychoacoustics attempts to measure and codify these factors, and the results can provide useful guidelines about the representation and processing of audio signals.
In this section, we present a small overview of some important psychoacoustic principles; a more thorough treatment can be found in \cite{cook1999music,bosi2012introduction}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-audio_signal.pdf}
  \caption[Audio recording of an acoustic guitar]{Audio signal of a few notes played on an acoustic guitar.
  The peaks of amplitude correspond to each individual note being plucked.}
  \label{fig:audio_signal}
\end{figure}

Three of the most important psychoacoustic characteristics of audio signals are the pitch, loudness, and timbre.

\paragraph{Pitch}

The pitch of a signal mainly depends on its periodicity (e.g.\ how often the signal repeats itself); if a signal is has no periodic components, it might not be perceived as having a pitch.
The longest periodicity, or the slowest vibration, is referred to as the fundamental frequency $f_0$ of a sound.
Generally, we can perceive frequencies from 20 Hz to 20,000 Hz, although these limits depend on a variety of factors including the number of independent frequencies in a signal and the healthiness of the listener's auditory system.
Below 20 Hz, we begin to hear the periodicities of an audio signal as distinct events rather than as a ``pitch''.

An additional important characteristic of pitch perception is that it is roughly logarithmic, i.e.\ we perceive the ``distance'' between signals with fundamental frequencies $f_0$ and $2f_0$ to be about the same as $2f_0$ and $4f_0$.
This may be one reason that the spacing between pitches (called semitones) in Western music is defined on a logarithmic scale.
In addition, our inner ear has different regions which are sensitive to different ranges of frequencies, which are also spaced roughly on a logarithmic scale.
This helps our auditory system distinguish between sources sounding simultaneously which differ in pitch.
Whether or not a given signal is perceived as having multiple pitches or sources (e.g.\ in the case of multiple notes being played simultaneously on a single musical instrument) depends both on the characteristics of the signal and on psychological factors.

\paragraph{Loudness}

Loudness corresponds to how ``strong'' a sound is perceived as being and most directly depends on the signal's sound pressure level (SPL).
The SPL is proportional to the logarithm of the signal's amplitude and is measured in Decibels (dB).
In addition, perception of loudness is also heavily frequency-dependent; for example, a 20 dB signal with a fundamental frequency 1 kHz will be perceived to have the same loudness as a 100 Hz signal at about 45 dB.
This relationship is typically characterized by equal loudness curves, which relate the required sound pressure level for each frequency to be perceived as the same loudness.
An example curve can be seen in \cref{fig:equal_loudness}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-equal_loudness.pdf}
  \caption[Equal loudness curve]{Equal loudness curve, along which all frequencies are perceived to have approximately the same loudness.
  These curves reveal important characteristics about our sensitivity to different frequencies; for example, in order for a low-frequency sound to be perceived loudly, they must have a relatively high sound pressure level.}
  \label{fig:equal_loudness}
\end{figure}

\paragraph{Timbre}

The remaining characteristics of a sound which allow it to be distinguished from other sounds are referred to as its ``timbre''.
For example, the different frequencies (or periodicities) present and the evolution of their relative amplitudes over time strongly effect our perception of a sound.
Being sensitive to timbre can help us distinguish between the voices of different people and notes played on different musical instruments.
\Cref{fig:timbres} shows waveforms of a piano and a violin playing the same note at the same volume; despite having the same pitch and loudness, these sounds are easily distinguishable.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-timbres.pdf}
  \caption[Waveforms of piano and violin]{Waveforms of a few periods of the same note being played on a piano (blue) and a violin (green) at the same loudness.
  These two signals will be perceived as having the same pitch and loudness, but are distinguishable due to differences in timbre.}
  \label{fig:timbres}
\end{figure}

\subsubsection{Sampling Frequency}

Typically, the temporal spacing between subsequent elements in a digital signal is fixed and is determined by the signal's sampling frequency $f_s$.
This is in contrast to continuous-time signals encountered in the real world, such as sonic waves, which can be considered functions of a continuous timebase.
A fundamental relationship between the content of signals and their sampling frequency is codified in the Sampling Theorem.
This theorem states that any continuous-time signal can be perfectly reconstructed from discrete-time samples at sampling frequency $f_s$ as long as it has no frequency content above $f_s/2$.

Due to our hearing range, in audio signals we are mostly interested in reproducing signals up to 20,000 Hz.
Following the sampling theorem, this implies that in order to perfectly perceptually reconstruct audio signals, we should utilize a sampling rate of at least 40,000 Hz.
In practice, a common sampling rate for audio signals is 44,100 Hz, which is utilized on compact discs and other recorded audio formats.
Because our sensitivity to high frequencies is limited, for analysis purposes it is common to use a lower sampling rate.
Throughout this thesis, we will utilize a sampling rate of 22,050 Hz.

\subsection{Time-Frequency Analysis}

In many cases, it is useful to decompose a signal into the combination of simpler signals.
An extremely common method is the Fourier Transform, which is capable of representing any signal as a sum of complex sinusoids.
This representation is well-suited for signals from the natural world which are produced by vibrating objects such as musical instruments.
In addition, as mentioned above, our inner ear performs a similar decomposition because its different regions vary in their sensitivity to different frequencies.
\cite{smith2011spectral,smith2007mathematics} give a comprehensive discussion of time-frequency analysis of audio signals.

For a discrete-time length-$N$ single-dimensional signal $x$, the Discrete Fourier Transform (DFT) is computed as
\begin{equation}
        X[k] = \sum_{n = 1}^N x[n] e^{-2\pi \jmath (k - 1)(n - 1)/N}, k \in \{1, \ldots, N\}
\end{equation}
The resulting complex values $X[k]$ are called the Fourier coefficients of $x$, and the collection of coefficients $X$ is called the Fourier spectrum (or, simply, the spectrum) of $x$.
From its definition, it is clear that the DFT can be computed in $\mathcal{O}(N^2)$ time; in practice, the DFT is computed using the Fast Fourier Transform (FFT) algorithm, which runs in $\mathcal{O}(N \log(N))$ time.

For real-valued signals (the only kind encountered in this thesis) where $N$ is even, the Fourier coefficients are Hermitian symmetric so that
\begin{equation}
        X[k] = \overline{X[N - k + 2]}, k \in \{2, \ldots, N/2 + 1\}
\end{equation}
For this reason, the DFT of real-valued signals of even length $N$ is typically only computed for $k \in \{1, \ldots, N/2 + 1\}$.
If the real-valued input signal has sampling rate $f_s$, then the $k$th Fourier coefficient (also called a ``frequency bin'') corresponds to a frequency of
\begin{equation}
        \frac{(k - 1)f_s}{N}, k \in \{1, \ldots, N/2 + 1\}
        \label{eq:bin_freq}
\end{equation}

The DFT provides us with a decomposition of a signal in the form of complex-valued coefficients for sinusoids of varying frequency.
However, for most audio signals, the human auditory system is much more sensitive to the {\em magnitude} of different frequencies than it is to the phase of these components.
For this reason, for analysis tasks the phase of the Fourier coefficients $X[k]$ is typically discarded and the magnitude $|X[k]|$ is used instead.
As an example, the magnitude spectrum of the audio signal shown in \cref{fig:audio_signal} is shown in \cref{fig:spectrum}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-spectrum.pdf}
  \caption[Magnitude spectrum of the acoustic guitar recording]{Magnitude spectrum of the audio recording of an acoustic guitar shown in \cref{fig:audio_signal}.
  The tall peaks correspond to the dominant frequencies; for example, the largest peak around 440 Hz corresponds to an A note which is played frequently and loudly in the recording.
  For clarity, only the portion of the spectrum corresponding to frequencies up to 2000 Hz is shown.}
  \label{fig:spectrum}
\end{figure}

\subsubsection{The Short-Time Fourier Transform}

In many cases, we are most interested in the way the frequency content of a signal changes over time.
The DFT as defined above can only provide a global summary in the form of a spectrum computed across the entire duration of a signal.
This prompts the use of the Short-Time Fourier Transform (STFT), which computes a series of spectra over short, potentially overlapping ``frames'' of the signal.
This produces a ``spectrogram'', computed by
\begin{equation}
        S[n, k] = \sum_{m = 1}^M w[m]x[m - M/2 + nR] e^{-2\pi \jmath (k - 1)(m - 1)/M}, k \in \{1, \ldots, M\}
\end{equation}
where $w \in \mathbb{R}^{M}$ is a ``window'' function and $R$ is the ``hop'' size which determines how much (if at all) subsequent frames overlap.
The resulting spectrogram $S$ will contain $\lfloor (N - M)/R \rfloor$ spectra, each with $M$ Fourier coefficients.

The STFT is useful for audio signals because we are generally interested in how the audio content changes over time.
For example, in music analysis we might be interested in where notes played in a recording start and stop.
This information is discarded in the magnitude spectrum of the entire recording, but the individual spectra which encompass the start and end of notes retain this information in a localized manner.
The magnitude spectrogram of the guitar recording from \cref{fig:audio_signal} is shown in \cref{fig:spectrogram}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-spectrogram.pdf}
  \caption[Magnitude spectrogram of the acoustic guitar recording]{Magnitude spectrogram of the audio recording of an acoustic guitar shown in \cref{fig:audio_signal}.
  As in \cref{fig:spectrum}, we only display frequencies up to 2,000 Hz.
  The horizontal tracks correspond to different notes which were played.}
  \label{fig:spectrogram}
\end{figure}

Important considerations when computing a spectrogram include the window function $w$, the frame length $M$, and the hop size $R$.

\paragraph{Window Function}

The choice of window function is generally a trade-off between the resolution and the sensitivity of the individual spectra.
For example, given a signal which consists of a pure tone (i.e.\ a sinusoid) at some frequency $f_0$, using a windowing function with low resolution would result in spectral energy being spread amongst the frequency bins around the one corresponding to $f_0$.
On the other hand, using a window function with low sensitivity will result in the inability to detect low-amplitude signals in the presence of noise or other high-amplitude signals.
The rectangular window
\begin{equation}
        w[m] = 1, m \in \{1, \ldots, M\}
\end{equation}
has the highest-possible resolution, but has poor sensitivity due to ``spectral leakage''.
Spectral leakage manifests itself as periodic ``side lobes`` in the frequency domain which decay away from each peak in the spectrum.
For spectra with high-energy bins, this leakage can prevent the accurate detection of other peaks, which decreases the resulting sensitivity.

Leakage is caused by truncating each frame immediately to zero at the window edges.
A common method for mitigating this is to use a window which tapers to zero.
The Hann window
\begin{equation}
        w[m] = \frac{1}{2}\left(1 - \cos\left(\frac{2\pi m}{M}\right)\right)
\end{equation}
provides a good trade-off between resolution and sensitivity; it has half of the resolution of the rectangular window but decreases side-lobe level and decay by a few orders of magnitude.
As a result, we will use the Hann window throughout this thesis.

\paragraph{Frame Length}

Increasing the frame length $M$ will result in higher-resolution spectra due to the fact that the frequencies corresponding to each bin are spaced according to \cref{eq:bin_freq}.
However, it will also result in lower time resolution, due to the fact that if multiple events happen within a single frame then it is impossible to determine their ordering by inspecting the magnitude spectrum.
Choosing $M$ is therefore a tradeoff of time and frequency resolution, i.e.\ we would like to be able to resolve differences of frequency and the timing of events which are relevant to the task.
A more practical consideration is that the FFT is optimized for frame lengths which are a power of two.
Based on these constraints, a common choice is to utilize a frame size of 2048 samples at a 22,050 Hz sampling rate, which corresponds to 92.8 ms with a 10.8 Hz resolution.

\paragraph{Hop Size}

Typically, there is some overlap between subsequent frames in an STFT.
This is a necessity when using windows which taper to zero in order to avoid loss of information.
For the Hann window in particular, in order to retain all information the hop size must be set no larger than half of the frame size.
Using more overlap can also smooth subsequent spectra in an STFT and can provide an additional form of temporal resolution, at a higher computational expense.
We therefore utilize a hop size of 512 samples throughout this thesis which corresponds to 23.2 ms at 22,050 Hz; this is believed to be at the upper limit of the rate at which our auditory system performs spectral analysis.

\subsubsection{Log-Magnitude}

As discussed in \cref{sec:psychoacoustics}, our sensitivity to loudness is closer to logarithmic than linear in amplitude.
The magnitude spectrograms we have studied so far report the amplitude of different frequencies across time.
We can make this representation more similar to our perception by using log-magnitude spectra, i.e.\ computing the logarithm of the magnitude in each frequency bin.
While sound pressure level is defined on a specific scale based on a reference SPL of 0 dB corresponding to the quietest perceptible sound, it is unimportant for the scale to be calibrated in analysis tasks which are scale invariant.

A log-magnitude version of the spectrogram from \cref{fig:spectrogram} is shown in \cref{fig:log_spectrogram}.
Note that this representation shows much more energy in higher frequencies.
These higher-frequency signals are the harmonics of the notes played, which were much less apparent in \cref{fig:spectrogram} where notes appeared almost as pure tones.
Accurately representing the our perception of these harmonics is important for representing different timbres.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-log_spectrogram.pdf}
  \caption[Log-magnitude spectrogram of the acoustic guitar recording]{Log-magnitude spectrogram of the audio recording of an acoustic guitar shown in \cref{fig:audio_signal}, for frequencies up to 2,000 Hz.}
  \label{fig:log_spectrogram}
\end{figure}

\subsubsection{Constant-Q Transforms}

An additional way in which the spectrograms we have discussed so far do not match perception is that they are linear in frequency (as defined by \cref{eq:bin_freq}).
This is in contrast to our roughly logarithmic perception of pitch, as well as the spacing of semitones in the common Western music scale.
As a result, it can be beneficial to use a time-frequency representation which is also logarithmic in frequency.
A common choice is a constant-Q transform (CQT), which has frequency bins which are logarithmically spaced and all have equal center frequencies-to-bandwidth ratios \cite{brown1991calculation}.
Fortunately, like the DFT, fast algorithms exist for computing the CQT \cite{brown1992efficient,schorkhuber2010constant}.

In this thesis, we will utilize the transform proposed by \cite{schorkhuber2010constant}, which is defined as follows:
\begin{equation}
        S[n, k] = \frac{1}{M_k} \sum_{m = 1}^{M_k} w_k[m]x[m - M_k/2 + nR] e^{2\pi \jmath m f_k/f_s}
\end{equation}
where $f_k$ is the center frequency of each of the $k$ bins, $R$ is the hop size, $w_k \in \mathbb{R}^{M_k}$ is a window function, $f_s$ is the sampling frequency, and
\begin{equation}
        M_k = \frac{f_s}{f_k (\sqrt[B]{2} - 1)}
\end{equation}
where $B$ is the number of frequency bins per octave.
For music signals, it is common to set $B$ to 12 because there are 12 semitones per octave in the Western music scale and to set $f_k$ to a sequence of subsequent semitone frequencies.
Note that there is a different window size for each frequency bin; this is what allows the center frequency-to-bandwidth ratio to remain constant across bins.
The choice of $w_k$ depends on the desired time-frequency resolution trade-off.
In this thesis, we set $w_k$ to the length-$M_k$ Hann window, which provides a good balance for analysis tasks.

As with the DFT, the CQT produces complex-valued coefficients, and a more more perceptually accurate transform can be obtained by computing a log-magnitude CQT.
For comparison, the log-magnitude CQT of the audio signal in \cref{fig:audio_signal} is shown in \cref{fig:cqt}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-cqt.pdf}
  \caption[Log-magnitude constant-Q spectrogram of the acoustic guitar recording]{Log-magnitude constant-Q spectrogram of the audio recording of an acoustic guitar shown in \cref{fig:audio_signal}.
  The constant-Q transforms are computed over logarithmically-spaced bins from C2 to B5 (65.4 to 987.8 Hz), with 12 bins per octave.}
  \label{fig:cqt}
\end{figure}

\subsection{Dynamic Time Warping}
\label{sec:dtw}

A common task in signal processing is measuring the similarity between two signalssequences of feature vectors.
For example, given a collection of recorded speech utterances with word labels, we could perform rudimentary speech recognition by computing the similarity between a query utterance and all database entries and choosing the word corresponding to the most similar entry.
In many domains, time-varying differences in phase can complicate direct comparison of sequences.
For word utterances, this is realized by different speakers speaking different parts of words more quickly or slowly.
As a result, it is often effective to use a warping similarity measure which allows for non-co-occuring sequence elements to be compared.

One such measure which will see extensive use in this thesis is Dynamic Time Warping (DTW).
First proposed for in our exemplary domain of comparing speech utterances \cite{sakoe1978dynamic}, DTW uses dynamic programming to find a monotonic alignment such that the sum of a distance-like cost between aligned feature vectors is minimized.
Because DTW produces an optimal alignment, it is also useful in tasks where the actual correspondense between sequence elements is required, as in the problem of audio-to-MIDI alignment explored in \cref{ch:dtw}.
In this section, we give an overview of DTW; a more thorough treatment can be found in \cite{muller2007dynamic}.

Suppose we are given two sequences of feature vectors $X \in \mathbb{R}^{M \times D}$ and $Y \in \mathbb{R}^{N \times D}$, where $D$ is the feature dimensionality and $M$ and $N$ are the number of feature vectors in $X$ and $Y$ respectively.
DTW produces two nondecreasing sequences $p, q \in \mathbb{N}^L$ which define the optimal alignment between $X$ and $Y$, such that $p[i] = n, q[i] = m$ implies that the $m$th feature vector in $X$ should be aligned to the $n$th in $Y$.
Finding $p$ and $q$ involves globally solving the following minimization problem:
\begin{equation}
p, q = \mathrm{arg}\min_{p, q} \sum_{i = 1}^{L} \|X[p[i]] - Y[q[i]]\|_2
\label{eq:dtw_objective}
\end{equation}
The constraint of monotonicity is enforced by only allowing certain ``step patterns'', the simplest and most common being
\begin{align}
                      & p[i + 1] = p[i] + 1, q[i + 1] = q[i] + 1 \\
        \label{eq:diagonal}
        \mathrm{or\;} & p[i + 1] = p[i] + 1, q[i + 1] = q[i] \\
        \mathrm{or\;} & p[i + 1] = p[i], q[i + 1] = q[i] + 1
\end{align}
although many others have been proposed \cite{muller2007dynamic, sakoe1978dynamic}.
This minimization can be solved in $\mathcal{O}(MN)$ time using dynamic programming \cite{sakoe1978dynamic}.
A common equivalent way to frame this problem which fits straightforwardly into the framework of dynamic programming is to construct a pairwise distance matrix $C \in \mathbb{R}^{M \times N}$ by computing $C[i, j] = \|X[i], Y[i]\|_2$ and then finding the lowest-cost path (subject to appropriate constraints) through the matrix.
This perspective prompts path entries where $p[i + 1] = p[i] + 1, q[i + 1] = q[i] + 1$ to be reffered to as ``diagonal moves''.

DTW is often constrained so that $p$ and $q$ span the entirety of $X$ and $Y$, i.e.\ $p[1] = q[1] = 1$ and $p[L] = N; q[L] = M$.
However, this constraint is inapplicable when subsequence alignment is allowed in which case it is required that either $gN \le p[L] \le N$ or $gM \le q[L] \le M$ where $g \in (0, 1]$ (the ``gully'') is a parameter which determines the proportion of the subsequence which must be successfully matched.
In addition, the path is occasionally further constrained so that
$$
q[i] - p[i] + R \le N, p[i] - q[i] + R \le M
$$
for $i \in \{1, \ldots, L\}$ where $R = g\min(M, N)$ is the ``radius'', sometimes called the Sakoe-Chiba band \cite{sakoe1978dynamic}.
An alternative way to encourage the alignment to favor ``diagonal'' paths is to add a penalty to \cref{eq:dtw_objective}:
\begin{equation}
        p, q = \mathrm{arg}\min_{p, q} \sum_{i = 1}^{L} \|X[p[i]] - Y[q[i]]\|_2 + \begin{cases}
                \phi, p[i] = p[i - 1] \mathrm{\;or\;} q[i] = q[i - 1]\\
                0, \mathrm{otherwise}
        \end{cases}
\end{equation}
where $\phi$ is a tunable parameter controlling how much to discourage non-diagonal path entries.

Once an optimal alignment path is found, a rough similarity measure can be obtained as the sum of the pairwise distances between aligned sequence elements from $X$ and $Y$.
In addition, to achieve alignment one sequence can be adjusted (via resampling or otherwise) so that so that $t_X[p[i]] = t_Y[q[i]], \forall i \in \{1, \ldots, L\}$, where $t_X \in \mathbb{R}^M, t_Y \in \mathbb{R}^N$ are the times corresponding to the feature vectors in $X$ and $Y$ respectively.
An example visualization of DTW on two one-dimensional sequences can be seen in \cref{fig:example_dtw}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/2-example_dtw_sequences.pdf}

  \includegraphics[width=.66\textwidth]{figures/2-example_dtw_matrix.pdf}
  \caption[DTW alignment of two signals]{Example DTW alignment between two one-dimensional sequences.  The top figure shows the determined correspondences between elements of the two sequences as dashed lines.  The lower figure shows the pairwise distance matrix $C$ and the lowest-cost path through it, which defines the optimal alignment.  The x- and y-axes of the distance matrix correspond to sequence indices in the green and blue sequences respectively.}
  \label{fig:example_dtw}
\end{figure}
