\chapter{Tools} \label{ch:tools}

Throughout this thesis, we will use a common collection of tools to help solve problems of interest.
Broadly speaking, we will make use of techniques from the fields of machine learning and signal processing.
In this chapter, we give a high-level overview of these fields and a more specific definition of the various techniques we will utilize.

\section{Machine Learning}

Machine learning is broadly defined as a suite of methods for enabling computers to carry out particular tasks without being explicitly programmed to do so.
Such methods make use of a collection of data, from which patterns or the desired behaviors are automatically determined.
In this sense, machine learning techniques allow computers to ``learn'' the correct procedure based on the data provided.
In addition to data, machine learning algorithms also often require an objective which quantifies the extent to which they are successfully carrying out a task.
This objective function allows them to be optimized in order to maximize their performance.

Traditionally, machine learning techniques are divided into three types: Supervised, unsupervised, and reinforcement learning.
Supervised learning requires a collection of training data that specifies both the input to the algorithm and the desired output.
Unsupervised learning, on the other hand, does not utilize target output data, and thus is limited to finding structure in the input data.
Finally, reinforcement learning refers to the broader problem of determining a policy to respond to input data in a dynamic ``environment'' based on a potentially rare signal which tells the algorithm whether it is successful or not.
All of the problems in this thesis fall into the category of supervised learing, so we will not discuss unsupervised or reinforcement learning further.

\subsection{Supervised Learning}

In the supervised setting, our machine learning algorithm produces a function $f$ (called a {\em model}) which, given input $x$ and the function's parameters $\theta$, produces an output $\hat{y}$:

\begin{equation}
\hat{y} = f(x, \theta)
\end{equation}

Supervised learning requires a training set $\mathcal{S}$ of input-target pairs $(x_1, y_1), (x_2, y_2), \ldots (x_{|\mathcal{S}|}, y_{|\mathcal{S}|})$, where $x_n \in \mathcal{X}$ and $y_n \in \mathcal{Y}$.
We assume that these pairs are drawn from an unknown joint probability distribution $p(x, y)$, of which the only information we explicitly know is the training set $\mathcal{S}$.
The goal of a supervised learning algorithm is to capture the probability distribution $p(x, y)$ with $f$ by adjusting the parameters $\theta$ based solely on $\mathcal{S}$.

One way to formulate this goal is to state that for a sample from $p(x, y)$ which is not in $\mathcal{S}$, the function $f$ should produce the correct output.
Of course, because we are only given the samples in $\mathcal{S}$, the parameters $\theta$ must be optimized solely according the training pairs in $\mathcal{S}$.
This approach, where $f$ is used directly to produce the predicted value for $y$, is referred to as discriminant training.
An alternative approach, called probabilistic training, is to use $f$ to model $p(x, y)$ by estimating $p(y | x)$ and then choosing the most probable $y$.
In either case, choosing $f$ typically involves using a task-specific ``loss function'' $\mathcal{L}$ which computes a nonnegative scalar value which measures the extent to which $\hat{y}$ agress with $y$.
Ideally, minimizing this loss over the training set by adjusting $\theta$ will yield an $f$ which closely approximates $p(x, y)$.

The choice of a loss function will depend on the characteristics of the problem, in particular the target space $\mathcal{Y}$.
For example, in {\em classification} problems, the targets $y$ may take one of $K$ discrete values.
A natural loss function in this case is the zero-one loss

\begin{equation}
\mathcal{L}(y, \hat{y}) = \begin{cases}
1,& \hat{y} = y\\
0,& \hat{y} \ne y
\end{cases}
\end{equation}

In {\em regression} problems, the targets are continuously valued, and a common loss function is the squared error

\begin{equation}
\mathcal{L}(y, \hat{y}) = (y - \hat{y})^2
\end{equation}

Note that in both cases when $y = \hat{y}$, $\mathcal{L} = 0$; this is a common property of loss functions.

In general, a suitable goal for training an unsupervised learning algorithm is to adjust $\theta$ to minimize the average loss over the training set:

\begin{equation}
\frac{1}{|\mathcal{S}|} \sum_{(x, y) \in \mathcal{S}} \mathcal{L}(y, f(x, \theta))
\end{equation}

Minimizing this average loss is equivalent to minimizing the empirical risk of $f$.
A common paradigm is to compute the derivative of the loss function $\mathcal{L}$ with respect to the model parameters $\theta$ and use this derivative information to incrementally change $\theta$ to make $\mathcal{L}$ smaller.
In this case, it is beneficial to have a loss function which is continuous and smooth, which precludes the use of loss functions like the zero-one loss.
A common solution to this problem is to use a surrogate loss function which has the desired properties (e.g.\ smoothness) and whose minimization will also minimize $\mathcal{L}$.

There are supervised learning methods which do not follow the above framework.
For example, the k-nearest neighbors algorithm is a classification algorithm which labels its input by assigning it the most common $y$ among the $k$ points in $\mathcal{S}$ which are closest to the input.
This classifier has no loss function or indeed any parameters other than $k$ and $\mathcal{S}$ itself (i.e.\ it is nonparametric).
However, all of the methods we will utilize in this thesis follow the recipe of defining a model with parameters $\theta$ which are adjusted by miniimzing the average loss over $\mathcal{S}$.

\subsection{Sequential Data}

A common scenario is that the data space $\mathcal{X}$ is fixed-dimensional, e.g.\ $\mathcal{X} \in \mathbb{R}^{D_1 \times D_2 \times \cdots D_N}$.
However, in some cases, one (or more) dimensions may vary in size.
An important example of this is sequential data, where a ``temporal'' dimension can differ between examples.
In addition, for real-world data the temporal dimension often contains strong dependencies and correlations.
For some examples of common types of data typically represented as sequences, see \cref{fig:example_sequences}.

An illustrative example is text data where each datapoint is a sentence.
Clearly, the length of each sequence will vary (e.g.\ ``I am happy.'' and ``This sentence is a few words longer.'' differ in length).
In addition, there may be a strong dependency between a given word in a sentence and the words that precede it.
For example, ``I am hungry, please give me'' is much more likely to be followed by the word ``food'' than, say, ``dirt''.

Different possible tasks involving sequential data can be broadly divided into three categories: Sequence classification, embedding, and transduction.
Sequence classification involves applying a single label to entire sequences; for example, classifying which of a finite number of people was speaking in an audio recording of an utterance.
Sequence embedding involves representing sequences as a single point in a fixed-dimensional space with some desired properties.
For example, instead of classifying which person was speaking, we might instead embed audio recordings as vectors in a Euclidean space such that recordings of the same speaker have a small embedded distance and recordings of different speakers have a large distance.
Finally, sequence transduction involves converting an input sequence to a new sequence, potentially of different length and whose elements may lie in a different space.
Speech recognition is an exemplary case, where a sequence of discrete words must be generated based on an audio recording.

\subsection{Neural Networks}

Most of the machine learning algorithms used in this thesis will come from a class of models known as ``neural netorks''.
Neural networks can be considered learnable functions which consist of a sequence of nonlinear processing stages.
First proposed in the XXXXs, these models were designed as an artificial model of the way the brain was understood to work at that time.
The modern understanding of the brain has diverged substantially from this model; nevertheless, the name ``neural network'' has endured.

A predecessor to modern neural network models was the perceptron algorithm.
The perceptron is a simple linear model which can perform binary classification of its input.
Given an input feature vector $x \in \mathbb{R}^D$, the perceptron computes

\begin{equation}
f(x) = \begin{cases}
1,& w^\top x + b > 0\\
0,& \mathrm{otherwise}
\end{cases}
\end{equation}

where $w \in \mathbb{R}^D$ and $b \in \mathbb{R}$ are the parameters of the model.
These parameters are adjusted so that the desired label ($1$ or $0$) is produced given different inputs.

The perceptron algorithm can only correctly classify its input when the data which falls into the two classes is linearly separable by class.
To remedy this, the ``multilayer perceptron'' was proposed, which can be viewed as a sequence of perceptrons organized in ``layers'', each taking its input from the previous perceptron.
In the multilayer perceptron, each perceptron can output a vector of values rather a single value.
Furthermore, rather than producing a binary 1 or 0 depending on the sign of $w^\top x + b$, the layers in a multilayer perceptron may use any of a variety of nonlinear functions.
In summary, a given layer's output is computed as
\begin{equation}
f(x) = \sigma(W x + b)
\end{equation}
where $\sigma$ is a nonlinear ``activation'' function, $W \in \mathbb{R}^{M \times D}$ is the weight matrix, $b \in \mathbb{R}^M$ is the bias vector, and $M$ is the output dimensionality of the layer.
The original perceptron is recovered when $\sigma$ is the Heaviside step function and $M = 1$.
Under this generic definition, multilayer perceptrons are, in fact, equivalent to what are now called feedforward networks; we will use this terminology for the remainder of this thesis.

In a seminal paper, it was shown that feedforward networks with at least two layers are {\em universal function approximators} when the activation function of each layer is a ``squashing'' function (i.e.\ is monotonically increasing, $\lim_{x \rightarrow \infty} \sigma(x) = 1$ and $\lim_{x \rightarrow -\infty} \sigma(x) = 0$).
This implies that such models can approximate any function to arbitrary precision.
Similar results were later demonstrated for other classes of activation functions.

Of course, the ability of a model to approximate any function is only valuable when there is a reliable method to adjust its parameters so that the desired function is suitably approximated.
Currently, the most pervasive method to achieve this is ``backpropagation'', which computes the gradient of an error function which measures the deviation between the network's output and the target output with respect to the model parameters.
We give a derivation of backpropagation for feedforward networks in the following section.
In the supervised learning context, the error function computes the deviation between the network's output and the target for input-target pairs in the training set $\mathcal{S}$.
As a result, for supervised learning, universal approximation only implies that the multilayer feedforward network can ``memorize'' the target output for each input in $\mathcal{S}$, not that it will accurately approximate the data's underlying joint probability distribution $p(x, y)$.
This is referred to as ``overfitting'', and is an undesirable property of machine learning models.

Despite this issue, feedforward networks have proven to be extremely effective models in a variety of machine learning tasks.
This success is thanks to a suite of methods and model designs which allow these models to be trained effectively, over potentially very large datasets, while avoiding overfitting.
It has also led to the development of variants which are more appropriate for sequential data (recurrent networks) and data with local regularities (convolutional networks).
In the following sections, we give an overview of these different developments and approaches.

\subsubsection{Backpropagation}

Backpropagation is a method for efficiently computing the gradient of a loss function applied to a neural network with respect to its parameters.
These partial derivatives can then be used to update the network's parameters using gradient descent methods.
In truth, backpropagation is a special case of the more generic ``reverse-mode automatic differentiation'' method.
Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.

While originally introduced earlier, the wide-spread adoption of backpropagation for training neural networks is largely thanks to Rumelhart et.\ al's seminal paper \cite{}.
Backpropagation is so successful and widely applied that in some sense the term ``neural network'' has become genericized to mean ``a machine learning model for which backpropagation is applicable''.
Because it is fundamental in many of the approaches proposed in this thesis, we give a full derivation of backpropagation for a simple multilayer feedforward network in this section.
A similar, higher-level and more verbose derivation can be found at \cite{}.

By way of review, the chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.
If $\mathcal{L}$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial \mathcal{L}}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
For scalar-valued functions of more than one variable (e.g.\ a scalar-valued function of a vector), the chain rule essentially becomes additive.
In other words, if $L$ is a scalar-valued function of a vector $z \in \mathbb{R}^N$, whose dimensions are each a function of some variable $w$, the chain rule states that
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w} = \sum_{i = 1}^N \frac{\partial \mathcal{L}}{\partial z[i]}\frac{\partial z[i]}{\partial w}
\end{equation}

In the following, we'll discuss backpropagation through an $L$-layer feedforward network, where $N_n$ is dimensionality of layer $n \in \{0, \ldots, L\}$.
$N_0$ is the dimensionality of the input; $N_L$ is the dimensionality of the output.
The $m^{th}$ layer, for $m \in \{1, \ldots, L\}$ has parameters $W_m \in \mathbb{R}^{N_m \times N_{m - 1}}$, the weight matrix, and $b_m \in \mathbb{R}^{N_m}$, the bias vector.
$W_m[i, j]$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.
Each layer computes $z_m \in \mathbb{R}^{N_m}$, a linear mix of its inputs, by $z_m = W_m a_{m - 1} + b_m$, and the activation $a_m = \sigma_m(z_m)$ where $\sigma_m$ is the layer's nonlinear activation function.
$a_L$ is the output of the network.
We define the special case $a_0$ as the input of the network.
We denote $y \in \mathbb{R}^{N_L}$ as the target output, treated as a constant.
The loss function $\mathcal{L}(a_L, y)$ is used to measure the error of the network's output compared to the target.

In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the loss function $\mathcal{L}$; that is, we need to know $\partial \mathcal{L}/\partial W_m$ and $\partial \mathcal{L}/\partial b_m$.
It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture: $\partial \mathcal{L}/\partial a_L$, the derivative of the loss function with respect to the output of the network, and $\partial a_m/\partial z_m$, the derivative of the nonlinearity used in layer $m$ with respect to its argument $z_m$.

To compute the gradient of our loss function $\mathcal{L}$ with respect to $W_m[i, j]$ (a single entry in the weight matrix of the layer $m$), we can first note that $\mathcal{L}$ is a function of $a_L$, which is itself a function of the linear mix output entries $z_m[k]$, which are themselves functions of the weight matrices $W_m$ and biases $b_m$.
With this in mind, we can use the chain rule as follows:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m[i, j]} = \sum_{k = 1}^{N_m} \frac{\partial \mathcal{L}}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial W_m[i, j]}
\label{eq:L_wrt_W_m}
\end{equation}
Note that by definition
\begin{equation}
z_m[k] = \sum_{l = 1}^{N_m} W_m[k, l] a_{m - 1}[l] + b_m[k]
\end{equation}
It follows that $\partial z_m[k]/\partial W_m[i, j]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any elements in $W_m$ except for those in the $k^{th}$ row, and we are only considering the entry $W_m[i, j]$.
When $k = i$, we have
\begin{align}
\frac{\partial z_m[i]}{\partial W_m[i, j]} &= \frac{\partial}{\partial W_m[i, j]}\left(\sum_{l = 1}^{N_m} W_m[i, l] a_{m - 1}[l] + b_m[i]\right)\\
                                           &= a_{m - 1}[j]\\
\rightarrow \frac{\partial z_m[k]}{\partial W_m[i, j]} &= \begin{cases}
0 & k \ne i\\
a_{m - 1}[j] & k = i
\end{cases}
\label{eq:z_m_wrt_W_m}
\end{align}

Combining \cref{eq:z_m_wrt_W_m} with \cref{eq:L_wrt_W_m} causes the summation to collapse, giving
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m[i, j]} = \frac{\partial \mathcal{L}}{\partial z_m[i]} a_{m - 1}[j]
\end{equation}
or in vector form
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_m} = \frac{\partial \mathcal{L}}{\partial z_m} a_{m - 1}^\top
\end{equation}

Similarly for the bias variables $b_m$, we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m[i]} = \sum_{k = 1}^{N_m} \frac{\partial \mathcal{L}}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial b_m[i]}
\label{eq:L_wrt_b_m}
\end{equation}

As above, it follows that $\partial z_m[k]/\partial b_m[i]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any element in $b_m$ except $b_m[k]$.  When $k = i$, we have

\begin{align}
\frac{\partial z_m[i]}{\partial b_m[i]} &= \frac{\partial}{\partial b_m[i]}\left(\sum_{l = 1}^{N_m} W_m[il] a_{m - 1}[l] + b_m[i]\right)\\
                                        &= 1\\
\rightarrow \frac{\partial z_m[k]}{\partial b_m[i]} &= \begin{cases}
0 & k \ne i\\
1 & k = i
\end{cases}
\label{eq:z_m_wrt_b_m}
\end{align}

Combining \cref{eq:z_m_wrt_b_m} and \cref{eq:L_wrt_b_m} again causes the summation to collapse to give
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m[i]} = \frac{\partial \mathcal{L}}{\partial z_m[i]}
\end{equation}

or in vector form
\begin{equation}
\frac{\partial \mathcal{L}}{\partial b_m} = \frac{\partial \mathcal{L}}{\partial z_m}
\end{equation}

Now, we must compute $\partial \mathcal{L}/\partial z_m[k]$.
For the final layer ($m = L$), this term is straightforward to compute using the chain rule:

\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_L[k]} = \frac{\partial \mathcal{L}}{\partial a_L[k]} \frac{\partial a_L[k]}{\partial z_L[k]}
\end{equation}

or, in vector form

\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_L} = \frac{\partial \mathcal{L}}{\partial a_L} \frac{\partial a_L}{\partial z_L}
\end{equation}

The first term $\partial \mathcal{L}/\partial a_L$ is just the derivative of the loss function with respect to its argument, whose form depends on the loss function chosen.
Similarly, $\partial a_m/\partial z_m$ (for any layer $m$ includling $L$) is the derivative of layer $m$'s nonlinearity with respect to its argument and will depend on the choice of nonlinearity.
For other layers, we again invoke the chain rule:

\begin{align}
        \frac{\partial \mathcal{L}}{\partial z_m[k]} &= \frac{\partial \mathcal{L}}{\partial a_m[k]} \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\frac{\partial z_{m + 1}[l]}{\partial a_m[k]}\right)\frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\frac{\partial}{\partial a_m[k]} \left(\sum_{h = 1}^{N_m} W_{m + 1}[l, h] a_m[h] + b_{m + 1}[l]\right)\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]} W_{m + 1}[l, k]\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                                     &= \left(\sum_{l = 1}^{N_{m + 1}}W_{m + 1}^\top[k, l] \frac{\partial \mathcal{L}}{\partial z_{m + 1}[l]}\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
\end{align}

where the last simplification was made because by convention $\partial \mathcal{L}/\partial z_{m + 1}$ is a column vector, allowing us to write the following vector form:
\begin{align}
        \frac{\partial \mathcal{L}}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial \mathcal{L}}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\label{eq:L_wrt_z_m}
\end{align}
where $\circ$ is the element-wise (Hadamard) product.

We now have the ingredients to efficiently compute the gradient of the loss function with respect to the network's parameters: First, we compute $\partial \mathcal{L}/\partial z_L$ based on the choice of loss function and $\partial a_m/\partial z_m$ based on each layer's nonlinearity.
Then, we recursively can compute $\partial \mathcal{L}/\partial z_m$ layer-by-layer based on the term $\partial \mathcal{L}/\partial z_{m + 1}$ computed from the previous layer (this is called the ``backward pass'').
Finally, the resulting $\partial \mathcal{L}/\partial z_m$ terms are then used to compute $\partial \mathcal{L}/\partial W_m$ and $\partial \mathcal{L}/\partial b_m$ using \cref{eq:L_wrt_W_m} and \cref{eq:L_wrt_b_m} respectively.
Using this recipe requires $\partial \mathcal{L}/\partial z_L$ and $\partial a_m/\partial z_m$ to be known ahead of time.
By way of example, \cref{tab:nonlinearities} and \cref{tab:loss_functions} list some common loss functions and nonlinearities and their appropriate derivatives.

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Nonlinearity & $a_m$                                            & $\frac{\partial a_m}{\partial z_m}$                   \\
  \midrule
  Logistic     & $\dfrac{1}{1 + e^{z_m}}$                         & $a_m(1 - a_m)$                                        \\[1.2ex]
  $\tanh$      & $\dfrac{e^{z_m} - e^{-z_m}}{e^{z_m} + e^{-z_m}}$ & $1 - (a_m)^2$                                         \\[1.2ex]
  Rectifier    & $\max(0, z_m)$                                   & $\begin{cases}0,& z_m < 0\\ 1,& z_m \ge 0\end{cases}$ \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common nonlinearities used in neural networks]{Common nonlinearities $\sigma_m(z_m)$ used in neural networks and their derivatives with respect to their input $z_m$.}
  \label{tab:nonlinearities}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Loss Function & $\mathcal{L}(a_L, y)$                           & $\frac{\partial \mathcal{L}}{\partial a^L}$ \\
  \midrule
  Squared Error & $\dfrac{1}{2}(y - a^L)^\top(y - a^L)$ & $y - a^L$                         \\
  Cross-Entropy & $(y - 1)\log(1 - a^L) - y\log(a^L)$   & $\dfrac{a^L - y}{a^L(1 - a^L)}$   \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common loss functions used in neural networks]{Common loss (loss) functions $\mathcal{L}(a_L, y)$ used in neural networks and their derivatives with respect to their input $a_L$.}
  \label{tab:loss_functions}
\end{table}

In summary, backpropagation proceeds in the following manner for each training sample: For the ``forward pass'', given the network input $a_0$, compute $a_m$ for $m \in \{1, \ldots, L\}$ recursively by $a_m = \sigma_m(W_m a_{m - 1} + b_m)$.
For the ''backward pass'', compute
\begin{equation}
\frac{\partial \mathcal{L}}{\partial z_L} = \frac{\partial \mathcal{L}}{\partial a_L} \frac{\partial a_L}{\partial z_L}
\end{equation}
whose terms are all known a priori, then recursively compute
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial \mathcal{L}}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\end{equation}
and use the resulting values to compute
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial W_m} = \frac{\partial \mathcal{L}}{\partial z_m} a_{m - 1}^\top
\end{equation}
and
\begin{equation}
        \frac{\partial \mathcal{L}}{\partial b_m} = \frac{\partial \mathcal{L}}{\partial z_m}
\end{equation}

From this procedure, we can see why backpropagation is an efficient way to compute these partial derivatives: By reusing terms, all of the partial derivatives can be computed by a single forward and backward pass.
For a feedforward network, the computational loss of each of these passes is effectively $L$ matrix multiplications.
A simple alternative would be to measure the change in $\mathcal{L}$ by adjusting each of the parameters (i.e.\ individual elements of the weight matrices and bias vectors) by a small amount.
However, this would require a separate forward pass for each parameter.
Modern neural networks contain millions of parameters, so clearly backpropagation is an efficient choice.

\subsubsection{Tricks}

Combined with a gradient descent technique, backpropagation provides an effective method for optimizing the parameters of a neural network (``training'') to minimize a given loss function.
However, training neural networks is made difficult in practice by a variety of factors.
First, the composition of the many nonlinear processing layers in a neural network make the resulting loss function highly non-convex.
Nonconvexity means that, when minimizing the network's loss function with gradient descent methods, there is no guarantee that a given stationary point is a global minimum.
It also implies that both the way the parameters are initialized prior to optimization and the gradient descent technique used may have a very strong effect on the best solution found during optimization.

As mentioned above, the universal approximation property of neural networks can also have negative implications for their utility.
The fact that a possible outcome of training a neural network is the simple memorization of correspondences in the training set means that the resulting model may be of little use in practice.
As a result, in many cases it is necessary to regularize the network to prevent it from overfitting to the training set.

Finally, feedforward networks with many layers are also prone to the problem of ``vanishing and exploding gradients''.
This issue is caused by the fact that as gradients are backpropagated through layers, they are multiplied by each successive weight matrix and the derivative of each nonlinearity (as shown in \cref{eq:L_wrt_z_m}).
When the spectral norm of the weight matrices and/or the gradients of the nonlinearities are substantially greater than or less than one, the resulting backpropagated gradients can become exponentially smaller or larger as they approach ``early'' layers in the network.
This can effectively prevent the parameters of the first few layers from being adjusted, which can greatly inhibit learning.

Fortunately, there has been a great deal of development into methods for mitigating these issues.
Collectively, these techniques have been dubbed ``deep learning'' thanks to their applicability to networks with many layers.
In this section, we motivate and describe those techniques which are used in this thesis.
Practical and broader overviews can be found in \cite{bengio} \cite{lecun} \cite{nature}.

\paragraph{Number and Size of Layers}

When designing a neural network model, one of the first choices which must be made is the number of layers and their sizes (output dimensionality).
Because early theory proved that networks with a single layer could approximate any function arbitrarily well given that the layer was sufficiently ``wide'' (high-dimensional) \cite{}, and because networks with many layers can be more difficult to train due to the vanishing and exploding gradients problem \cite{}, early applications of neural networks typically only used a single hidden layer.
However, recent results have shown that using many layers can help networks learn a hierarchical transformation to more and more abstract representations which, for real-world problems, can be highly beneficial \cite{}.
Coupled with recent advances in training very deep networks \cite{}, the paradigm of ``deep'' networks (having many layers) has come to dominate.

Regardless of depth, the dimensionality of each intermediate representation (the ``layer sizes'') must be chosen.
While increasing the dimensionality may increase modeling power, it also exacerbates overfitting and incurs a greater computational loss.
However, Graphics Processing Units (GPUs) have recently been used to accelerate neural network models thanks to their ability to quickly perform large matrix multiplications.
As a result, in modern practice high-dimensional layers are used in combination with regularization techniques \cite{}.

\paragraph{Nonlinearities}

A fundamental ingredient of neural networks is the elementwise nonlinearity applied at each layer; without them, the network would only be able to learn linear relationships between its input and output \cite{}.
Early work typically used sigmoidal nonlinearities such as the logistic or hyperbolic tangent functions (see \cref{tab:nonlinearities}).
However, these nonlinearities have a very small gradient for much of their domain which can cause vanishing gradient problems particularly for networks with many layers.
Recently, the rectifier nonlinearity has become the standard for deep networks due to its computational efficiency and tendency to produce sparse representations \cite{}.
The rectifier nonlinearity has zero gradient for half of its domain, which can hinder training in some cases; as a result a number of variants have recently been proposed which have been shown empirically to produce better results in some cases \cite{}.

\paragraph{Regularization}

Due to their tendency to overfit, some form of regularization is typically necessary to ensure that the optimized neural network's variance is not too high.
A common regularizer which is used in many machine learning models is to include in the loss function a penalty on the norm of the model's parameters.
In neural networks, adding the $L^2$ penalty $\lambda \sum_i \theta_i^2$ where $\lambda$ is a hyperparameter and $\theta_i$ are the model's parameters (e.g. individual entries of the weight matrices and bias vectors) is referred to as ``weight decay''.
This can encourge the weight matrix of a given layer from focusing too heavily (via a very large weight value) on a single dimension of its input.
A related term is $\lambda \sum_i |\theta_i|$ which effectively encourages parameter values to be zero.

A completely different regularization method which has recently proven popular in neural networks is ``dropout'' \cite{}.
In dropout, at each training iteration each dimension of each layer is randomly set to zero with probability $p$.
After training, the weights in each layer are scaled by $1/p$.
Dropout intends to prevent the dimensions of the output of a given layer from being too heavily dependent (correlated) with one of the inputs by randomly artificially removing connections.
More simply, it provides a source of noise which prevents memorization of correspondences in the training set and has repeatedly been shown empirically to be an effective regularizer.

In practice, the technique of ``early stopping'' is almost always used to avoid overfitting in neural network models.
To utilize early stopping, during training the performance on a held-out ``validation set'' (over which the parameters of the network are not optimized) is computed.
Overfitting is indicated by performance degrading on the validation set, which simulates real-world performance.
As a result, early stopping effectively prevents overfitting by simply stopping training once the performance begins to degrade.
The measure of performance and criteria for stopping my vary widely from task to task (and practitioner to practitioner) but the straightforwardness and effectiveness of this approach has led it to be nearly universally applied.

Of course, regularization can be rendered unnecessary when a huge amount of training data is available.
This is commonly simulated by applying hand-designed realistic deformations to the training data, which is referred to as ``data augmentation''.
For example, applying elastic distortions to images can allow simple, unregularized neural network models to match the performance of more complex architectures \cite{}.
Data augmentation has also been explored in the domain of audio and music signals \cite{}.

\paragraph{Parameter Initialization}

Because the objective functions used for training neural networks tend to be highly nonconvex, the way parameters are initialized prior to training can have a substantial effect on the solutions found after optimization.
In fact, the recent sucess of neural network models was prompted by a new method for setting parameter values prior to supervised training called ``unsupervised pretraining''.
Unsupervised pretraining starts by training an unsupervised model (e.g.\ a Restricted Boltzmann Machine \cite{} or an autoencoder \cite{}) and then using the parameters from that model to initialize a supervised model.
Ideally, the unsupervised model will take care of disentangling factors of variation and producing a more efficient representation of the input space.
After initialization with values from the unsupervised model, training of the model proceeds in the normal supervised fashion.
This can greatly accelerate learning, and is of particular use when there is a great deal more unlabeled data available than labeled data.

It has more recently been shown that unsupervised pretraining can be skipped (provided that there is a sufficient amount of labeled data) by carefully initializing the network's parameters.
This typically involves sampling the values of the weight matrices from zero-mean Gaussian distributions whose variances are set so that the representation at each layer's output has unit variance.
The actual variances used for the random variables depends on the input and output dimensionality as well as the nonlinearity; \cite{} provides a derivation for linear networks, \cite{} for rectifier networks and \cite{} gives a generic empirical procedure for any network architecture.
An alternative is to use orthogonal matrices \cite{} or to use a sparse initialization so that a fixed number of output dimensions are non-zero \cite{}.


\paragraph{Input Normalization}

Most of the analysis performed on neural networks in order to determine how to facilitate training rely on the assumption that the feature dimensions of the input to the network have zero mean and unit variance.
A necessary step in order to take advantage of many of these tricks is then to standardize the feature dimensions of the input, typically using statistics computed on the training set.
An alternative which can also facilitate convergence is to whiten data using Pricipal Component Analysis before presenting it to the network.

\paragraph{Model Architecture}

Beyond the simple feedforward network model we have focused on thus far, a variety of more specialized models have been proposed which intend to exploit different types of structure present in real-world data.
For example, presenting sequential data one step at a time to a feedforward network assumes temporal independence, which is invalid for most real-world temporal data.
Other types of data, such as images, exhibit local dependencies in their dimensions which are invariant to translation.
Modeling this type of structure can be greatly facilitated by the important choice of model architecture.
We give an overview of the two most relevant architectures for sequential data, recurrent and convolutional networks, in the next two sections.

\subsubsection{Recurrent Networks}

A natural extension to feedforward networks are {\em recurrent} connections, which feed the output of the model back into its input.
The resulting model can be considered a learnable function which at each time step produces an output based on its current input and previous output.
This class of models, called a recurrent networks, are well-suited for sequential data due to their ability to model temporal dependencies.

Training of recurrent networks is typically accomplished using ``backpropagation through time'' (BPTT) \cite{}.
BPTT is a simple extension of backpropagation where, given a sequence of length $T$, the recurrent network is ``unrolled'' over time and is treated as a feedforward network with $T$ layers, each of which corresponds to a single time step.
While BPTT provides a straightforward way to train recurrent networks, this approach can suffer dramatically from vanishing and exploding gradients for large values of $T$ for the same reasons that networks with many layers do \cite{}.
The use of gating architectures \cite{hochreiter1997long,cho2014learning}, sophisticated optimization techniques \cite{martens2011learning,sutskever2013importance}, gradient clipping \cite{pascanu2012difficulty,graves2013generating}, and/or careful initialization \cite{sutskever2013importance,jaegar2012long,mikolov2014learning,le2015simple} can help mitigate this issue and has facilitated the success of RNNs in a variety of fields (see e.g.\ \cite{graves2012supervised} for an overview).
However, these approaches don't {\em solve} the problem of vanishing and exploding gradients for recurrent networks, and as a result these models are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps \cite{martens2011learning,sutskever2013importance,le2015simple,hochreiter1997long,krueger2015regularizing,arjovsky2015unitary}.
Because the representations used for audio data are typically at least thousands of time steps in length, we will not use recurrent networks in this thesis, although we will use them as a point of comparison and motivation in some places.

\subsubsection{Convolutional Networks}

For many types of data (represented as multi-dimensional arrays), one or more of the axes may have an explicit ordering.
Most relevant to this thesis is sequential data, which always has a temporal axis.
Traversing this axis corresponds to the passing of time; if one event occurred before another, it will appear first.
Similarly, images have ``width'' and ``height'' axes, which correspond to left-right and top-bottom traversal.
In many cases, the correlations on these axes are highly localized.
For example, in a text document the probability that a given word appears in a certain place depends much more strongly on the words appearing around it than it does on words which appeared hundreds of words ago.

The ``fully-connected'' feedforward networks we have discussed so far cause the output of each layer to depend on each dimension of the layer's input due to the fact that the weight matrix has an entry which connects each input dimension to each output dimension.
This ignores the structure of data with an ordered axis and may be inefficient when most of the dependencies are local.
The {\em convolutional network} architecture is formulated specifically to take advantage of this common structure.
As their name suggests, rather than utilizing an affine transformation using a fully-connected matrix, layers in these models convolve small filter ``kernels'' across their input.
Networks with this structure were first proposed by Fukushima \cite{} and training them with backpropagation was popularized by LeCun et.\ al \cite{}.
Recently, convolutional networks have provided breakthroughs in a variety of domains, including image classification \cite{}, speech recognition \cite{}, music information retrieval \cite{}, and natural language processing \cite{}.

In the two-dimensional case (as used in this thesis), a convolutional layer takes as input $x \in \mathbb{R}^{M \times I_x \times I_y}$ and outputs $y \in \mathbb{R}^{N \times J_x \times J_y}$ by computing
\begin{equation}
        y[n, j_x, j_y] = \sigma\left(\sum_{m = 1}^M \sum_{k_x = 1}^{K_x} \sum_{k_y = 1}^{K_y} w[n, m, k_x, k_y] x[m, j_x + k_x, j_y + k_y] + b[n]\right)
\label{eq:convolution}
\end{equation}
where $w \in \mathbb{R}^{N \times M \times K_x \times K_y}$ are the layer's filters (also referred to as kernels), $\sigma$ is a nonlinearity, and $b \in \mathbb{R}^N$ are the layer's biases.
The first dimension of $x$ and $y$ is referred to as the ``channels'' and can be considered different views or representations of the data (e.g.\ R, G, and B channels in images or left and right channels in a stereo audio recording).
The remaining dimensions are feature dimensions each of which has an inherent ordering, which is exploited by the translation inherent in the convolution operation.
The exact form of \cref{eq:convolution} depends on a variety of design choices, such as the convolution's stride, whether the convolution is zero-centered, whether biases are shared across input channels, whether a correlation or convolution is computed, and whether the input will be zero-padded.
We forgo a complete enumeration of these differences here and instead refer to \cite{}, which provides a thorough discussion of the arithmetic involved in convolutional networks.

From this definition, we can observe a number of beneficial properties of using convolutional networks.
First, the number of parameters per layer is potentially dramatically decreased: Following the common practice of ``flattening'' dimensions, a fully-connected layer with the same input and output dimensionality would have a weight matrix of shape $NJ_xJ_y \times I_xI_y$, compared with $N \times M \times K_x \times K_y$ with $K_x < J_x \le I_x$ and $K_y < J_y \le I_x$.
This is commonly referred to as ``parameter sharing'', because the same set of weights are applied at different locations in the input rather than using a unique weight for each position.
This property also gives the convolutional layer translational invariance, i.e.\ the ability to detect a certain pattern regardless of where it appears in the image.
A final advantage to using a convolution operation is that the layer can naturally handle inputs whose feature dimensions vary in size - the kernels simply slide over the input regardless of its shape.
This makes convolutional networks well-suited for sequential data whose length varies.

An additional ingredient commonly found in convolutional networks are {\em pooling} layers, which provide additional translational invariance by aggregating their input over small regions.
The most common pooling layer, and the only one used in this thesis, is the max-pooling layer, which outputs the maximum value within each input region.
Given input $x \in \mathbb{R}^{M \times I_x \times I_y}$, a max-pooling layer with stride $p$ produces $y \in \mathbb{R}^{M \times \frac{I_x}{p} \times \frac{I_y}{p}}$ by
\begin{equation}
y[m, j_x, j_y] = \max(x[m, j_x:j_x + p, j_y:j_y + p])
\end{equation}
for $j_x \in 1, \ldots, \frac{I_x}{p}$ and $j_y \in 1, \ldots, \frac{I_y}{p}$.
The value of $p = 2$ is almost uniformly used.
The max-pooling layer can thus be seen as indicating whether a given feature is present or absent over a region of the input.
Because pooling layers effectively downsample their input, they can be used to increase the ``receptive field'' (region of the input effecting a single dimension of the output) of layers in networks consisting of many convolutional and pooling layers.

Finally, in convolutional networks it is common for the last few layers to be standard fully-connected layers.
This gives the output of the network access to the entire input region.
In this framework, the series of convolutional and pooling layers transform and downsample the input, providing appropriate invariances and often learning a hierarchy of higher and higher-level representations \cite{}.
In domains where none of the dimensions of the input vary from sample to sample, all of the channels and dimensions at the output of the convolutional portion of the network are flattened prior to being passed to the fully-connected layers.
When one or more dimensions are variable in size, only the fixed-size dimensions are processed by the fully-connected layers.
For example, in sequences where the sequence length varies, the output of the convolutional layersfor each time step are passed to the fully-connected layers independently.

\subsection{Stochastic Optimization}

In modern practice, the datasets used for training neural networks are typically huge, so optimizing their parameters over the entire training set is often inefficient and sometimes infeasible.
The prevailing solution is to train over a tiny subset of the training set (called a ``minibatch'') which is sampled randomly at each iteration.
This causes the objective to change stochastically at each iteration of optimization.
Beyond practical limitations, using a stochastic objective has also been shown to help avoid stationary points because the objective surface is constantly changing \cite{}.

So far, we have only alluded to the fact that the gradient information produced by backpropagation is used to train a neural network.
This is achieved using gradient descent methods, which specify a formula to adjust the network's parameters to minimize an objective.
In many problems, it is beneficial to leverage second-order gradient information to estimate the local curvature of the objective function during optimization.
However, the number of entries in the Hessian (matrix of second-order derivatives) is quadratic in the number of parameters, which makes computing it infeasible for modern neural network models which typically have millions of parameters.
As a result, neural networks are typically trained using gradient descent methods which only utilize first-order gradient information.

A final difficulty in training neural networks is that changing different parameters can have substantially different effects on the objective value.
For example, even in simple feedforward networks, adjusting a value in the final layer's bias vector may induce a considerable change in typical outputs of the network while changing a value in the first layer's weight matrix may only produce a small effect in a limited number of cases.
This problem can be exacerbated when different structures are used in different parts of the network, e.g.\ convolutional or recurrent layers.
As a result, it can be beneficial to update different parameters by different amounts according to the extent to which they effect the objective value.

The stochastic objective arising from training on minibatches and the requirement of only utilizing first-order gradients prompts the use of a special class of gradient descent methods called stochastic optimization.
Effective stochastic optimization techniques typically include mechanisms for smoothing the updates, which helps mitigate noise from stochasticity, for estimating second-order information, which improves convergence, and for adaptively updating different parameters by different amounts.
In addition, most optimization methods have at least one hyperparameter; good optimization techniques are able to be effective without being particularly sensitive to their hyperparameter values.
In this section, we discuss some of the more popular stochastic optimization methods, particularly those which are used or referenced to in this thesis.
Similar overviews are available in \cite{}, and \cite{} provides a comparison of many of these techniques on a variety of toy optimization problems.

In the following, we will use $\mathcal{L}$ to refer to the loss function being minimized.
$\mathcal{L}$ is typically a function of the target output, the input, and the parameters being optimized; because all but the latter can be treated as a constant, we omit those terms in this section.
In addition, because the parameters change from iteration to iteration we will use $\theta_t$ to denote the parameters of the model at iteration $t$.
Finally, $\nabla \mathcal{L}(\theta_t)$ denotes the gradient of the loss function with respect to the parameters at iteration $t$.

\subsubsection{Stochastic Gradient Descent}

Stochastic gradient descent (SGD) simply updates each parameter by subtracting the gradient of the loss with respect to the parameter, scaled by the ``learning rate'' hyperparameter $\eta$:
\begin{equation}
\theta_{t + 1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
\end{equation}
SGD provides the fundamental ingredient on which other stochastic optimization methods are built.
Provided that $\eta$ is set correctly, SGD will decrease the loss function at each iteration because it is adjusting the parameters in the negative gradient direction.
However, if $\eta$ is too large, SGD can diverge (increase the loss function or oscillate between increasing and decreasing it); if it's too small, it will decrease the objective value very slowly \cite{}.
In addition, when the loss surface is ill-conditioned (i.e.\ the curvature of the loss surface is dramatically different along different parameter axes), convergence can also be extremely slow because $\eta$ must be set small enough so that it does not diverge along any parameter axis.
Nevertheless, SGD can be effective when $\eta$ is chosen correctly; a common heuristic is to choose the largest value of $\eta$ which does not cause divergence by starting with a large $\eta$ and decreasing it by a factor of $3$ until optimization no lonver diverges \cite{}.

\subsubsection{Momentum}

In SGD, the gradient $\nabla \mathcal{L}(\theta_t)$ often changes rapidly at each iteration due to the fact that the loss is being computed over different data.
It can therefore be beneficial to smooth the updates at each iteration.
This can be achieved by re-using the gradient value from the previous iteration, scaled by a ``momentum'' hyperparameter $\mu$, as follows \cite{}:

\begin{align}
v_{t + 1} &= \mu v_t - \eta \nabla \mathcal{L}(\theta_t) \\
\theta_{t + 1} &= \theta_t + v_{t+1}
\end{align}

In addition, it has been argued that including the previous gradient step has the effect of approximating some second-order information about the gradient because directions with low curvature will tend to persist across iterations and accumulate, which can mitigate conditioning issues \cite{}.
In practice, $\mu$ is often set close to $1$; this will cause parameters to change slowly from iteration to iteration.
While early theory and intuition suggested that some of the benefits of momentum are lost in the stochastic setting \cite{}, it can be particularly beneficial in the early stages of learning before the loss function is close to any minima.

\subsubsection{Nesterov's Accelerated Gradient}

In Nesterov's Accelerated Gradient (NAG) \cite{}, the gradient of the loss at each step is computed at $\theta_t + \mu v_t$ instead of $\theta_t$.  The update rules are then as follows:

\begin{align}
v_{t + 1} &= \mu v_t - \eta \nabla\mathcal{L}(\theta_t + \mu v_t) \\
\theta_{t + 1} &= \theta_t + v_{t+1}
\end{align}

In momentum, the parameter update could be written $\theta_{t + 1} = \theta_t + \mu v_t - \eta \nabla \mathcal{L}(\theta_t)$, so NAG effectively computes the gradient at the new parameter location but without considering the gradient term (i.e.\ using the first two terms of this update only).
In practice, this causes NAG to behave more stably than regular momentum in many situations, and so it is often chosen over classical momentum.
A more thorough analysis can be found in \cite{}.

\subsubsection{Adagrad}

Adagrad \cite{} effectively rescales the learning rate for each parameter according to the history of the gradients for that parameter.
This is done by dividing each term in $\nabla \mathcal{L}$ by the square root of the sum of squares of its historical gradient:
\begin{align}
g_{t + 1} &= g_t + \nabla \mathcal{L}(\theta_t)^2 \\
\theta_{t + 1} &= \theta_t - \frac{\eta\nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t + 1}} + \epsilon}
\end{align}
where division is elementwise and $\epsilon$ is a small constant included for numerical stability.
Rescaling in this way effectively lowers the learning rate for parameters which consistently have large gradient values, which can be particularly useful in ill-conditioned settings.
It also effectively decreases the learning rate over time, because the sum of squares will continue to grow with the iteration.
This can prove beneficial as the loss function becomes close to a minimum, where large changes in the loss function would be detrimental.
\cite{} provides a discussion of its theoretical guarantees and empirical benefits.

\subsubsection{RMSProp}

In its originally proposed form \cite{}, RMSProp is very similar to Adagrad.
The only difference is that the $g_t$ term is computed as an exponentially decaying average instead of an accumulated sum.
This makes $g_t$ an estimate of the second moment of $\nabla \mathcal{L}$ and prevents the learning rate from effectively shrinking over time.
The name "RMSProp" comes from the fact that the update step is normalized by a decaying approximate RMS of recent gradients.
The update is as follows:

\begin{align}
g_{t + 1} &= \gamma g_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t)^2 \\
\theta_{t + 1} &= \theta_t - \frac{\eta\nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t + 1}} + \epsilon}
\end{align}

In the original lecture slides where it was proposed, $\gamma$ is set to $.9$, but it is most commonly treated as a variable hyperparameter.
In \cite{}, it is shown that the $\sqrt{g_{t + 1}}$ term approximates (in expectation) the diagonal of the absolute value of the Hessian matrix (assuming the update steps are $\mathcal{N}(0, 1)$ distributed).
It is also argued that the absolute value of the Hessian is better to use for non-convex problems which may have many saddle points.

Alternatively, \cite{} defines RMSProp with an additional first moment approximator $m_t$ and a momentum term $v_t$:
\begin{align}
m_{t + 1} &= \gamma m_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t) \\
g_{t + 1} &= \gamma g_t + (1 - \gamma) \nabla \mathcal{L}(\theta_t)^2 \\
v_{t + 1} &= \mu v_t - \frac{\eta \nabla \mathcal{L}(\theta_t)}{\sqrt{g_{t+1} - m_{t+1}^2 + \epsilon}}  \\
\theta_{t + 1} &= \theta_t + v_{t + 1}
\end{align}
Including the first-order moment estimate in the denominator in this way causes the learning rate to be effectively normalized by the standard deviation of $\nabla \mathcal{L}$ in each direction.


\subsubsection{Adam}

Adam \cite{} is somewhat similar to Adagrad and RMSProp in that it computes a decayed moving average of the gradient and squared gradient (first and second moment estimates) at each time step.
It differs mainly in two ways: First, the separate moving average coefficients $\gamma_1$ and $\gamma_2$ are used for first and second moments respectively (\cref{eq:adam_m} and \cref{eq:adam_g}).
Second, because the first and second moment estimates are initialized to zero, some bias-correction is used to counteract the resulting bias towards zero (\cref{eq:adam_mhat} and \cref{eq:adam_ghat}).
Given hyperparameters $\gamma_1$, $\gamma_2$, $\lambda$, and $\eta$, and setting $m_0 = 0$ and $g_0 = 0$, the update rule is as follows:

\begin{align}
m_{t + 1} &= \gamma_1 m_t + (1 - \gamma_1) \nabla \mathcal{L}(\theta_t) \\
\label{eq:adam_m}
g_{t + 1} &= \gamma_2 g_t + (1 - \gamma_2) \nabla \mathcal{L}(\theta_t)^2 \\
\label{eq:adam_g}
\hat{m}_{t + 1} &= \frac{m_{t + 1}}{1 - \gamma_1^{t + 1}} \\
\label{eq:adam_mhat}
\hat{g}_{t + 1} &= \frac{g_{t + 1}}{1 - \gamma_2^{t + 1}} \\
\label{eq:adam_ghat}
\theta_{t + 1} &= \theta_t - \frac{\eta \hat{m}_{t + 1}}{\sqrt{\hat{g}_{t + 1}} + \epsilon}
\end{align}

The use of the estimated first and second moments ensure that, in most cases, the step size is $\approx \pm \eta$ and that in magnitude it is less than $\eta$.
However, as $\theta_t$ approaches a true minimum, the uncertainty of the gradient will increase and the step size will decrease.
It is also invariant to the scale of the gradients.
These properties make it well-suited for neural network architectures, and as a result has seen rapid adoption.

\subsection{Bayesian Optimization}

From the discussion above, it is clear that beyond the parameters of neural network models which are optimized by gradient descent (weight matrices, bias vectors, etc.), there are additional ``hyperparameters'' which govern specifics of the model architecture, loss function, and training.
These hyperparameters are so called because they are not optimized through gradient descent with the rest of the parameters.
Nevertheless, the success or failure of training a given model will often heavily depend on these hyperparamters being set appropriately.
This problem fits into the general framework of black-box optimization, where we would like to maximize an objective function with tunable parameters which produces different values based on different parameter settings.
In this framework, we assume that we do not have a way to compute an analytical solution or access to gradient information about the function and thus can only make decisions about what parameter settings to try based on the history of function evaluation results for different settings.
For example, the function may be a mapping between hyperparameters governing the structure of a neural network model (number and width of layers, weight initialization scheme, etc.), its loss function (which loss function to use, whether to include regularization), and training procedure (which stochastic optimization technique to use, each of which has its own hyperparamwters such as the learning rate) and the objective value could be the trained model's accuracy on a held-out validation set.
Alternatively, one could consider the process of baking a cake in this framework, where the parameters are the amount and quantity of each ingredient as well as the baking temperature and time and the objective value is how good the cake tastes.

In practice, this type of problem is often tackled by adjusting parameters by hand based on a series of trials of different settings.
This approach can be extremely time consuming and expensive and may produce suboptimal results, so various approaches for automating this process have been proposed.
A popular choice is ``grid search'', which first quantizes any continuously-valued parameters and then exhaustively tries all combinations of all settings.
This approach is only feasible when there are few parameters and possible settings due to the exponential number of trials which must be run.
In addition, it has been shown that grid search is often less efficient than simply randomly trying different parameter values, particularly when the setting of one or more parameters does not have a strong effect on the objective value \cite{}.
While grid search and random search can be effective, neither utilize the results of previous trials to inform subsequent parameter settings.

A valuable alternative is Bayesian Optimization, which utilizes all of the information available in previous trials to produce a probabilistic model of the black-box function being optimized.
This requires more computation than methods which only use local approximations of the function, but can produce good solutions to highly non-convex functions after making only a small number of function evaluations.
This is particularly valuable when the function itself is expensive to evaluate, as is the case when training machine learning models which take hours, days, or weeks (or when baking cakes, due to the price of ingredients).
The use of a probabilistic model of the black-box function being optimized both allows for candidate parameter settings to be chosen in a principaled way and for prior information to be integrated into the optimization process.
We utilize Bayesian Optimization techniques throughout this thesis for black-box optimization of different algorithms and systems.
This section provides a brief overview of Bayesian Optimization; for more thorough treatments see \cite{}.

\subsubsection{Gaussian Processes}

Utilizing Bayesian Optimization requires first choosing a prior over functions which expresses beliefs about the behavior of the function being optimized.
A common choice are Gaussian processes, which associate every point in the (continuous) parameter space with a normally distributed random variable.
An alternative definition is that every finite collection of points in the parameter space induces a multivariate Gaussian distribution.
In the setting of Bayesian Optimization, this corresponds to assuming that the objective values are characterized by Gaussian distributions.

More formally, given parameter settings $\theta \in \Theta$ where $\Theta$ is the parameter space, a Gaussian process is characterized by a mean function $m : \Theta \rightarrow \mathbb{R}$ and covariance function $k : \Theta \times \Theta \rightarrow \mathbb{R}$.
The mean function is (without loss of generality) typically assumed to be zero, while the choice of the covariance function allows strong assumptions to be made about the function being estimated.
A common choice is the squared exponential kernel
\begin{equation}
        k(\theta_i, \theta_j) = \mathrm{exp}\left(-\frac{1}{2\omega^2}\|\theta_i - \theta_j\|^2\right)
\end{equation}
where $\omega$ is a hyperparameter controlling the width of the kernel.
\cite{} argues that this kernel is too smooth for practical optimization problems and recommends the use of the Mat\'ern 5/2 kernel instead.
In general, the covariance function should be defined so that it approaches $1$ as its arguments get closer together and $0$ as they get farther apart.
The hyperparameters of the covariance function function can either be set according to prior beliefs about the function being modeled, or can be estimated by optimizing the marginal likelihood of the observed function evaluations under the Gaussian process.

Once mean and covariance functions have been chosen, the Gaussian process can be fit to training data and a posterior can be obtained by the following process:
Denoting the $t$ parameter settings and observed objective values as $\theta_T = \{\theta_1, \ldots, \theta_t\}$ and $y_T = \{y_1, \ldots, y_t\}$ respectively and an unseen evaluation of the objective function $y_n$, the joint marginal likelihood of $y_n$ and the observed objective values is given by
\begin{equation}
        p(y_T, y_n) = \mathcal{N}\left(0,
                \begin{bmatrix}
                        K_{T \times T} & K_{n \times T}^\top\\
                        K_{n \times T} & k(\theta_n, \theta_n)\\
        \end{bmatrix}\right)
\end{equation}
where
\begin{align}
        K_{T \times T} &= \begin{bmatrix}
                k(\theta_1, \theta_1) & \cdots & k(\theta_1, \theta_t) \\
                \vdots & \ddots & \vdots \\
                k(\theta_t, \theta_1) & \cdots & k(\theta_t, \theta_t) \\
        \end{bmatrix} \in \mathbb{R}^{t \times t} \\
        K_{n \times T} &= \begin{bmatrix}
                k(\theta_n, \theta_1) & \cdots & k(\theta_n, \theta_t)
        \end{bmatrix} \in \mathbb{R}^{1 \times t}
\end{align}
\begin{equation}
\end{equation}
This can be used to obtain the predictive distribution (see \cite{} for a derivation)
\begin{equation}
        p(y_{n} | y_T, \theta_T, \theta_n) = \mathcal{N}(\mu_T(\theta_n), \sigma_T^2(\theta_n))
\end{equation}
where
\begin{align}
        \mu_T(\theta_n) &= K_{n \times T} K_{T \times T}^{-1} \begin{bmatrix}
                y_1 \\
                \vdots \\
                y_t
        \end{bmatrix} \\
        \sigma_T^2(\theta_n) &= k(\theta_n, \theta_n) - K_{n \times T} K_{T \times T}^{-1} K_{n \times T}^\top
\end{align}

Note that the predictive distribution's covariance matrix will have 1 all along its diagonal because any point is perfectly correlated with itself.
This is an invalid assumption in the presence of noise, so a common modification to include a term $\sigma_{noise}^2$ for the variance of the noise in the covariance, which yields the predictive distribution
\begin{equation}
p(y_{n} | y_T, \theta_T, \theta_n) = \mathcal{N}(\mu_T(\theta_n), \sigma_T^2(\theta_n) + \sigma_{noise}^2)
\end{equation}

\subsubsection{Acquisition Functions}

We now have a recipe for predicting the mean and variance of the objective function's value at any point based on observed values.
In order to perform optimization, we also need a principaled approach for selecting new parameter settings to evaluate the function at, both in hopes of achieving higher objective values and for obtaining a better estimate of the objective function.
This is achieved through acquisition functions, which utilize the predictive distribution to construct a function whose maximum corresponds to the point at which the objective function should be evaluated at next.
Acquisition functions can be considered a proxy function which can be optimized to estimate the objective function's maximum value under certain assumptions.
Importantly, sampling values from acquisition functions is potentially much cheaper than evaluating the objective function at new parameter values, which is one of the core reasons that Bayesian optimization is beneficial.
To facilitate optimization, acquisition functions should appropriately balance exploration and exploitation, i.e.\ the trade-off between reducing uncertainty and finding the absolute best value.

\paragraph{Probability of Improvement}

An intuitive choice is to maximize the probability that a given point is larger than the best observed value, which we denote $y_{best} = \max(y_T)$.
This probability can be computed as
\begin{equation}
        \mathrm{PI}(\theta) = \Phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right)
        \label{eq:probability_of_improvement}
\end{equation}
where $\Phi$ is the normal cumulative distribution function.
In the case that $\sigma_T(\theta) = 0$, $\mathrm{PI}(\theta)$ is set to $0$.
This strategy will always choose the $\theta$ which has the highest probability of yielding a larger objective value regardless of whether this value will provide more information about the function, i.e.\ it is completely explotation-based.
A simple way to mitigate this is to add an exploration trade-off parameter $\xi$ to instead compute
\begin{equation}
        \mathrm{PI}(\theta) = \Phi\left(\frac{\mu_T(\theta) - y_{best} + \xi}{\sigma_T(\theta)}\right)
\end{equation}

\paragraph{Expected Improvement}

Instead of simply choosing the point which is most likely to yield an improvement, we can also consider the expected magnitude of the improvement.
This can be encapsulated using an ``improvement function'' which is positive when the objective value for the predicted value of $\theta$ is larger than $y_{best}$.
Maximizing the expectation of this improvement function has a closed-form solution, yielding
\begin{equation}
        \mathrm{EI}(\theta) = (\mu_T(\theta) - y_{best})\Phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right) + \sigma(\theta)\phi\left(\frac{\mu_T(\theta) - y_{best}}{\sigma_T(\theta)}\right)
\end{equation}
where $\phi$ is the normal probability density function.
\cite{} claims that Expected Improvement yielded the best performance among acquisition functions for hyperparameter optimization of machine learning systems.

\paragraph{Upper Confidence Bound}

An alternative approach, which is also conceptually simple, is to maximize the upper confidence bound
\begin{equation}
        \mathrm{UCB}(\theta) = \mu_T(\theta) + \kappa \sigma_T(\theta)
\end{equation}
$\kappa \ge 0$ is a hyperparameter which scales the confidence limits and therefore balances exploration and exploitation.

\section{Signal Processing}

\subsection{Audio Signals and Psychoacoustics}

proposal1

\subsection{Time-Frequency Analysis}

\subsection{Dynamic Time Warping}
\label{sec:dtw}

A common task in signal processing is measuring the similarity between sequences of feature vectors.
For example, given a collection of recorded speech utterances with word labels, we could perform rudimentary speech recognition by computing the similarity between a query utterance and all database entries and choosing the word corresponding to the most similar entry.
In many domains, time-varying differences in phase can complicate direct comparison of sequences.
For word utterances, this is realized by different speakers speaking different parts of words more quickly or slowly.
As a result, it is often effective to use a warping similarity measure which allows for non-co-occuring sequence elements to be compared.

One such measure which will see extensive use in this thesis is Dynamic Time Warping (DTW).
First proposed for in our exemplary domain of comparing speech utterances \cite{sakoe1978dynamic}, DTW uses dynamic programming to find a monotonic alignment such that the sum of a distance-like cost between aligned feature vectors is minimized.
Because DTW produces an optimal alignment, it is also useful in tasks where the actual correspondense between sequence elements is required, as in the problem of audio-to-MIDI alignment explored in \cref{ch:dtw}.
In this section, we give an overview of DTW; a more thorough treatment can be found in \cite{muller2007dynamic}.

Suppose we are given two sequences of feature vectors $X \in \mathbb{R}^{M \times D}$ and $Y \in \mathbb{R}^{N \times D}$, where $D$ is the feature dimensionality and $M$ and $N$ are the number of feature vectors in $X$ and $Y$ respectively.
DTW produces two nondecreasing sequences $p, q \in \mathbb{N}^L$ which define the optimal alignment between $X$ and $Y$, such that $p[i] = n, q[i] = m$ implies that the $m$th feature vector in $X$ should be aligned to the $n$th in $Y$.
Finding $p$ and $q$ involves globally solving the following minimization problem:
\begin{equation}
p, q = \mathrm{arg}\min_{p, q} \sum_{i = 1}^{L} \|X[p[i]] - Y[q[i]]\|_2
\label{eq:dtw_objective}
\end{equation}
The constraint of monotonicity is enforced by only allowing certain ``step patterns'', the simplest and most common being
\begin{align}
                      & p[i + 1] = p[i] + 1, q[i + 1] = q[i] + 1 \\
        \label{eq:diagonal}
        \mathrm{or\;} & p[i + 1] = p[i] + 1, q[i + 1] = q[i] \\
        \mathrm{or\;} & p[i + 1] = p[i], q[i + 1] = q[i] + 1
\end{align}
although many others have been proposed \cite{muller2007dynamic, sakoe1978dynamic}.
This minimization can be solved in $\mathcal{O}(MN)$ time using dynamic programming \cite{sakoe1978dynamic}.
A common equivalent way to frame this problem which fits straightforwardly into the framework of dynamic programming is to construct a pairwise distance matrix $C \in \mathbb{R}^{M \times N}$ by computing $C[i, j] = \|X[i], Y[i]\|_2$ and then finding the lowest-cost path (subject to appropriate constraints) through the matrix.
This perspective prompts path entries where $p[i + 1] = p[i] + 1, q[i + 1] = q[i] + 1$ to be reffered to as ``diagonal moves''.

DTW is often constrained so that $p$ and $q$ span the entirety of $X$ and $Y$, i.e.\ $p[1] = q[1] = 1$ and $p[L] = N; q[L] = M$.
However, this constraint is inapplicable when subsequence alignment is allowed in which case it is required that either $gN \le p[L] \le N$ or $gM \le q[L] \le M$ where $g \in (0, 1]$ (the ``gully'') is a parameter which determines the proportion of the subsequence which must be successfully matched.
In addition, the path is occasionally further constrained so that
$$
q[i] - p[i] + R \le N, p[i] - q[i] + R \le M
$$
for $i \in \{1, \ldots, L\}$ where $R = g\min(M, N)$ is the ``radius'', sometimes called the Sakoe-Chiba band \cite{sakoe1978dynamic}.
An alternative way to encourage the alignment to favor ``diagonal'' paths is to add a penalty to \cref{eq:dtw_objective}:
\begin{equation}
        p, q = \mathrm{arg}\min_{p, q} \sum_{i = 1}^{L} \|X[p[i]] - Y[q[i]]\|_2 + \begin{cases}
                \phi, p[i] = p[i - 1] \mathrm{\;or\;} q[i] = q[i - 1]\\
                0, \mathrm{otherwise}
        \end{cases}
\end{equation}
where $\phi$ is a tunable parameter controlling how much to discourage non-diagonal path entries.

Once an optimal alignment path is found, a rough similarity measure can be obtained as the sum of the pairwise distances between aligned sequence elements from $X$ and $Y$.
In addition, to achieve alignment one sequence can be adjusted (via resampling or otherwise) so that so that $t_X[p[i]] = t_Y[q[i]], \forall i \in \{1, \ldots, L\}$, where $t_X \in \mathbb{R}^M, t_Y \in \mathbb{R}^N$ are the times corresponding to the feature vectors in $X$ and $Y$ respectively.
An example visualization of DTW on two one-dimensional sequences can be seen in \cref{fig:example_dtw}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/2-example_dtw_sequences.pdf}

  \includegraphics[width=.6\textwidth]{figures/2-example_dtw_matrix.pdf}
  \caption[DTW alignment of two signals]{Example DTW alignment between two one-dimensional sequences.  The top figure shows the determined correspondences between elements of the two sequences as dashed lines.  The lower figure shows the pairwise distance matrix $C$ and the lowest-cost path through it, which defines the optimal alignment.  The x- and y-axes of the distance matrix correspond to sequence indices in the green and blue sequences respectively.}
  \label{fig:example_dtw}
\end{figure}

\subsubsection{Pruning Methods}

icassp2016pruning1
