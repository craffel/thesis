\chapter{Tools} \label{ch:tools}

Throughout this thesis, we will use a common collection of tools to help solve problems of interest.
Broadly speaking, we will make use of techniques from the fields of machine learning and signal processing.
In this chapter, we give a high-level overview of these fields and a more specific definition of the various techniques we will utilize.

\section{Machine Learning}

Machine learning is broadly defined as a suite of methods for enabling computers to carry out particular tasks without being explicitly programmed to do so.
Such methods make use of a collection of data, from which patterns or the desired behaviors are automatically determined.
In this sense, machine learning techniques allow computers to ``learn'' the correct procedure based on the data provided.
In addition to data, machine learning algorithms also often require an objective which quantifies the extent to which they are successfully carrying out a task.
This objective function allows them to be optimized in order to maximize their performance.

Traditionally, machine learning techniques are divided into three types: Supervised, unsupervised, and reinforcement learning.
Supervised learning requires a collection of training data that specifies both the input to the algorithm and the desired output.
Unsupervised learning, on the other hand, does not utilize target output data, and thus is limited to finding structure in the input data.
Finally, reinforcement learning refers to the broader problem of determining a policy to respond to input data in a dynamic ``environment'' based on a potentially rare signal which tells the algorithm whether it is successful or not.
All of the problems in this thesis fall into the category of supervised learing, so we will not discuss unsupervised or reinforcement learning further.

\subsection{Supervised Learning}

In the supervised setting, our machine learning algorithm produces a function $f$ (called a {\em model}) which, given input $x$ and the function's parameters $\theta$, produces an output $\hat{y}$:

\begin{equation}
\hat{y} = f(x, \theta)
\end{equation}

Supervised learning requires a training set $\mathcal{S}$ of input-target pairs $(x_1, y_1), (x_2, y_2), \ldots (x_{|\mathcal{S}|}, y_{|\mathcal{S}|})$, where $x_n \in \mathcal{X}$ and $y_n \in \mathcal{Y}$.
We assume that these pairs are drawn from an unknown joint probability distribution $p(x, y)$, of which the only information we explicitly know is the training set $\mathcal{S}$.
The goal of a supervised learning algorithm is to capture the probability distribution $p(x, y)$ with $f$ by adjusting the parameters $\theta$ based solely on $\mathcal{S}$.

One way to formulate this goal is to state that for a sample from $p(x, y)$ which is not in $\mathcal{S}$, the function $f$ should produce the correct output.
Of course, because we are only given the samples in $\mathcal{S}$, the parameters $\theta$ must be optimized solely according the training pairs in $\mathcal{S}$.
This approach, where $f$ is used directly to produce the predicted value for $y$, is referred to as discriminant training.
An alternative approach, called probabilistic training, is to use $f$ to model $p(x, y)$ by estimating $p(y | x)$ and then choosing the most probable $y$.
In either case, choosing $f$ typically involves using a task-specific ``loss function'' $L$ which computes a nonnegative scalar value which measures the extent to which $\hat{y}$ agress with $y$.
Ideally, minimizing this loss over the training set by adjusting $\theta$ will yield an $f$ which closely approximates $p(x, y)$.

The choice of a loss function will depend on the characteristics of the problem, in particular the target space $\mathcal{Y}$.
For example, in {\em classification} problems, the targets $y$ may take one of $K$ discrete values.
A natural loss function in this case is the zero-one loss

\begin{equation}
L(y, \hat{y}) = \begin{cases}
1,& \hat{y} = y\\
0,& \hat{y} \ne y
\end{cases}
\end{equation}

In {\em regression} problems, the targets are continuously valued, and a common loss function is the squared error

\begin{equation}
L(y, \hat{y}) = (y - \hat{y})^2
\end{equation}

Note that in both cases when $y = \hat{y}$, $L = 0$; this is a common property of loss functions.

In general, a suitable goal for training an unsupervised learning algorithm is to adjust $\theta$ to minimize the average loss over the training set:

\begin{equation}
\frac{1}{|\mathcal{S}|} \sum_{(x, y) \in \mathcal{S}} L(y, f(x, \theta))
\end{equation}

Minimizing this average loss is equivalent to minimizing the empirical risk of $f$.
A common paradigm is to compute the derivative of the loss function $L$ with respect to the model parameters $\theta$ and use this derivative information to incrementally change $\theta$ to make $L$ smaller.
In this case, it is beneficial to have a loss function which is continuous and smooth, which precludes the use of loss functions like the zero-one loss.
A common solution to this problem is to use a surrogate loss function which has the desired properties (e.g.\ smoothness) and whose minimization will also minimize $L$.

There are supervised learning methods which do not follow the above framework.
For example, the k-nearest neighbors algorithm is a classification algorithm which labels its input by assigning it the most common $y$ among the $k$ points in $\mathcal{S}$ which are closest to the input.
This classifier has no loss function or indeed any parameters other than $k$ and $\mathcal{S}$ itself (i.e.\ it is nonparametric).
However, all of the methods we will utilize in this thesis follow the recipe of defining a model with parameters $\theta$ which are adjusted by miniimzing the average loss over $\mathcal{S}$.

\subsection{Sequential Data}

A common scenario is that the data space $\mathcal{X}$ is fixed-dimensional, e.g.\ $\mathcal{X} \in \mathbb{R}^{D_1 \times D_2 \times \cdots D_N}$.
However, in some cases, one (or more) dimensions may vary in size.
An important example of this is sequential data, where a ``temporal'' dimension can differ between examples.
In addition, for real-world data the temporal dimension often contains strong dependencies and correlations.
For some examples of common types of data typically represented as sequences, see \cref{fig:example_sequences}.

An illustrative example is text data where each datapoint is a sentence.
Clearly, the length of each sequence will vary (e.g.\ ``I am happy.'' and ``This sentence is a few words longer.'' differ in length).
In addition, there may be a strong dependency between a given word in a sentence and the words that precede it.
For example, ``I am hungry, please give me'' is much more likely to be followed by the word ``food'' than, say, ``dirt''.

Different possible tasks involving sequential data can be broadly divided into three categories: Sequence classification, embedding, and transduction.
Sequence classification involves applying a single label to entire sequences; for example, classifying which of a finite number of people was speaking in an audio recording of an utterance.
Sequence embedding involves representing sequences as a single point in a fixed-dimensional space with some desired properties.
For example, instead of classifying which person was speaking, we might instead embed audio recordings as vectors in a Euclidean space such that recordings of the same speaker have a small embedded distance and recordings of different speakers have a large distance.
Finally, sequence transduction involves converting an input sequence to a new sequence, potentially of different length and whose elements may lie in a different space.
Speech recognition is an exemplary case, where a sequence of discrete words must be generated based on an audio recording.

\subsection{Neural Networks}

Most of the machine learning algorithms used in this thesis will come from a class of models known as ``neural netorks''.
Neural networks can be considered learnable functions which consist of a sequence of nonlinear processing stages.
First proposed in the XXXXs, these models were designed as an artificial model of the way the brain was understood to work at that time.
The modern understanding of the brain has diverged substantially from this model; nevertheless, the name ``neural network'' has endured.

A predecessor to modern neural network models was the perceptron algorithm.
The perceptron is a simple linear model which can perform binary classification of its input.
Given an input feature vector $x \in \mathbb{R}^D$, the perceptron computes

\begin{equation}
f(x) = \begin{cases}
1,& w^\top x + b > 0\\
0,& \mathrm{otherwise}
\end{cases}
\end{equation}

where $w \in \mathbb{R}^D$ and $b \in \mathbb{R}$ are the parameters of the model.
These parameters are adjusted so that the desired label ($1$ or $0$) is produced given different inputs.

The perceptron algorithm can only correctly classify its input when the data which falls into the two classes is linearly separable by class.
To remedy this, the ``multilayer perceptron'' was proposed, which can be viewed as a sequence of perceptrons organized in ``layers'', each taking its input from the previous perceptron.
In the multilayer perceptron, each perceptron can output a vector of values rather a single value.
Furthermore, rather than producing a binary 1 or 0 depending on the sign of $w^\top x + b$, the layers in a multilayer perceptron may use any of a variety of nonlinear functions.
In summary, a given layer's output is computed as
\begin{equation}
f(x) = \sigma(W x + b)
\end{equation}
where $\sigma$ is a nonlinear ``activation'' function, $W \in \mathbb{R}^{M \times D}$ is the weight matrix, $b \in \mathbb{R}^M$ is the bias vector, and $M$ is the output dimensionality of the layer.
The original perceptron is recovered when $\sigma$ is the Heaviside step function and $M = 1$.
Under this generic definition, multilayer perceptrons are, in fact, equivalent to what are now called feedforward networks; we will use this terminology for the remainder of this thesis.

In a seminal paper, it was shown that feedforward networks with at least two layers are {\em universal function approximators} when the activation function of each layer is a ``squashing'' function (i.e.\ is monotonically increasing, $\lim_{x \rightarrow \infty} \sigma(x) = 1$ and $\lim_{x \rightarrow -\infty} \sigma(x) = 0$).
This implies that such models can approximate any function to arbitrary precision.
Similar results were later demonstrated for other classes of activation functions.

Of course, the ability of a model to approximate any function is only valuable when there is a reliable method to adjust its parameters so that the desired function is suitably approximated.
Currently, the most pervasive method to achieve this is ``backpropagation'', which computes the gradient of an error function which measures the deviation between the network's output and the target output with respect to the model parameters.
We give a derivation of backpropagation for feedforward networks in the following section.
In the supervised learning context, the error function computes the deviation between the network's output and the target for input-target pairs in the training set $\mathcal{S}$.
As a result, for supervised learning, universal approximation only implies that the multilayer feedforward network can ``memorize'' the target output for each input in $\mathcal{S}$, not that it will accurately approximate the data's underlying joint probability distribution $p(x, y)$.
This is referred to as ``overfitting'', and is an undesirable property of machine learning models.

Despite this issue, feedforward networks have proven to be extremely effective models in a variety of machine learning tasks.
This success is thanks to a suite of methods and model designs which allow these models to be trained effectively, over potentially very large datasets, while avoiding overfitting.
It has also led to the development of variants which are more appropriate for sequential data (recurrent networks) and data with local regularities (convolutional networks).
In the following sections, we give an overview of these different developments and approaches.

\subsubsection{Backpropagation}

Backpropagation is a method for efficiently computing the gradient of a loss function applied to a neural network with respect to its parameters.
These partial derivatives can then be used to update the network's parameters using gradient descent methods.
In truth, backpropagation is a special case of the more generic ``reverse-mode automatic differentiation'' method.
Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.

While originally introduced earlier, the wide-spread adoption of backpropagation for training neural networks is largely thanks to Rumelhart et.\ al's seminal paper \cite{}.
Backpropagation is so successful and widely applied that in some sense the term ``neural network'' has become genericized to mean ``a machine learning model for which backpropagation is applicable''.
Because it is fundamental in many of the approaches proposed in this thesis, we give a full derivation of backpropagation for a simple multilayer feedforward network in this section.
A similar, higher-level and more verbose derivation can be found at \cite{}.

By way of review, the chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.
If $L$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that
\begin{equation}
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
For scalar-valued functions of more than one variable (e.g.\ a scalar-valued function of a vector), the chain rule essentially becomes additive.
In other words, if $L$ is a scalar-valued function of a vector $z \in \mathbb{R}^N$, whose dimensions are each a function of some variable $w$, the chain rule states that
\begin{equation}
\frac{\partial C}{\partial w} = \sum_{i = 1}^N \frac{\partial C}{\partial z[i]}\frac{\partial z[i]}{\partial w}
\end{equation}

In the following, we'll discuss backpropagation through an $L$-layer feedforward network, where $N_n$ is dimensionality of layer $n \in \{0, \ldots, L\}$.
$N_0$ is the dimensionality of the input; $N_L$ is the dimensionality of the output.
The $m^{th}$ layer, for $m \in \{1, \ldots, L\}$ has parameters $W_m \in \mathbb{R}^{N_m \times N_{m - 1}}$, the weight matrix, and $b_m \in \mathbb{R}^{N_m}$, the bias vector.
$W_m[i, j]$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.
Each layer computes $z_m \in \mathbb{R}^{N_m}$, a linear mix of its inputs, by $z_m = W_m a_{m - 1} + b_m$, and the activation $a_m = \sigma_m(z_m)$ where $\sigma_m$ is the layer's nonlinear activation function.
$a_L$ is the output of the network.
We define the special case $a_0$ as the input of the network.
We denote $y \in \mathbb{R}^{N_L}$ as the target output, treated as a constant.
The loss or ``cost'' function $C(a_L, y)$ is used to measure the error of the network's output compared to the target.

In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\partial C/\partial W_m$ and $\partial C/\partial b_m$.
It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture: $\partial C/\partial a_L$, the derivative of the cost function with respect to the output of the network, and $\partial a_m/\partial z_m$, the derivative of the nonlinearity used in layer $m$ with respect to its argument $z_m$.

To compute the gradient of our cost function $C$ with respect to $W_m[i, j]$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a_L$, which is itself a function of the linear mix output entries $z_m[k]$, which are themselves functions of the weight matrices $W_m$ and biases $b_m$.
With this in mind, we can use the chain rule as follows:
\begin{equation}
\frac{\partial C}{\partial W_m[i, j]} = \sum_{k = 1}^{N_m} \frac{\partial C}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial W_m[i, j]}
\label{eq:C_wrt_W_m}
\end{equation}
Note that by definition
\begin{equation}
z_m[k] = \sum_{l = 1}^{N_m} W_m[k, l] a_{m - 1}[l] + b_m[k]
\end{equation}
It follows that $\partial z_m[k]/\partial W_m[i, j]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any elements in $W_m$ except for those in the $k^{th}$ row, and we are only considering the entry $W_m[i, j]$.
When $k = i$, we have
\begin{align}
\frac{\partial z_m[i]}{\partial W_m[i, j]} &= \frac{\partial}{\partial W_m[i, j]}\left(\sum_{l = 1}^{N_m} W_m[i, l] a_{m - 1}[l] + b_m[i]\right)\\
                                           &= a_{m - 1}[j]\\
\rightarrow \frac{\partial z_m[k]}{\partial W_m[i, j]} &= \begin{cases}
0 & k \ne i\\
a_{m - 1}[j] & k = i
\end{cases}
\label{eq:z_m_wrt_W_m}
\end{align}

Combining \cref{eq:z_m_wrt_W_m} with \cref{eq:C_wrt_W_m} causes the summation to collapse, giving
\begin{equation}
        \frac{\partial C}{\partial W_m[i, j]} = \frac{\partial C}{\partial z_m[i]} a_{m - 1}[j]
\end{equation}
or in vector form
\begin{equation}
        \frac{\partial C}{\partial W_m} = \frac{\partial C}{\partial z_m} a_{m - 1}^\top
\end{equation}

Similarly for the bias variables $b_m$, we have
\begin{equation}
\frac{\partial C}{\partial b_m[i]} = \sum_{k = 1}^{N_m} \frac{\partial C}{\partial z_m[k]} \frac{\partial z_m[k]}{\partial b_m[i]}
\label{eq:C_wrt_b_m}
\end{equation}

As above, it follows that $\partial z_m[k]/\partial b_m[i]$ will evaluate to zero when $k \ne i$ because $z_m[k]$ does not interact with any element in $b_m$ except $b_m[k]$.  When $k = i$, we have

\begin{align}
\frac{\partial z_m[i]}{\partial b_m[i]} &= \frac{\partial}{\partial b_m[i]}\left(\sum_{l = 1}^{N_m} W_m[il] a_{m - 1}[l] + b_m[i]\right)\\
                                        &= 1\\
\rightarrow \frac{\partial z_m[k]}{\partial b_m[i]} &= \begin{cases}
0 & k \ne i\\
1 & k = i
\end{cases}
\label{eq:z_m_wrt_b_m}
\end{align}

Combining \cref{eq:z_m_wrt_b_m} and \cref{eq:C_wrt_b_m} again causes the summation to collapse to give
\begin{equation}
\frac{\partial C}{\partial b_m[i]} = \frac{\partial C}{\partial z_m[i]}
\end{equation}

or in vector form
\begin{equation}
\frac{\partial C}{\partial b_m} = \frac{\partial C}{\partial z_m}
\end{equation}

Now, we must compute $\partial C/\partial z_m[k]$.
For the final layer ($m = L$), this term is straightforward to compute using the chain rule:

$$
\frac{\partial C}{\partial z_L[k]} = \frac{\partial C}{\partial a_L[k]} \frac{\partial a_L[k]}{\partial z_L[k]}
$$

or, in vector form

$$
\frac{\partial C}{\partial z_L} = \frac{\partial C}{\partial a_L} \frac{\partial a_L}{\partial z_L}
$$

The first term $\partial C/\partial a_L$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.
Similarly, $\partial a_m/\partial z_m$ (for any layer $m$ includling $L$) is the derivative of layer $m$'s nonlinearity with respect to its argument and will depend on the choice of nonlinearity.
For other layers, we again invoke the chain rule:

\begin{align}
        \frac{\partial C}{\partial z_m[k]} &= \frac{\partial C}{\partial a_m[k]} \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                           &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial C}{\partial z_{m + 1}[l]}\frac{\partial z_{m + 1}[l]}{\partial a_m[k]}\right)\frac{\partial a_m[k]}{\partial z_m[k]}\\
                                           &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial C}{\partial z_{m + 1}[l]}\frac{\partial}{\partial a_m[k]} \left(\sum_{h = 1}^{N_m} W_{m + 1}[l, h] a_m[h] + b_{m + 1}[l]\right)\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                           &= \left(\sum_{l = 1}^{N_{m + 1}}\frac{\partial C}{\partial z_{m + 1}[l]} W_{m + 1}[l, k]\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
                                           &= \left(\sum_{l = 1}^{N_{m + 1}}W_{m + 1}^\top[k, l] \frac{\partial C}{\partial z_{m + 1}[l]}\right) \frac{\partial a_m[k]}{\partial z_m[k]}\\
\end{align}

where the last simplification was made because by convention $\partial C/\partial z_{m + 1}$ is a column vector, allowing us to write the following vector form:
\begin{align}
\frac{\partial C}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial C}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\label{eq:C_wrt_z_m}
\end{align}
where $\circ$ is the element-wise (Hadamard) product.

We now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters: First, we compute $\partial C/\partial z_L$ based on the choice of cost function and $\partial a_m/\partial z_m$ based on each layer's nonlinearity.
Then, we recursively can compute $\partial C/\partial z_m$ layer-by-layer based on the term $\partial C/\partial z_{m + 1}$ computed from the previous layer (this is called the ``backward pass'').
Finally, the resulting $\partial C/\partial z_m$ terms are then used to compute $\partial C/\partial W_m$ and $\partial C/\partial b_m$ using \cref{eq:C_wrt_W_m} and \cref{eq:C_wrt_b_m} respectively.
Using this recipe requires $\partial C/\partial z_L$ and $\partial a_m/\partial z_m$ to be known ahead of time.
By way of example, \cref{tab:nonlinearities} and \cref{tab:cost_functions} list some common cost functions and nonlinearities and their appropriate derivatives.

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Nonlinearity & $a_m$                                            & $\frac{\partial a_m}{\partial z_m}$                   \\
  \midrule
  Logistic     & $\dfrac{1}{1 + e^{z_m}}$                         & $a_m(1 - a_m)$                                        \\[1.2ex]
  $\tanh$      & $\dfrac{e^{z_m} - e^{-z_m}}{e^{z_m} + e^{-z_m}}$ & $1 - (a_m)^2$                                         \\[1.2ex]
  Rectifier    & $\max(0, z_m)$                                   & $\begin{cases}0,& z_m < 0\\ 1,& z_m \ge 0\end{cases}$ \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common nonlinearities used in neural networks]{Common nonlinearities $\sigma_m(z_m)$ used in neural networks and their derivatives with respect to their input $z_m$.}
  \label{tab:nonlinearities}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{lll}
  \toprule
  Cost Function & $C(a_L, y)$                           & $\frac{\partial C}{\partial a^L}$ \\
  \midrule
  Squared Error & $\dfrac{1}{2}(y - a^L)^\top(y - a^L)$ & $y - a^L$                         \\
  Cross-Entropy & $(y - 1)\log(1 - a^L) - y\log(a^L)$   & $\dfrac{a^L - y}{a^L(1 - a^L)}$   \\
  \bottomrule
\end{tabular}
\end{center}
\caption[Common cost functions used in neural networks]{Common cost (loss) functions $C(a_L, y)$ used in neural networks and their derivatives with respect to their input $a_L$.}
  \label{tab:cost_functions}
\end{table}

In summary, backpropagation proceeds in the following manner for each training sample: For the ``forward pass'', given the network input $a_0$, compute $a_m$ for $m \in \{1, \ldots, L\}$ recursively by $a_m = \sigma_m(W_m a_{m - 1} + b_m)$.
For the ''backward pass'', compute
\begin{equation}
\frac{\partial C}{\partial z_L} = \frac{\partial C}{\partial a_L} \frac{\partial a_L}{\partial z_L}
\end{equation}
whose terms are all known a priori, then recursively compute
\begin{equation}
\frac{\partial C}{\partial z_m} = \left(W_{m + 1}^\top \frac{\partial C}{\partial z_{m + 1}}\right) \circ \frac{\partial a_m}{\partial z_m}
\end{equation}
and use the resulting values to compute
\begin{equation}
\frac{\partial C}{\partial W_m} = \frac{\partial C}{\partial z_m} a_{m - 1}^\top
\end{equation}
and
\begin{equation}
\frac{\partial C}{\partial b_m} = \frac{\partial C}{\partial z_m}
\end{equation}

From this procedure, we can see why backpropagation is an efficient way to compute these partial derivatives: By reusing terms, all of the partial derivatives can be computed by a single forward and backward pass.
For a feedforward network, the computational cost of each of these passes is effectively $L$ matrix multiplications.
A simple alternative would be to measure the change in $C$ by adjusting each of the parameters (i.e.\ individual elements of the weight matrices and bias vectors) by a small amount.
However, this would require a separate forward pass for each parameter.
Modern neural networks contain millions of parameters, so clearly backpropagation is an efficient choice.

\subsubsection{Tricks}

Combined with a gradient descent technique, backpropagation provides an effective method for optimizing the parameters of a neural network (``training'') to minimize a given loss function.
However, training neural networks is made difficult in practice by a variety of factors.
First, the composition of the many nonlinear processing layers in a neural network make the resulting loss function highly non-convex.
Nonconvexity means that, when minimizing the network's loss function with gradient descent methods, there is no guarantee that a given stationary point is a global minimum.
It also implies that both the way the parameters are initialized prior to optimization and the gradient descent technique used may have a very strong effect on the best solution found during optimization.

As mentioned above, the universal approximation property of neural networks can also have negative implications for their utility.
The fact that a possible outcome of training a neural network is the simple memorization of correspondences in the training set means that the resulting model may be of little use in practice.
As a result, in many cases it is necessary to regularize the network to prevent it from overfitting to the training set.

Finally, feedforward networks with many layers are also prone to the problem of ``vanishing and exploding gradients''.
This issue is caused by the fact that as gradients are backpropagated through layers, they are multiplied by each successive weight matrix and the derivative of each nonlinearity (as shown in \cref{eq:C_wrt_z_m}).
When the spectral norm of the weight matrices and/or the gradients of the nonlinearities are substantially greater than or less than one, the resulting backpropagated gradients can become exponentially smaller or larger as they approach ``early'' layers in the network.
This can effectively prevent the parameters of the first few layers from being adjusted, which can greatly inhibit learning.

Fortunately, there has been a great deal of development into methods for mitigating these issues.
Collectively, these techniques have been dubbed ``deep learning'' thanks to their applicability to networks with many layers.
In this section, we motivate and describe those techniques which are used in this thesis.
Practical and broader overviews can be found in \cite{bengio} \cite{lecun} \cite{nature}.

\paragraph{Number and Size of Layers}

When designing a neural network model, one of the first choices which must be made is the number of layers and their sizes (output dimensionality).
Because early theory proved that networks with a single layer could approximate any function arbitrarily well given that the layer was sufficiently ``wide'' (high-dimensional) \cite{}, and because networks with many layers can be more difficult to train due to the vanishing and exploding gradients problem \cite{}, early applications of neural networks typically only used a single hidden layer.
However, recent results have shown that using many layers can help networks learn a hierarchical transformation to more and more abstract representations which, for real-world problems, can be highly beneficial \cite{}.
Coupled with recent advances in training very deep networks \cite{}, the paradigm of ``deep'' networks (having many layers) has come to dominate.

Regardless of depth, the dimensionality of each intermediate representation (the ``layer sizes'') must be chosen.
While increasing the dimensionality may increase modeling power, it also exacerbates overfitting and incurs a greater computational cost.
However, Graphics Processing Units (GPUs) have recently been used to accelerate neural network models thanks to their ability to quickly perform large matrix multiplications.
As a result, in modern practice high-dimensional layers are used in combination with regularization techniques \cite{}.

\paragraph{Nonlinearities}

A fundamental ingredient of neural networks is the elementwise nonlinearity applied at each layer; without them, the network would only be able to learn linear relationships between its input and output \cite{}.
Early work typically used sigmoidal nonlinearities such as the logistic or hyperbolic tangent functions (see \cref{tab:nonlinearities}).
However, these nonlinearities have a very small gradient for much of their domain which can cause vanishing gradient problems particularly for networks with many layers.
Recently, the rectifier nonlinearity has become the standard for deep networks due to its computational efficiency and tendency to produce sparse representations \cite{}.
The rectifier nonlinearity has zero gradient for half of its domain, which can hinder training in some cases; as a result a number of variants have recently been proposed which have been shown empirically to produce better results in some cases \cite{}.

\paragraph{Regularization}

Due to their tendency to overfit, some form of regularization is typically necessary to ensure that the optimized neural network's variance is not too high.
A common regularizer which is used in many machine learning models is to include in the cost function a penalty on the norm of the model's parameters.
In neural networks, adding the $L^2$ penalty $\lambda \sum_i \theta_i^2$ where $\lambda$ is a hyperparameter and $\theta_i$ are the model's parameters (e.g. individual entries of the weight matrices and bias vectors) is referred to as ``weight decay''.
This can encourge the weight matrix of a given layer from focusing too heavily (via a very large weight value) on a single dimension of its input.
A related term is $\lambda \sum_i |\theta_i|$ which effectively encourages parameter values to be zero.

A completely different regularization method which has recently proven popular in neural networks is ``dropout'' \cite{}.
In dropout, at each training iteration each dimension of each layer is randomly set to zero with probability $p$.
After training, the weights in each layer are scaled by $1/p$.
Dropout intends to prevent the dimensions of the output of a given layer from being too heavily dependent (correlated) with one of the inputs by randomly artificially removing connections.
More simply, it provides a source of noise which prevents memorization of correspondences in the training set and has repeatedly been shown empirically to be an effective regularizer.

In practice, the technique of ``early stopping'' is almost always used to avoid overfitting in neural network models.
To utilize early stopping, during training the performance on a held-out ``validation set'' (over which the parameters of the network are not optimized) is computed.
Overfitting is indicated by performance degrading on the validation set, which simulates real-world performance.
As a result, early stopping effectively prevents overfitting by simply stopping training once the performance begins to degrade.
The measure of performance and criteria for stopping my vary widely from task to task (and practitioner to practitioner) but the straightforwardness and effectiveness of this approach has led it to be nearly universally applied.

Of course, regularization can be rendered unnecessary when a huge amount of training data is available.
This is commonly simulated by applying hand-designed realistic deformations to the training data, which is referred to as ``data augmentation''.
For example, applying elastic distortions to images can allow simple, unregularized neural network models to match the performance of more complex architectures \cite{}.
Data augmentation has also been explored in the domain of audio and music signals \cite{}.

\paragraph{Parameter Initialization}

Because the objective functions used for training neural networks tend to be highly nonconvex, the way parameters are initialized prior to training can have a substantial effect on the solutions found after optimization.
In fact, the recent sucess of neural network models was prompted by a new method for setting parameter values prior to supervised training called ``unsupervised pretraining''.
Unsupervised pretraining starts by training an unsupervised model (e.g.\ a Restricted Boltzmann Machine \cite{} or an autoencoder \cite{}) and then using the parameters from that model to initialize a supervised model.
Ideally, the unsupervised model will take care of disentangling factors of variation and producing a more efficient representation of the input space.
After initialization with values from the unsupervised model, training of the model proceeds in the normal supervised fashion.
This can greatly accelerate learning, and is of particular use when there is a great deal more unlabeled data available than labeled data.

It has more recently been shown that unsupervised pretraining can be skipped (provided that there is a sufficient amount of labeled data) by carefully initializing the network's parameters.
This typically involves sampling the values of the weight matrices from zero-mean Gaussian distributions whose variances are set so that the representation at each layer's output has unit variance.
The actual variances used for the random variables depends on the input and output dimensionality as well as the nonlinearity; \cite{} provides a derivation for linear networks, \cite{} for rectifier networks and \cite{} gives a generic empirical procedure for any network architecture.
An alternative is to use orthogonal matrices \cite{} or to use a sparse initialization so that a fixed number of output dimensions are non-zero \cite{}.


\paragraph{Input Normalization}

Most of the analysis performed on neural networks in order to determine how to facilitate training rely on the assumption that the feature dimensions of the input to the network have zero mean and unit variance.
A necessary step in order to take advantage of many of these tricks is then to standardize the feature dimensions of the input, typically using statistics computed on the training set.
An alternative which can also facilitate convergence is to whiten data using Pricipal Component Analysis before presenting it to the network.

\paragraph{Model Architecture}

Beyond the simple feedforward network model we have focused on thus far, a variety of more specialized models have been proposed which intend to exploit different types of structure present in real-world data.
For example, presenting sequential data one step at a time to a feedforward network assumes temporal independence, which is invalid for most real-world temporal data.
Other types of data, such as images, exhibit local dependencies in their dimensions which are invariant to translation.
Modeling this type of structure can be greatly facilitated by the important choice of model architecture.
We give an overview of the two most relevant architectures for sequential data, recurrent and convolutional networks, in the next two sections.

\subsubsection{Recurrent Networks}

A natural extension to the feedforward network are {\em recurrent} connections, which feed the output of the model back into its input.
The resulting model can be considered a learnable function which at each time step produces an output based on its current input and previous output.
This class of models, called a recurrent networks, are well-suited for sequential data due to their ability to model temporal dependencies.

Training of recurrent networks is typically accomplished using ``backpropagation through time'' (BPTT) \cite{}.
BPTT is a simple extension of backpropagation where, given a sequence of length $T$, the recurrent network is ``unrolled'' over time and is treated as a feedforward network with $T$ layers, each of which corresponds to a single time step.
While BPTT provides a straightforward way to train recurrent networks, this approach can suffer dramatically from vanishing and exploding gradients for large values of $T$ for the same reasons that networks with many layers do \cite{}.
The use of gating architectures \cite{hochreiter1997long,cho2014learning}, sophisticated optimization techniques \cite{martens2011learning,sutskever2013importance}, gradient clipping \cite{pascanu2012difficulty,graves2013generating}, and/or careful initialization \cite{sutskever2013importance,jaegar2012long,mikolov2014learning,le2015simple} can help mitigate this issue and has facilitated the success of RNNs in a variety of fields (see e.g.\ \cite{graves2012supervised} for an overview).
However, these approaches don't {\em solve} the problem of vanishing and exploding gradients for recurrent networks, and as a result these models are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps \cite{martens2011learning,sutskever2013importance,le2015simple,hochreiter1997long,krueger2015regularizing,arjovsky2015unitary}.
Because the representations used for audio data are typically at least thousands of time steps in length, we will not use recurrent networks in this thesis, although we will use them as a point of comparison and motivation in some places.

\subsubsection{Convolutional Networks}

\subsubsection{Attention}

\subsection{Stochastic Optimization}

\subsection{Bayesian Optimization}

\section{Signal Processing}

\subsection{Audio Signals and Psychoacoustics}

proposal1

\subsection{Time-Frequency Analysis}

\subsection{Dynamic Time Warping}

ismir2015large2, icassp2016optimizing2

\subsubsection{Pruning Methods}

icassp2016pruning1
