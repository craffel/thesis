\chapter{Optimizing Dynamic Time Warping}
\label{ch:dtw}

The central application of this thesis is the problem of matching musical scores to corresponding audio recordings in a large database.
This is motivated by our collection of 178,561 unique MIDI transcriptions which, as we explored in \cref{ch:midi}, can provide a bounty of ground-truth data for content-based music information retrieval tasks.
To maximize its value, a given MIDI file must be both matched and aligned in time to a corresponding music audio file.
In both scenarios, it is of great value to produce a confidence score which can communicate both how well the content in a MIDI file matches a given audio file and the quality of the transcription.

Most previous research has focused on systems meant for either alignment or matching but not both.
In the context of MIDI-to-audio alignment, a wide variety of techniques have been proposed to determine a correspondence between discrete times in the audio and MIDI files.
While some approaches use edit distance measures such as Smith-Waterman \cite{ewert2012towards} and Needleman-Wunsch \cite{grachten2013automatic}, the most common approach is dynamic time warping (DTW) which we describe in \cref{sec:dtw}.
Because DTW produces monotonic alignments, it is well-suited for audio-to-MIDI alignment when we expect that the MIDI is an accurate continuous transcription (i.e.\ without out-of-sequence or incorrect sections).

The total distance between aligned pairs of feature vectors produced by DTW provides a single global value which can be used as a natural measure of the ``similarity'' between two sequences.
In fact, DTW has seen extensive use as a way of measuring sequence similarity in the data mining literature \cite{berndt1994using}.
In the context of MIDI and audio files, \cite{hu2003polyphonic} evaluated DTW's effectiveness at matching a small collection of Beatles MIDIs to recordings of Beatles songs.

Despite its widespread use, DTW's success is dependent on its parametrization as well as system design choices such as the feature representation used for sequence elements.
To our knowledge, there has been no large-scale quantitative comparison of different DTW-based alignment systems.
This is likely due to the fact that evaluating a given system's performance would require either a large collection of MIDI and audio pairs for which the correct alignment is already known (which does not exist) or manual audition and rating of the output of the systems (which is time-consuming).

This chapter aims to remedy this by searching across a large space of DTW designs to optimize both alignment accuracy and confidence reporting.
First, we enumerate some common design choices in DTW-based alignment systems in \cref{sec:dtw_parameters}.
In \cref{sec:synthetic}, we propose a method for creating a synthetic dataset of MIDI-audio pairs by applying realistic corruptions to MIDI files, allowing us to create an arbitrary amount of data where we know a priori the correct alignment.
We then tune parameters for alignment using Bayesian optimization (\cref{sec:optimizing}) and for confidence reporting using an exhaustive search (\cref{sec:confidence}).
Finally, in \cref{sec:qualitative} we perform a large-scale qualitative evaluation of our proposed alignment system on real-world data and discuss possibilities for improvement.

\section{DTW-Based Alignment}
\label{sec:dtw_parameters}

From the discussion in \cref{sec:dtw}, it is clear that there are many parameters which govern the behavior of DTW.
In addition, because DTW involves aligning sequences of feature vectors, domain-specific considerations such as the data representation will also affect its outcome.
In general, there is no consensus as to what parameter settings generally work best for MIDI-to-audio alignment.
The following list enumerates some of the more relevant parameters which must be tuned in DTW-based audio-to-MIDI alignment systems and references prior works to give example settings.
For reference, we follow the notation used in \cref{sec:dtw}.

\begin{description}
\item[Feature representation ($X$ and $Y$):] Prior to alignment, audio and MIDI data must be converted to an intermediate representation where their distance can be computed.
This is often done by synthesizing the MIDI file to obtain an audio signal and computing a common spectral transform of the audio recording and synthesized MIDI audio.
Chroma vectors, which represent the amount of energy in each semitone summed across octaves \cite{fujishima1999realtime} are a common choice \cite{hu2003polyphonic, ewert2012towards}.
The constant-Q transform (CQT) (discussed in \cref{sec:cqt}), which represents the amount of energy in logarithmically spaced bins \cite{brown1991calculation} has also been used \cite{raffel2015large, dixon2005match, ellis2013aligning}.
Occasionally, log-magnitude features are used in order to more closely mimic human perception \cite{raffel2015large, ellis2013aligning, turetsky2003ground}.
In \cite{turetsky2003ground} and \cite{hu2003polyphonic} it is noted that Mel-Frequency Cepstral Coefficients result in poor performance for music signals because they obscure pitch information.
\item[Time scale ($t_X$ and $t_Y$):] Feature vectors are frequently computed over short, overlapping frames of audio \cite{dixon2005match, turetsky2003ground, hu2003polyphonic}.
Note that the spacing between feature vectors must be sufficiently small compared to the auditory temporal resolution (e.g.\ tens of milliseconds \cite{blauert1997spatial}) in order for a perceptually accurate alignment to be possible.
Occasionally, beat-synchronous feature vectors are used \cite{raffel2015large,ellis2013aligning}, which can reduce computation time and can produce accurate alignments provided that the beat tracking is correct.
\item[Normalization:] In \cite{rakthanmanon2012searching}, it is argued that z-scoring (standardizing) the feature sequences is critical for data mining applications of DTW, which was used in audio-to-MIDI alignment in \cite{hu2003polyphonic}.
In addition, various normalizations have been applied to the feature vectors in $X$ and $Y$ before computing their local distances.
A common choice is normalizing each vector by its $L^2$ norm, which is equivalent to using the cosine distance \cite{turetsky2003ground, ewert2012towards, raffel2015large, ellis2013aligning}.
\item[Penalty ($\phi$):] In many applications of DTW to audio-to-MIDI alignment, no additive penalty is used, which corresponds to setting $\phi = 0$.
However, as long as the MIDI and audio files have consistent tempi, non-diagonal moves should be discouraged.
In addition, it has been argued \cite{raffel2015large} that when subsequence alignment is allowed, an additive penalty can be crucial to ensure that the entire subsequence is used, and so $\phi$ is set to the 90th percentile of the distances $\|X[n] - Y[m]\|_2^2$ for $m \in \{1, \ldots, M\}; n \in \{1, \ldots, N\}$.
In \cite{ellis2013aligning}, a fixed value of $\phi = .5$ is used.
\item[Gully ($g$) and band path constraint:] The ``gully'' and band path constraint are also often omitted, which corresponds to setting $g = 1$.
A value of $g$ which is close to 1 will afford some tolerance to the possibility that the beginning or end of the MIDI transcription is incorrect (e.g.\ a fade-out or lead-in), so \cite{ellis2013aligning} sets $g = .7$ and \cite{raffel2015large} sets $g = .95$.
In data mining applications \cite{ratanamahatana2004everything} it is argued that the band radius path constraint both reduces computational complexity and results in more reliable alignments by avoiding paths with many non-diagonal moves.
\end{description}

\section{Creating a Synthetic Alignment Dataset}
\label{sec:synthetic}

The list above defines a large space of parameter settings/design choices for DTW-based audio-to-MIDI alignment systems.
Previous research has typically involved manually tuning alignment parameters based on a modest-sized test set of MIDI/audio pairs and informally auditioning the aligned MIDI data.
Because this method only facilitates small-scale comparisons, there is an obvious question of what parameter settings would yield the best general-purpose alignment system.
Unfortunately, manual audition of even a tiny subset of possible parameter settings on a modestly-sized collection of paired MIDI and audio files is infeasible, let alone a collection large enough to make substantive judgements about the general performance of a given system.
Furthermore, automatic evaluation has been blocked by the lack of a large ground-truth dataset of ``correct'' alignments.
We therefore propose a method for synthetically creating MIDI/audio pairs with known alignments by applying a series of corruptions to MIDI files to resemble real-world conditions.
When applying the corruptions, we keep track of the adjustments needed to correctly align the corrupted MIDIs, which allows us to rapidly and automatically evaluate a huge number of possible systems.

To create this dataset, we first collected 1,000 MIDI files which were transcriptions of Western popular music songs.
We then applied the following series of transformations, based on our experience with common differences between MIDI transcriptions and audio recordings:
First, to simulate differences in tempo, we adjusted the timing in each MIDI file by a low-frequency length-$N$ random signal $r$, defined as
\begin{align}
s[m] &\sim \mathcal{N}(0, \sigma_t), m \in \{1, \ldots, N + 1\}\\
R[k] &= \begin{cases}
s[k]e^{-k + 1}, &k \in \{1, \ldots, N + 1\}\\
R[2N - k + 1], &k \in \{N + 2, \ldots, 2N \}
\end{cases}\\
r[n] &= \sum_{k = 1}^{2N} R[k]e^{j\pi (k - 1)(n - 1)/N}, n \in \{1, \ldots, N\}
\label{eq:offset_signal}
\end{align}
i.e.\ the inverse discrete Fourier transform of an exponentially decaying Gaussian-distributed random spectrum with standard deviation $\sigma_t$.
Second, the first 10\% and last 10\% of the transcription were each cropped out with probability 50\%, which simulates the MIDI file being an incomplete transcription.
In addition, 1\% of each transcription was cut out at a random location with 10\% probability, which simulates a missing measure.
Third, because it is common for a MIDI transcription to be missing an instrument (for example, karaoke renditions in which the lead vocal line has been deleted are frequently distributed as MIDI files), we removed each instrument track in each MIDI file with probability $p_r$, making sure never to remove all instruments.
Fourth, in all MIDI files, we randomly added 1 or -1 to the program number of each instrument.
This simulated the fact that when comparing a synthesized MIDI file to an audio recording, the timbre of a synthesized MIDI instrument is always somewhat different from its real-world counterpart.
Finally, for each note in each instrument, we multiplied the velocity by a random number sampled from $\mathcal{N}(1, \sigma_v)$ while keeping it in the MIDI velocity range $[1, 127]$.
This was meant to further simulate differences in instrument characteristics in real-world vs.\ synthesized songs, and also simulated missing notes for large $\sigma_v$.
All MIDI data manipulations were realized with \texttt{pretty\char`_midi} \cite{raffel2014pretty_midi}.
Example constant-Q spectrograms of a synthesized MIDI file before and after undergoing this corruption process can be seen in \cref{fig:corruption}.
In addition, the timing offset curve which produced this corruption is shown in \cref{fig:warping}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/4-original_cqt.pdf}
  \includegraphics[width=\textwidth]{figures/4-corrupted_cqt.pdf}
  \caption[Constant-Q spectrograms of a MIDI file before and after corruption]{Constant-Q spectrograms of a synthesized MIDI file before (top) and after (bottom) undergoing the corruption process described in \cref{sec:synthetic}.
After synthesis, log-magnitude spectrograms with $L^2$-normalized spectra were computed every 46 milliseconds, with frequencies from MIDI note C2 (65.4 Hz) to B6 (987.8 Hz).
This corruption was achieved using the parameters from the ``hard'' dataset, i.e. $\sigma_t = 20, p_r = .5, \sigma_v = 1$.}
  \label{fig:corruption}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/4-warping.pdf}
  \caption[Example synthetic warping offset]{Warping offset curve which produced the time distortion in the corrupted MIDI file visualized in \cref{fig:corruption}.
The low-frequency variation is due to the random signal $r$ (\cref{eq:offset_signal}) while the sudden jump around 125 seconds is caused by 10\% of the MIDI file being cropped out.
In our synthetic alignment task, the goal is to reverse this timing distortion over a large dataset of corrupted MIDI files.}
  \label{fig:warping}
\end{figure}

As described above, a DTW-based alignment system can serve two purposes: First, to align a MIDI transcription in time to an audio recording, and second, to produce a confidence score denoting the quality of the transcription or whether the MIDI file is a transcription of the recording at all.
We therefore produced two sets of corrupted versions of the 1,000 MIDI files we collected, one to measure alignment performance, and one to evaluate confidence reporting.
For the first (``easy'') set, we focused on corruption parameters corresponding to real-world conditions for a high-quality transcription, setting $\sigma_t = 5, p_r = .1, \sigma_v = .2$.
For the second (``hard''), we set $\sigma_t = 20, p_r = .5, \sigma_v = 1$ so that the alignment task is sufficiently difficult to result in a significant number of incorrect alignments, which allows us to test whether an alignment system can automatically report failure.

\section{Optimizing DTW-Based Alignment}
\label{sec:optimizing}

Given a dataset of MIDI/audio pairs with known correct alignments, we can evaluate a given alignment scheme via the mean absolute alignment error across the set.
The mean error quantifies the extent to which the alignment was able to remove the timing distortions (random warping and cropping) described in \cref{sec:synthetic}.
When the alignment has failed for a portion of the song, the error between the mapped times and the correct times may be very large, so we clip the mean error to $.5$ seconds (which essentially denotes an error threshold above which all local alignment discrepancies are equally incorrect).  Thus, our error metric is:
\begin{equation}
\frac{1}{L}\sum_{i = 1}^{L} \min(|t_X[p[i]] - \hat{t}_X[q[i]]|, 0.5)
\end{equation}
where $\hat{t}_X$ is the ground-truth ``warped'' time scale.  We average this measure across the test set.
The resulting error $|t_X[p[i]] - \hat{t}_X[q[i]]|$ from aligning the spectrograms shown in \cref{fig:corruption} using our ``gold standard'' system (described later in \cref{sec:goldstandard}) is shown in \cref{fig:correction_error}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/4-correction_error.pdf}
  \caption[Absolute error from correcting synthetic warping]{Absolute error between the true warping (shown in \cref{fig:warping}) and the estimated warping found by aligning the spectrograms in \cref{fig:corruption} using the system described in \cref{sec:goldstandard}.
The error is clipped at $.5$ seconds, above which point we consider all discrepancies equally bad.
The spike of largest error around 125 seconds is caused by the estimated warping not perfectly correcting the missing 10\% of the corrupted MIDI which was randomly cropped out.}
  \label{fig:correction_error}
\end{figure}

Rapid calculation of this precise metric (over the ``easy'' test set) allows us to perform large-scale comparisons of different parameter settings.
To decide which settings to try, we use Bayesian optimization, which approximates the relationship between parameter settings and objective values as a Gaussian process.
Using this formulation, Bayesian optimization can automatically propose new alignment systems based on the performance of previously-evaluated systems.
A more thorough description of Bayesian optimization can be found in \cref{sec:bayesian_optimization}.
We utilized the framework and software proposed in \cite{snoek2012practical} in all of our experiments.

Based on the design choices outlined in \cref{sec:dtw_parameters}, we chose to optimize over the following parameter space:
\begin{description}
\item[Feature representation:] We used either chroma vectors or constant-Q spectra.
The constant-Q spectra spanned 4 octaves, starting from MIDI note C2 (65.4 Hz) with 12 bins per octave.
In preliminary experiments, we found that all of the best-performing alignment systems used log-magnitude features regardless of whether chroma vectors or constant-Q spectra were used, so we computed log-magnitude features in all experiments.
\item[Time scale:] We either computed feature vectors every 46 milliseconds or utilized beat-synchronous features.
\item[Normalization:] We optionally z-scored the feature vectors, and normalized them by their $L^1$, $L^2$, $L^\infty$ (max) norm, or not at all.
\item[Penalty:] For the penalty, we optimized a scale in $[0, 3]$ to apply to the median distance between all pairs of feature vectors in $X$ and $Y$.
Using the median distance made this penalty adaptive to different feature representations and normalization schemes.
\item[Gully and band path constraint:] We allowed the gully to take any value in $[0, 1]$ and optionally enforced the band path constraint.
\end{description}
This space subsumes most of the systems discussed in \cref{sec:dtw_parameters}.
All feature extraction was realized with \texttt{librosa} \cite{mcfee2015librosa, mcfee2015librosa_scipy}.

When performing Bayesian optimization, it is helpful to ``seed'' the optimization with objective values for many randomly-chosen parameter settings to ensure that the optimization thoroughly explores the possible parameter space.
We therefore computed the accuracy for 10,000 randomly configured alignment systems and seeded the optimizer with the 100 systems which achieved the lowest mean error.
In order to avoid local minima in the parameter space, we carried out 10 differently initialized Bayesian optimization runs with 100 trials each, resulting in 1,000 total trials.

The best-performing alignment system achieved an objective value of $0.0181$, meaning that the average absolute error across the entire dataset was about 18 milliseconds.
This is both close to the limit of the auditory temporal resolution and less than half of the time-scale resolution used for non-beat-synchronous feature vectors, so attaining a higher accuracy is likely unrealistic.
A one-sample $t$-test was performed to determine which systems gave alignment quality statistically equivalent to the best performance.
Testing the per-pair scores rather than the mean error across all pairs gave better robustness to outliers.
Using a $p$-value of $0.05$ as the threshold returned 51 configurations of indistinguishable quality.

As a high-level overview of these systems, none used beat-synchronization or a path constraint and all of them used log-magnitude constant-Q spectra as a feature representation and set both the penalty median scale and ``gully'' close to 1.
Almost all of these systems used $L^2$ normalization (resulting in a cosine distance for local feature comparisons); a few used $L^1$ normalization.
There was no clear trend in the use of z-scoring.
\Cref{tab:best_systems} displays the parameter settings and the mean absolute error achieved by the 10 best-performing alignment systems.
Interestingly, a relatively wide range (about $0.9$ to $1.0$) of median scale values for the additive penalty $\phi$ were effective.
We propose that this indicates a relative insensitivity to this parameter setting, i.e.\ as long as $\phi$ is close to the distance matrix median the alignment system can produce high-quality alignments.

\begin{table}
\centering
        \begin{tabular}{rrrr}
        \toprule
        $\phi$ Median Scale  & Standardize?   &   Gully $g$ &   Mean Error \\
        \midrule
                  $1.035070$ & Yes            &    $0.967203$ &    $0.0180094$ \\
                  $0.840218$ & No             &    $0.970180$ &    $0.0180690$ \\
                  $0.944247$ & No             &    $0.967956$ &    $0.0181160$ \\
                  $0.822652$ & No             &    $0.974826$ &    $0.0181264$ \\
                  $0.929215$ & No             &    $0.971404$ &    $0.0181429$ \\
                  $0.920111$ & Yes            &    $0.962121$ &    $0.0181909$ \\
                  $0.947526$ & No             &    $0.973988$ &    $0.0181951$ \\
                  $0.893840$ & Yes            &    $0.963922$ &    $0.0181988$ \\
                  $0.906607$ & Yes            &    $0.965711$ &    $0.0182004$ \\
                  $0.937798$ & Yes            &    $0.969747$ &    $0.0182236$ \\
        \bottomrule
        \end{tabular}
  \caption[Parameters and mean absolute errors of the 10 best-performing systems]{Parameters and mean absolute errors of the 10 systems which achieved the best performance on aligning the ``easy'' synthetic dataset.
All of these systems utilized constant-Q transforms, normalized feature vectors by their $L^2$ norm, did not beat-synchronize, and did not use a band mask constraint.}
  \label{tab:best_systems}
\end{table}


\section{Optimizing Confidence Reporting}
\label{sec:confidence}

Having found alignment systems which achieve high accuracy on the ``easy'' dataset, we move on to the question of computing reliable alignment confidence scores.
As described in \cref{sec:dtw}, DTW provides a ``raw'' score as the sum of distances between all aligned feature vectors, such that a small distance denotes high confidence.
This measure is inappropriate, however, when the number of aligned feature vectors varies from song to song; in this setting, the mean distance is usually used instead of the total distance.
Furthermore, it's not clear whether the non-diagonal path penalties $\phi$ should be included in this distance.
Finally, \cite{raffel2015large} advocates further normalizing this distance by
\begin{equation}
\frac{1}{L^2} \sum_{i = \min(p)}^{\max(p)} \sum_{j = \min(q)}^{\max(q)} \|X[i] - Y[j]\|_2^2
\label{eq:dtw_mean_distance_normalization}
\end{equation}
i.e.\ the mean distance between all pairs of frames over the entire aligned portion of both feature sequences.
This is intended to help normalize out global differences between different distance matrices; for example, if a MIDI synthesis' timbre does not closely match an audio recording, entries in the resulting distance matrix will tend to be larger which would inflate the DTW distance.

To choose an optimal score normalization scheme, we first aligned all of the MIDI/audio pairs in both the ``hard'' and ``easy'' datasets using every alignment system generated during our Bayesian optimization trials.
Then, for each system we computed the normalized DTW distance for every MIDI/audio pair using all combinations of the normalization schemes listed above (with and without length normalization, diagonal penalties, and mean distance normalization) resulting in 8 scores per file pair.
Finally, we computed the Kendall rank correlation coefficient \cite{kendall1938new} between each score and the mean absolute error produced by all alignment systems for every pair in both datasets.
A high correlation indicates that the system is able to accurately report the quality of its alignments.

Among our 51 highest-accuracy systems, the highest correlation was 0.710.
In general, the high-accuracy systems all produced rank correlations close to this value.
All of the systems achieved the highest correlation when including the penalties in the score calculation, normalizing by the path length, and normalizing by the mean distance across the aligned portions.
This suggests that these steps are all helpful for producing a reliable score.
We visualize the normalized DTW distances and mean errors for each alignment produced by the system which had the best performance on the ``easy'' dataset in \cref{fig:correlation}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/4-correlation.pdf}
  \caption[Alignment errors and normalized DTW distances for the best-performing system]{Scatter plot of alignment errors (x-axis, log-scaled) and normalized DTW distance (y-axis) produced by the highest-performing alignment system using the DTW distance normalization scheme which produced the highest correlation between the two axes.
Ideally, alignments with a small normalized distance will have a small mean error, indicating that the score can reliably predict when alignment was successful.
The group of points in the top right correspond to alignments which were globally wrong (resulting in a mean error of $.5$, the maximum value).}
  \label{fig:correlation}
\end{figure}

\subsection{Choosing Easily-Reproducible Parameters}
\label{sec:goldstandard}

As a final step in producing a ``gold standard'' alignment system, we decided to find parameter settings that were easy to report and implement but nevertheless produced alignments and confidence scores which were not significantly different than the best systems.
We chose the following design: For a feature representation, we used log-magnitude constant-Q transforms computed on the non-beat-synchronous time scale described in \cref{sec:optimizing}.
We normalized the spectra by their $L^2$ norm and did not z-score them.
We set the penalty $\phi$ to the median distance between all pairs of feature vectors in $X$ and $Y$ and used a gully parameter $g$ of $0.96$.
This system achieved a mean absolute alignment error on the ``easy'' dataset of 0.0188, with alignment errors which were not significantly different from the best-performing system.
By normalizing the DTW distance (including penalties) by the path length and the mean distance across aligned portions, we achieved a rank correlation between confidence and accuracy of 0.700.
This ``gold standard'' system is straightforward to implement, and will be used for the remainder of this thesis.

\section{Qualitative Evaluation on Real-World Data}
\label{sec:qualitative}

In the experiments described above, we have found a system which can provide both high accuracy and a useful confidence measure on synthetic data.
To determine its applicability outside of synthetic contexts, we performed a large-scale qualitative evaluation on real-world data.
Our dataset consisted of 500 MIDI files which had artist and song title information in their filename, which allowed us to hand-matched them to entries in the Million Song Dataset \cite{bertin2011million}.
This collection comprises MIDI files with a full range of transcription qualities, and includes some pairs which are incorrectly matched due to metadata errors.
By manually evaluating whether each resulting alignment was successful, we can determine how accurately the ``gold standard'' system performs alignments and how reliable the reported confidence scores are.

Conveniently, the vast majority of the normalized DTW distances for our real-world alignments fell into the range $[0.5, 1.0]$.
In order to more intuitively report this measure as a ``confidence score'', we decided to re-scale it as follows:
\begin{equation}
        c = \max(0, \min(2(1 - s), 1))
        \label{eq:score_normalization}
\end{equation}
where $s$ is the normalized DTW distance and $c$ is the resulting confidence score.
This maps the range to $[0, 1]$ where, ideally, $0$ indicates a failure and $1$ indicates a potential perfect alignment.
We will use this re-scaled score for the remainder of this thesis.

After aligning all pairs in this dataset, we synthesized the resulting aligned MIDI files and created stereo recordings with the aligned synthesized MIDI audio in one channel and the original audio recording in the other.
We then listened to each aligned MIDI pair and annotated a score from 1-5 as follows:
\begin{enumerate}
\item MIDI and audio file are incorrectly matched
\item Alignment failed due to major differences
\item Alignment was mostly successful with minor issues
\item Perfect alignment with minor transcription issues
\item Perfect alignment and transcription
\end{enumerate}
For example, if a MIDI transcription was matched and successfully aligned to the correct song but was missing an instrument, a rating of 4 would be given; if the missing instrument caused the alignment to sound ``sloppy'', the rating would be 3 instead.
Ideally, alignments with high confidence scores will be given higher ratings, and vice versa.
To prevent biasing the results, we did not have access to the reported confidence score while rating a given aligned pair.
For further analysis, we also recorded notes about any interesting qualitative characteristics of each alignment.
For example, \cref{tab:ratings} lists all of the ratings and confidence scores of alignments where we annotated that the audio recording was likely a remix (alternate version) of the song that the MIDI file is a transcription of.

\begin{table}
\centering
\begin{tabular}{rrl}
\toprule
   Rating &   Confidence Score & Note                         \\
\midrule
        1 &          0.520807  & Audio sounds like a remix    \\
        2 &          0.497009  & Audio is remix               \\
        3 &          0.313761  & Remix?                       \\
        2 &          0.219259  & Audio is remix               \\
        2 &          0.21846   & Audio is remix/wrong section \\
        1 &          0.16089   & Audio is remix/live          \\
        2 &          0.146482  & Different versions (remix?)  \\
        2 &          0.126939  & Audio is remix               \\
        2 &          0.0978644 & Very sloppy/audio is remix?  \\
        1 &          0.0739608 & Audio is remix               \\
        1 &          0.0687189 & Audio may be remix           \\
        1 &          0.0610702 & Audio is probably remix      \\
        2 &          0.0484973 & Different versions (remix?)  \\
        1 &          0.0431807 & Audio is remix?              \\
        2 &          0.0353811 & Audio is remix               \\
        1 &          0.0259135 & Audio is remix               \\
        1 &          0.015929  & Audio is remix               \\
        2 &          0.0117205 & Audio is remix?  Very sloppy \\
        2 &          0.0116941 & Audio is remix               \\
        1 &          0         & Audio is a remix             \\
        2 &          0         & Very sloppy/remix?           \\
\bottomrule
\end{tabular}
  \caption[Ratings for alignments which we annotated as likely remixes]{Ratings, confidence scores, and annotations for real-world alignments where we thought the audio recording was a remix of the song the MIDI file was a transcription of.
Because remixes tend to have substantial differences in structure and instrumentation, it is likely that alignment will fail.}
  \label{tab:ratings}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/4-violin.pdf}
  \caption[Distributions of confidence scores for each rating]{Violin plot (box plot with rotated kernel density estimates) showing the distribution of confidence scores for each rating in our qualitative evaluation.
A smaller confidence score indicates a more successful alignment.
The area of each violin corresponds to the number of pairs which had a given rating.
Box plots in each violin show the median and upper and lower quartiles.}
  \label{fig:violin}
\end{figure}

\Cref{fig:violin} shows the distributions of confidence scores for pairs assigned each of the five ratings.
Apart from encountering some incorrect pairs (rated 1), we also found that various transcription issues prevented successful alignments.
A common issue for those pairs rated 2 was that the wrong section of the MIDI transcription was  matched to the audio, often due to different instrumentation, keys, or versions (e.g.\ the audio was a remix).
Any pairs rated 3 either had multiple missing instruments, many musical embellishments, or mismatched tempi.
In addition, the overlap between the confidence scores for pairs rated 4 and 5 indicates that our confidence score is largely invariant to minor transcription issues.
The most common transcription issue for pairs rated 4 was a single missing instrument or minor embellishments, most often on the vocal track.

Overall, our ``gold standard'' alignment system was able to successfully align most correctly-matched pairs and produced a reliable confidence score.
Considering pairs rated 3-5 as ``correct'' matches, the resulting confidence scores achieve an area under the ROC curve of 0.981 (95\% confidence interval [.973, .989], calculated by 1000-sample bootstrap), indicating a high quality measure.
Unfortunately, there were a few pairs rated 1 or 2 which were not assigned small confidence scores; without these outliers, we could use a wider range of thresholds to obtain high-confidence alignments.
The most important remaining flaw is the relative insensitivity to missing instruments and embellishments.

\section{Discussion}
\label{sec:discussion}

In summary, large-scale optimization over synthetic data has delivered a DTW-based system which is simple to implement and achieves accurate and reliable results for both alignment and matching.
We demonstrated that on real-world data, our alignment system produced a confidence score which provided a trustworthy indicator of whether the alignment was successful or not.
Nevertheless, there was some overlap between the confidence score distributions for each rating.
This implies that our system cannot detect fine-grained issues such as missing instruments or minor alignment errors.

We believe this is caused by the fact that alignment and confidence reporting have somewhat different goals which result in different requirements of the feature representation.
For example, alignment benefits from a representation which is invariant to missing instruments so that it can still be successful when a transcription is incomplete.
On the other hand, it would be helpful to have a confidence score which was sensitive to whether or not a transcription is missing instruments.
Given that the framework we have explored requires that the representation is the same for both tasks, it is unsurprising that there are minor alignment issues which our confidence score is insensitive to.
We therefore propose that exploring systems which utilize different representations for alignment and confidence reporting would be a fruitful research direction.

We also note that there are many other design choices and parameters for audio-to-MIDI alignment systems which we have not explored.
These include larger selections of possible step sizes such as those described in \cite{muller2007dynamic, sakoe1978dynamic}, different feature representations, and even completely different alignment methods such as Smith-Waterman \cite{ewert2012towards} and Needleman-Wunsch \cite{grachten2013automatic}.
In addition, there are additional avenues to be explored for creating realistic synthetic data, such as utilizing different synthesis programs, modifying pitch bend and other control change messages, and applying realistic acoustic noise and nonlinearities \cite{mcfee2015software}.
Our proposed framework for creating alignment datasets and optimizing system design using Bayesian optimization could be straightforwardly extended to accommodate these new directions.

Given the success of our system at discriminating between correct and incorrect matches, it would appear that it is a viable method for matching MIDI transcriptions to corresponding audio recordings in a database.
Specifically, we could compute the alignment and confidence score between a query MIDI file and every recording and assign matches according to those recordings which produced high confidence scores.
Unfortunately, the proposed method is much too inefficient for searching all but the smallest databases.
We explore this issue and develop methods for mitigating it in the following chapters.
