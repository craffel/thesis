\chapter{Learning an Efficient Representation for Dynamic Time Warping}
\label{ch:dhs}

The system developed in \cref{ch:dtw} produces a confidence score which proved to be extremely reliable at determining whether or not a MIDI file was a transcription of a given audio file.
The goal of this thesis is to match a large collection of 178,561 MIDI files to corresponding audio recordings.
We cannot rely on any source of metadata from these MIDI files (see \cref{sec:score_overview}), so we must instead utilize a {\em content-based} comparison scheme like the one proposed in \cref{ch:dtw}.
To maximize the chances of finding a matching audio recording for a given MIDI file, we need to use a large and comprehensive pool of audio files.
We use the 994,960 7digital preview clips \cite{schindler2012facilitating} corresponding to the Million Song Dataset (MSD) \cite{bertin2011million}, which consist of (typically) 30 second portions of recordings from the largest standard research corpus of popular music.
A complete search for matches could thus involve 994,960 alignments for each of our 178,561 MIDI files, resulting in 177,661,052,560 comparison.

The method proposed in \cref{ch:dtw} is much too inefficient to be used in this manner.
Under this approach, aligning a typical-length MIDI file to a single audio recording from the MSD takes on average about 132 milliseconds on an Intel Core i7-4930k processor when using an extremely efficient LLVM-compiled DTW routine and a parallelized distance matrix computation.
Matching our entire collection of MIDI files using this approach would therefore take approximately
\begin{equation}
\frac{(.132 \mathrm{\;seconds})(178{,}561 \mathrm{\;MIDI\;files})(994{,}960 \mathrm{\;audio\;files})}{(60 \mathrm{\;seconds})(60 \mathrm{\;minutes})(24 \mathrm{\;hours})(365 \mathrm{\;days})} \approx 745 \mathrm{\;years}
\end{equation}
Clearly, a more efficient approach is warranted.

In fact, the problem of finding the sequence in a large database (here, a collection of audio recordings) which is most similar under DTW distance to a query (here, a MIDI file) is common in the field of data mining \cite{berndt1994using}.
Unsurprisingly, a variety of methods have been proposed for speeding up this task to make it feasible for very large databases.
A pervasive framework is ``pruning'', where a computationally efficient operation is used to discard a large portion of the database and the remaining entries are compared using a more expensive, but highly accurate, method.
Under certain assumptions, there are methods which can achieve {\em exact} pruning (i.e.\ without every erroneously discarding the correct match) of large-scale nearest-neighbor DTW search.
\cite{rakthanmanon2012searching} provides a thorough overview of these techniques and shows that for some problems they can allow exact search of databases with trillions of data points.
However, one of the core assumptions of these methods is that the query is always a subsequence of its correct match, and equivalently that the length of the aligned sequence is known a priori, which does not hold in the setting of MIDI-to-audio matching.
In fact, because the 7digital MP3s are truncated preview clips and MIDI files are usually complete transcriptions, for our problem we nearly always have the opposite case where the correct match is a subsequence of the query.
We therefore cannot leverage these pruning methods for our problem.

One of the reasons DTW-based search is so expensive is that it has $\mathcal{O}(MN)$ complexity, where $M$ and $N$ are the lengths of the two sequences being compared.
Clearly, then, DTW can be made quadratically more efficient by decreasing the number of elements in the sequences being compared.
For example, \cite{keogh2001dimensionality,yi2000fast} proposed downsampling sequences by computing their average over non-overlapping blocks, which for some tasks provided orders of magnitude speedup without dramatically affecting accuracy.
Similarly, \cite{salvador2007toward} propose an iterative procedure for making DTW more efficient which starts by finding the alignment path based on a low-resolution estimate of the sequences being compared, and then refines this estimate at higher and higher resolutions.
The appropriateness of downsampling sequences depends on the sequences being oversampled (i.e.\ having an unnecessarily high correlation between consecutive sequence elements).

Computing the DTW distance between two sequences also potentially involves computing the pairwise Euclidean distance between all of their elements.
In data mining tasks, entries in sequences are often one-dimensional, so the cost of this operation is usually not considered independent of the overall cost of DTW.
For the time-frequency representation we utilize in \cref{ch:dtw}, our sequences consist of 48-dimensional feature vectors.
As a result, in our problem setting computing the pairwise distance matrix is actually more expensive than finding the lowest-cost path through it because each entry in the distance matrix requires at least $D$ operations, where $D$ is the feature dimensionality.
While computing the distance matrix similarly receives quadratic speed gains when the sequences are downsampled, it also can benefit when a lower-dimensional or otherwise more efficient feature representation is used.

In addition, the ``raw'' representation in which data is initially provided may not produce the most effective measure of similarity under the Euclidian distance utilized by DTW.
More concretely, while the audio-to-synthesized MIDI CQT comparison utilized in \cref{ch:dtw} proved effective, it may be that we are able to transform the individual feature vectors to a space where similarity is better represented so that matching MIDI files and audio recordings have a smaller DTW distance.
Recently, the framework of ``representation learning'' has proven to be effective for this type of problem \cite{bengio2013representation}.
Representation learning utilizes prior knowledge, such as a collection of pairs of known-similar and dissimilar elements, to learn a mapping to a more effective transformed space.
This process is also capable of producing ``mutilmodal'' mappings, i.e.\ transformations which allow the comparison of heterogeneous data.

From the above discussion, it is clear that mapping very long sequences of high-dimensional feature vectors to shorter, lower-dimensional sequences such that similarity is preserved would provide substantial gains when comparing sequences with DTW.
Motivated by this possibility, in this chapter we propose a system with the following capabilities:
\begin{description}
\item[Maps to a Hamming space:] By replacing continuous-valued feature vectors with bitvectors in an embedded Hamming space, computing the distance between a pair of feature vectors simplifies to an exclusive-or followed by a single POPCNT SSE4.2 operation \cite{intel2007programming}, which substantially speeds up distance matrix calculation.
\item[Downsamples sequences:] Rather than creating a one-to-one correspondence between the original featured vectors and mapped bitvectors, groups of subsequent feature vectors are mapped to a single bitvector, giving a quadratic increase in efficiency.
\item[Preserves similarity:] The system is trained with an objective which seeks to produce a mapping where aligned feature vectors from matching sequences have a small Hamming distance in the embedded space, and non-matched feature vectors have a large distance.
\item[Learns its representation:] Our approach is entirely data-driven, which allows it to adapt to different problem settings including multimodal data, as we show in \cref{sec:multimodal}.
\end{description}

In the following section, we describe our model and training approach in detail.
In \cref{sec:dhs_experiment}, we test our system's accuracy on the task of matching MIDI files to the Million Song Dataset.
Finally, we discuss possibilities for improvement in \cref{sec:dhs_discussion}.

\section{Learning to Downsample and Hash}
\label{sec:hashing_model}

As a high-level overview, our system will learn a mapping from sequences of feature vectors to downsampled sequences of binary vectors in a Hamming space where similarity is preserved based on a training set of matched and aligned sequences.
This training set can be constructed by obtaining a collection of sequences in which matching pairs are known, and then using DTW to find the optimal alignment of feature vectors in matching sequences.
From this data, we extract groups of sequentially co-occuring feature vectors to construct $\mathcal{P}$, such that $(x, y) \in \mathcal{P}$ indicates that $x$ is a group of feature vectors which is aligned to the group $y$ from a matching sequence.
For example, in the context of MIDI to audio matching, $x$ will be subsequent constant-Q spectra of a synthesized MIDI which are aligned in time to $y$, consisting of co-occuring constant-Q spectra from a matching audio recording.
We then construct $\mathcal{N}$, a set of ``dissimilar'' pairs, by repeatedly randomly choosing two pairs $(x_1, y_1), (x_2, y_2) \in \mathcal{P}$ and swapping to obtain $(x_1, y_2), (x_2, y_1) \in \mathcal{N}$.

Given this training data, our goal is to map sequences of feature vectors to sequences of binary vectors in a Hamming space where groups of sequentially co-occuring vectors in $\mathcal{P}$ have a small distance and groups in $\mathcal{N}$ have a large distance.
Motivated by the multimodal hashing technique of \cite{masci2014multimodal}, we use the following objective function to measure the quality of the mapping:

\begin{equation}
\mathcal{L} = \frac{1}{|\mathcal{P}|} \sum_{(x, y) \in \mathcal{P}} \| f(x) - g(y) \|_2^2  + \frac{\alpha}{|\mathcal{N}|} \sum_{(a, b) \in \mathcal{N}} \max(0, m - \|f(a) - g(b) \|_2)^2
\label{eq:hashing_objective}
\end{equation}

where $f$ and $g$ are learned nonlinear functions, $\alpha$ is a parameter to control the importance of separating dissimilar items, and $m$ is a target separation of dissimilar pairs.
The first term in the loss function encourages the embedded distance between similar pairs to be small and the second term encourages dissimilar pairs to have a distance of at least $m$.
More specifically, if $x, y \in \mathbb{R}^{G \times D}$ where $G$ is the number of subsequent feature vectors which are grouped together (and therefore the downsampling ratio) and $D$ is the feature dimensionality, then $f(x), g(y) \in \mathbb{H}^E$ where $\mathbb{H}^E$ is the Hamming space of dimensionality $E$.
This system is then optimized over $\mathcal{P}$ and $\mathcal{N}$ by to adjust the parameters of $f$ and $g$ such that $\mathcal{L}$ is minimized.
The use of two different functions $f$ and $g$ allows for different mappings to be learned for different types of data; we expect this to be beneficial for our problem setting because synthesized MIDI CQTs and real-world audio CQTs may have different characteristics.
This also allows us to experiment with multimodal data, i.e.\ cases where $x$ and $y$ come from different, and potentially non-comparable, feature spaces.
A visualization of this process is shown in \cref{fig:hashing_schematic}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/4-hashing_schematic.pdf}
  \caption[Hashing groups of subsequent feature vectors]{Diagram visualizing how subsequent groups of feature vectors are mapped to binary vectors.
Matching sequences of feature vectors which have been aligned in time are shown at the top and bottom.
Co-occuring groups of feature vectors, highlighted in light boxes, are passed through two learned nonlinear functions $f$ and $g$ (visualized as neural network diagrams).
The functions then output a single hash vector each, which are ideally close together in the embedded Hamming space because they represent co-occuring feature vectors from matched and aligned sequences.
For groups of feature vectors from non-matching sequences, the networks are trained to output binary vectors with a large Hamming distance.}
  \label{fig:hashing_schematic}
\end{figure}

After minimizing $\mathcal{L}$ on a training set, sequences of feature vectors in either modality can then be mapped to downsampled sequences of hash vectors using $f$ and $g$.
Once mapped, we can perform DTW much more efficiently to compute a distance between hash sequences, where the Euclidean distance calculation inherent in DTW has been replaced by Hamming distance.
More specifically, we expect a speedup by a factor of up to $G^2 D$, because we have effectively downsampled both sequences by a factor of $G$ and have replaced the $D$-operation Euclidean distance with a Hamming distance calculation.
In practice, the resulting binary vectors can be represented efficiently as unsigned integers, whose Hamming distance can be computed with an exclusive-or operation followed by a call to the POPCNT SSE4.2 instruction \cite{intel2007programming}.
The exclusive-or compares bit-by-bit whether the entries of the binary vectors are equal, and the POPCNT implements a ``population count'' or ``hamming weight'' calculation, which simply counts the number of nonzero bits in the result.
This process is visualized in \cref{fig:popcnt}.
Ideally, utilizing DTW to compare sequences which have been mapped to this efficient representation will effectively recover matching and non-matching sequences.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/4-popcnt.pdf}
  \caption[Computing Hamming distance with XOR and POPCNT]{Computing the Hamming distance between two binary vectors via an exclusive-or followed by a POPCNT instruction.
The exclusive-or of the top two vectors produces the bottom vector, which is $1$ where the input vectors differ and $0$ where they match.
The result of the exclusive-or is passed to the POPCNT SSE4.2 instruction, which simply counts the number of non-zero bits in its input.
These two steps therefore can produce the distance between two binary vectors in two machine instructions.}
  \label{fig:popcnt}
\end{figure}

Given our training procedure, we now need to choose appropriate models for the hashing functions $f$ and $g$.
Neural networks are an attractive choice, because they can in principle be optimized with respect to unusual loss functions like the one defined in \cref{eq:hashing_objective}.
In addition, the suite of tricks discussed in \cref{sec:tricks} can help ensure effective results.

In \cite{masci2014multimodal}, dense feed-forward neural networks are used for the learnable functions $f$ and $g$.
We could potentially utilize this type of network for our hashing and downsampling procedure by concatenating input $x, y \in \mathbb{R}^{G \times D}$ into single-dimensional vectors in $\mathbb{R}^{GD}$.
However, we argue that convolutional networks (discussed in \cref{sec:convolutional_networks}) are a better choice for sequential data for the following reasons:
Sequential data has at least one ordered (temporal) dimension, and in most natural data, the strongest dependencies along the temporal axis are localized.
This is exactly the kind of structure convolutional networks are designed to efficiently exploit.
Furthermore, the pooling layers which are a common ingredient in convolutional networks provide a natural method of downsampling the sequences.
They also allow the network's output a given point in time to depend on a large ``receptive field'' of the input.
In our problem setting, this means that a given hash vector produced by $f$ or $g$ will be affected not only by the corresponding group of $G$ feature vectors but also on a window of feature vectors surrounding it.
For the above reasons, we utilize convolutional networks for the rest of this chapter.

\section{Experiment: MIDI to MSD Matching}
\label{sec:dhs_experiment}

To determine the effectiveness of this approach on the task of matching MIDI files to the MSD, we evaluated its effectiveness on a smaller surrogate problem.
As an overview, this experiment proceeded as follows:
First, we assembled a collection of known-correct MIDI-MSD matches.
We then aligned these pairs using the method described in \cref{sec:goldstandard} and used a subset of the matched and aligned pairs to train our hashing model.
Finally, we utilized the trained model to produce hash sequences for the entire MSD and a held-out subset of MIDI files from our collection of known-correct matches.
Our approach was then evaluated based on how accurately it was able to recover the correct match in the MSD by nearest-neighbor DTW search utilizing the downsampled hash sequences.
To explore the ability of our approach to handle multi-modal data, we repeated this experiment using piano roll matrices to represent MIDI files rather than constant-Q spectrograms.
We also compared our technique to a simple baseline, to ensure that our learning-based approach is beneficial.

\subsection{Preparing Data}

For evaluation, we need a collection of ground-truth MIDI-audio pairs which are correctly matched.
To train our hashing model, we further require a collection of {\em aligned} MIDI and audio files, to supply the matching groups of feature vectors from each domain that will be used to train our model.
To obtain MIDI-audio pairs, we first separated a subset of MIDI files from our 178,561-file collection for which the directory name corresponded to the song's artist and the filename gave the song's title.
The resulting metadata needed additional canonicalization; for example, ``The Beatles'', ``Beatles, The'', ``Beatles'', and ``The Beatles John Paul Ringo George'' all appeared as artists.
To normalize these issues, we applied some manual text processing and resolved the artists and song titles against the Freebase \cite{bollacker2008freebase} and Echo Nest\footnote{\texttt{http://developer.echonest.com/docs/v4}} databases.
This resulted in a collection of 17,256 MIDI files for 10,325 unique songs, which we will refer to as the ``clean MIDI subset''.

We will leverage the clean MIDI subset in two ways: First, to obtain ground-truth pairings of MSD/MIDI matches, and second, to create training data for our hashing scheme.
The training data does not need to be restricted to the MSD, and using other sources to increase the training set size will likely improve our hashing performance, so we combined the MSD with three benchmark audio collections: CAL500 \cite{turnbull2007towards}, CAL10k \cite{tingle2010exploring}, and uspop2002 \cite{berenzweig2004large}.
To match these datasets to the clean MIDI subset, we used the Python search engine library \texttt{whoosh}\footnote{\texttt{https://github.com/dokipen/whoosh}} to perform a fuzzy matching of their metadata.
This resulted in 25,707 audio/MIDI file pairs corresponding to 5,249 unique songs.

Fuzzy metadata matching is not enough to ensure that we have MIDI and audio files with matching content; for instance, the metadata could be incorrect, the fuzzy text match could have failed, the MIDI could be a poor transcription (e.g., missing instruments or sections), and/or the MIDI and audio data could correspond to different versions of the song.
We therefore utilized the alignment technique developed in \cref{ch:dtw}, which was empirically shown to produce a confidence score which reliably reports whether the content of a MIDI file matches a given audio file.
Conveniently, utilizing an alignment system to verify matches also provides the aligned sequences we need for training our hashing system.
Based on the real-world results presented in \cref{sec:qualitative}, we decided to consider a match ``correct'' when its confidence score was above $0.5$; for the 500 matches annotated, this produces the correct label for all but 1 of the failed matches and for the majority of the correct matches.
Retaining all alignments with confidence scores higher than this threshold resulted in 10,094 successful matches.

Recall that these matched and aligned pairs serve two purposes: They provide training data for our hashing model; and we also use them to evaluate the entire content-based matching system.
For a fair evaluation, we exclude items used in training from the evaluation, thus we split the successful alignments into three parts: $65\%$ to use as training data, $10\%$ as a ``development set'' to tune the content-based matching system, and the remaining $25\%$ to use for final evaluation of our system.
Care was taken to split based on \textit{songs}, rather than by entry (since some songs appear multiple times).

\subsection{System Specifics}

ismir2015large3.2

Receptive field

\subsection{Baseline Methods}

Thresholding CQT PCA
LSH

\subsection{Adapting to Multi-Modal Data}
\label{sec:multimodal}

nips2015accelerating3

\subsection{Results}

ismir2015large4

Add qualitative network behavior plots (deep dream).
Add percentage-below-rank plot.

\section{Discussion}
\label{sec:dhs_discussion}
