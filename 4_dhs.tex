\chapter{Learning an Efficient Representation for Dynamic Time Warping}
\label{ch:dhs}

The system developed in \cref{ch:dtw} produces a confidence score which proved to be extremely reliable at determining whether or not a MIDI file was a transcription of a given audio file.
The goal of this thesis is to match a large collection of 178,561 MIDI files to corresponding audio recordings.
We cannot rely on any source of metadata from these MIDI files (see \cref{sec:score_overview}), so we must instead utilize a {\em content-based} comparison scheme like the one proposed in \cref{ch:dtw}.
To maximize the chances of finding a matching audio recording for a given MIDI file, we need to use a large and comprehensive pool of audio files.
We use the 994,960 7digital preview clips \cite{schindler2012facilitating} corresponding to the Million Song Dataset (MSD) \cite{bertin2011million}, which consist of (typically) 30 second portions of recordings from the largest standard research corpus of popular music.
A complete search for matches could thus involve 994,960 alignments for each of our 178,561 MIDI files, resulting in 177,661,052,560 comparison.

The method proposed in \cref{ch:dtw} is much too inefficient to be used in this manner.
Under this approach, aligning a typical-length MIDI file to a single audio recording from the MSD takes on average about 132 milliseconds on an Intel Core i7-4930k processor when using an extremely efficient LLVM-compiled DTW routine and a parallelized distance matrix computation.
Matching our entire collection of MIDI files using this approach would therefore take approximately
\begin{equation}
\frac{(.132 \mathrm{\;seconds})(178{,}561 \mathrm{\;MIDI\;files})(994{,}960 \mathrm{\;audio\;files})}{(60 \mathrm{\;seconds})(60 \mathrm{\;minutes})(24 \mathrm{\;hours})(365 \mathrm{\;days})} \approx 745 \mathrm{\;years}
\end{equation}
Clearly, a more efficient approach is warranted.

In fact, the problem of finding the sequence in a large database (here, a collection of audio recordings) which is most similar under DTW distance to a query (here, a MIDI file) is common in the field of data mining \cite{berndt1994using}.
Unsurprisingly, a variety of methods have been proposed for speeding up this task to make it feasible for very large databases.
A pervasive framework is ``pruning'', where a computationally efficient operation is used to discard a large portion of the database and the remaining entries are compared using a more expensive, but highly accurate, method.
Under certain assumptions, there are methods which can achieve {\em exact} pruning (i.e.\ without every erroneously discarding the correct match) of large-scale nearest-neighbor DTW search.
\cite{rakthanmanon2012searching} provides a thorough overview of these techniques and shows that for some problems they can allow exact search of databases with trillions of data points.
However, one of the core assumptions of these methods is that the query is always a subsequence of its correct match, and equivalently that the length of the aligned sequence is known a priori, which does not hold in the setting of MIDI-to-audio matching.
In fact, because the 7digital MP3s are truncated preview clips and MIDI files are usually complete transcriptions, for our problem we nearly always have the opposite case where the correct match is a subsequence of the query.
We therefore cannot leverage these pruning methods for our problem.

One of the reasons DTW-based search is so expensive is that it has $\mathcal{O}(MN)$ complexity, where $M$ and $N$ are the lengths of the two sequences being compared.
Clearly, then, DTW can be made quadratically more efficient by decreasing the number of elements in the sequences being compared.
For example, \cite{keogh2001dimensionality,yi2000fast} proposed downsampling sequences by computing their average over non-overlapping blocks, which for some tasks provided orders of magnitude speedup without dramatically affecting accuracy.
Similarly, \cite{salvador2007toward} propose an iterative procedure for making DTW more efficient which starts by finding the alignment path based on a low-resolution estimate of the sequences being compared, and then refines this estimate at higher and higher resolutions.
The appropriateness of downsampling sequences depends on the sequences being oversampled (i.e.\ having an unnecessarily high correlation between consecutive sequence elements).

Computing the DTW distance between two sequences also potentially involves computing the pairwise Euclidean distance between all of their elements.
In data mining tasks, entries in sequences are often one-dimensional, so the cost of this operation is usually not considered independent of the overall cost of DTW.
For the time-frequency representation we utilize in \cref{ch:dtw}, our sequences consist of 48-dimensional feature vectors.
As a result, in our problem setting computing the pairwise distance matrix is actually more expensive than finding the lowest-cost path through it because each entry in the distance matrix requires at least $D$ operations, where $D$ is the feature dimensionality.
While computing the distance matrix similarly receives quadratic speed gains when the sequences are downsampled, it also can benefit when a lower-dimensional or otherwise more efficient feature representation is used.

In addition, the ``raw'' representation in which data is initially provided may not produce the most effective measure of similarity under the Euclidian distance utilized by DTW.
More concretely, while the audio-to-synthesized MIDI CQT comparison utilized in \cref{ch:dtw} proved effective, it may be that we are able to transform the individual feature vectors to a space where similarity is better represented so that matching MIDI files and audio recordings have a smaller DTW distance.
Recently, the framework of ``representation learning'' has proven to be effective for this type of problem \cite{bengio2013representation}.
Representation learning utilizes prior knowledge, such as a collection of pairs of known-similar and dissimilar elements, to learn a mapping to a more effective transformed space.
This process is also capable of producing ``mutilmodal'' mappings, i.e.\ transformations which allow the comparison of heterogeneous data.

From the above discussion, it is clear that mapping very long sequences of high-dimensional feature vectors to shorter, lower-dimensional sequences such that similarity is preserved would provide substantial gains when comparing sequences with DTW.
Motivated by this possibility, in this chapter we propose a system with the following capabilities:
\begin{description}
\item[Maps to a Hamming space:] By replacing continuous-valued feature vectors with bitvectors in an embedded Hamming space, computing the distance between a pair of feature vectors simplifies to an exclusive-or followed by a single POPCNT SSE4.2 operation \cite{intel2007programming}, which substantially speeds up distance matrix calculation.
\item[Downsamples sequences:] Rather than creating a one-to-one correspondence between the original featured vectors and mapped bitvectors, groups of subsequent feature vectors are mapped to a single bitvector, giving a quadratic increase in efficiency.
\item[Preserves similarity:] The system is trained with an objective which seeks to produce a mapping where aligned feature vectors from matching sequences have a small Hamming distance in the embedded space, and non-matched feature vectors have a large distance.
\item[Learns its representation:] Our approach is entirely data-driven, which allows it to adapt to different problem settings including multimodal data, as we show in \cref{sec:multimodal}.
\end{description}

In the following section, we describe our model and training approach in detail.
In \cref{sec:dhs_experiment}, we test our system's accuracy on the task of matching MIDI files to the Million Song Dataset.
Finally, we discuss possibilities for improvement in \cref{sec:dhs_discussion}.

\section{Learning to Downsample and Hash}
\label{sec:hashing_model}

As a high-level overview, our system will learn a mapping from sequences of feature vectors to downsampled sequences of binary vectors in a Hamming space where similarity is preserved based on a training set of matched and aligned sequences.
This training set can be constructed by obtaining a collection of sequences in which matching pairs are known, and then using DTW to find the optimal alignment of feature vectors in matching sequences.
From this data, we extract groups of sequentially co-occuring feature vectors to construct $\mathcal{P}$, such that $(x, y) \in \mathcal{P}$ indicates that $x$ is a group of feature vectors which is aligned to the group $y$ from a matching sequence.
For example, in the context of MIDI to audio matching, $x$ will be subsequent constant-Q spectra of a synthesized MIDI which are aligned in time to $y$, consisting of co-occuring constant-Q spectra from a matching audio recording.
We then construct $\mathcal{N}$, a set of ``dissimilar'' pairs, by repeatedly randomly choosing two pairs $(x_1, y_1), (x_2, y_2) \in \mathcal{P}$ and swapping to obtain $(x_1, y_2), (x_2, y_1) \in \mathcal{N}$.

Given this training data, our goal is to map sequences of feature vectors to sequences of binary vectors in a Hamming space where groups of sequentially co-occuring vectors in $\mathcal{P}$ have a small distance and groups in $\mathcal{N}$ have a large distance.
Motivated by the multimodal hashing technique of \cite{masci2014multimodal}, we use the following objective function to measure the quality of the mapping:

\begin{equation}
\mathcal{L} = \frac{1}{|\mathcal{P}|} \sum_{(x, y) \in \mathcal{P}} \| f(x) - g(y) \|_2^2  + \frac{\alpha}{|\mathcal{N}|} \sum_{(a, b) \in \mathcal{N}} \max(0, m - \|f(a) - g(b) \|_2)^2
\label{eq:hashing_objective}
\end{equation}

where $f$ and $g$ are learned nonlinear functions, $\alpha$ is a parameter to control the importance of separating dissimilar items, and $m$ is a target separation of dissimilar pairs.
The first term in the loss function encourages the embedded distance between similar pairs to be small and the second term encourages dissimilar pairs to have a distance of at least $m$.
More specifically, if $x, y \in \mathbb{R}^{G \times D}$ where $G$ is the number of subsequent feature vectors which are grouped together (and therefore the downsampling ratio) and $D$ is the feature dimensionality, then $f(x), g(y) \in \mathbb{H}^E$ where $\mathbb{H}^E$ is the Hamming space of dimensionality $E$.
This system is then optimized over $\mathcal{P}$ and $\mathcal{N}$ by to adjust the parameters of $f$ and $g$ such that \cref{eq:hashing_objective} is minimized.
The use of two different functions $f$ and $g$ allows for different mappings to be learned for different types of data; we expect this to be beneficial for our problem setting because synthesized MIDI CQTs and real-world audio CQTs may have different characteristics.
This also allows us to experiment with multimodal data, i.e.\ cases where $x$ and $y$ come from different, and potentially non-comparable, feature spaces.
A visualization of this process is shown in \cref{fig:hashing_schematic}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/4-hashing_schematic.pdf}
  \caption[Hashing groups of subsequent feature vectors]{Diagram visualizing how subsequent groups of feature vectors are mapped to binary vectors.
Matching sequences of feature vectors which have been aligned in time are shown at the top and bottom.
Co-occuring groups of feature vectors, highlighted in light boxes, are passed through two learned nonlinear functions $f$ and $g$ (visualized as neural network diagrams).
The functions then output a single hash vector each, which are ideally close together in the embedded Hamming space because they represent co-occuring feature vectors from matched and aligned sequences.
For groups of feature vectors from non-matching sequences, the networks are trained to output binary vectors with a large Hamming distance.}
  \label{fig:hashing_schematic}
\end{figure}

After minimizing \cref{eq:hashing_objective} on a training set, sequences of feature vectors in either modality can then be mapped to downsampled sequences of hash vectors using $f$ and $g$.
Once mapped, we can perform DTW much more efficiently to compute a distance between hash sequences, where the Euclidean distance calculation inherent in DTW has been replaced by Hamming distance.
More specifically, we expect a speedup by a factor of up to $G^2 D$, because we have effectively downsampled both sequences by a factor of $G$ and have replaced the $D$-operation Euclidean distance with a Hamming distance calculation.
In practice, the resulting binary vectors can be represented efficiently as unsigned integers, whose Hamming distance can be computed with an exclusive-or operation followed by a call to the POPCNT SSE4.2 instruction \cite{intel2007programming}.
The exclusive-or compares bit-by-bit whether the entries of the binary vectors are equal, and the POPCNT implements a ``population count'' or ``hamming weight'' calculation, which simply counts the number of nonzero bits in the result.
This process is visualized in \cref{fig:popcnt}.
Ideally, utilizing DTW to compare sequences which have been mapped to this efficient representation will effectively recover matching and non-matching sequences.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/4-popcnt.pdf}
  \caption[Computing Hamming distance with XOR and POPCNT]{Computing the Hamming distance between two binary vectors via an exclusive-or followed by a POPCNT instruction.
The exclusive-or of the top two vectors produces the bottom vector, which is $1$ where the input vectors differ and $0$ where they match.
The result of the exclusive-or is passed to the POPCNT SSE4.2 instruction, which simply counts the number of non-zero bits in its input.
These two steps therefore can produce the distance between two binary vectors in two machine instructions.}
  \label{fig:popcnt}
\end{figure}

Given our training procedure, we now need to choose appropriate models for the hashing functions $f$ and $g$.
Neural networks are an attractive choice, because they can in principle be optimized with respect to unusual loss functions like the one defined in \cref{eq:hashing_objective}.
In addition, the suite of tricks discussed in \cref{sec:tricks} can help ensure effective results.

In \cite{masci2014multimodal}, dense feed-forward neural networks are used for the learnable functions $f$ and $g$.
We could potentially utilize this type of network for our hashing and downsampling procedure by concatenating input $x, y \in \mathbb{R}^{G \times D}$ into single-dimensional vectors in $\mathbb{R}^{GD}$.
However, we argue that convolutional networks (discussed in \cref{sec:convolutional_networks}) are a better choice for sequential data for the following reasons:
Sequential data has at least one ordered (temporal) dimension, and in most natural data, the strongest dependencies along the temporal axis are localized.
This is exactly the kind of structure convolutional networks are designed to efficiently exploit.
Furthermore, the pooling layers which are a common ingredient in convolutional networks provide a natural method of downsampling the sequences.
They also allow the network's output a given point in time to depend on a large ``receptive field'' of the input.
In our problem setting, this means that a given hash vector produced by $f$ or $g$ will be affected not only by the corresponding group of $G$ feature vectors but also on a window of feature vectors surrounding it.
For the above reasons, we utilize convolutional networks for the rest of this chapter.

\section{Experiment: MIDI to MSD Matching}
\label{sec:dhs_experiment}

To determine the effectiveness of this approach on the task of matching MIDI files to the MSD, we evaluated its effectiveness on a smaller surrogate problem.
As an overview, this experiment proceeded as follows:
First, we assembled a collection of known-correct MIDI-MSD matches.
We then aligned these pairs using the method described in \cref{sec:goldstandard} and used a subset of the matched and aligned pairs to train our hashing model.
Finally, we utilized the trained model to produce hash sequences for the entire MSD and a held-out subset of MIDI files from our collection of known-correct matches.
Our approach was then evaluated based on how accurately it was able to recover the correct match in the MSD by nearest-neighbor DTW search utilizing the downsampled hash sequences.
To explore the ability of our approach to handle multimodal data, we repeated this experiment using piano roll matrices to represent MIDI files rather than constant-Q spectrograms.
We also compared our technique to a simple baseline, to ensure that our learning-based approach is beneficial.

\subsection{Preparing Data}

For evaluation, we need a collection of ground-truth MIDI-audio pairs which are correctly matched.
To train our hashing model, we further require a collection of {\em aligned} MIDI and audio files, to supply the matching groups of feature vectors from each domain that will be used to train our model.
To obtain MIDI-audio pairs, we first separated a subset of MIDI files from our 178,561-file collection for which the directory name corresponded to the song's artist and the filename gave the song's title.
The resulting metadata needed additional canonicalization; for example, ``The Beatles'', ``Beatles, The'', ``Beatles'', and ``The Beatles John Paul Ringo George'' all appeared as artists.
To normalize these issues, we applied some manual text processing and resolved the artists and song titles against the Freebase \cite{bollacker2008freebase} and Echo Nest\footnote{\texttt{http://developer.echonest.com/docs/v4}} databases.
This resulted in a collection of 17,256 MIDI files for 10,325 unique songs, which we will refer to as the ``clean MIDI subset''.

We will leverage the clean MIDI subset in two ways: First, to obtain ground-truth pairings of MSD/MIDI matches, and second, to create training data for our hashing scheme.
The training data does not need to be restricted to the MSD, and using other sources to increase the training set size will likely improve our hashing performance, so we combined the MSD with three benchmark audio collections: CAL500 \cite{turnbull2007towards}, CAL10k \cite{tingle2010exploring}, and uspop2002 \cite{berenzweig2004large}.
To match these datasets to the clean MIDI subset, we used the Python search engine library \texttt{whoosh}\footnote{\texttt{https://github.com/dokipen/whoosh}} to perform a fuzzy matching of their metadata.
This resulted in 25,707 audio/MIDI file pairs corresponding to 5,249 unique songs.

Fuzzy metadata matching is not enough to ensure that we have MIDI and audio files with matching content; for instance, the metadata could be incorrect, the fuzzy text match could have failed, the MIDI could be a poor transcription (e.g., missing instruments or sections), and/or the MIDI and audio data could correspond to different versions of the song.
We therefore utilized the alignment technique developed in \cref{sec:goldstandard}, which was empirically shown to produce a confidence score which reliably reports whether the content of a MIDI file matches a given audio file.
Conveniently, utilizing an alignment system to verify matches also provides the aligned sequences we need for training our hashing system.
Based on the real-world results presented in \cref{sec:qualitative}, we decided to consider a match ``correct'' when its confidence score was above $0.5$; for the 500 matches annotated, this produces the correct label for all but 1 of the failed matches and for the majority of the correct matches.
Retaining all alignments with confidence scores higher than this threshold resulted in 10,094 successful matches.

Recall that these matched and aligned pairs serve two purposes: They provide training data for our hashing model; and we also use them to evaluate the entire content-based matching system.
For a fair evaluation, we exclude items used in training from the evaluation, thus we split the successful alignments into three parts: $60\%$ to use as training data, $5\%$ as a validation set used for rapid evaluation of models as they are being trained, $10\%$ as a ``development set'' to tune the content-based matching system, and the remaining $25\%$ to use for final evaluation of our system.
Care was taken to split based on \textit{songs}, rather than by entry (since some songs appear multiple times).

\subsection{System Specifics}
\label{sec:hashing_system}

Training the hashing model involves presenting training examples and backpropagating the gradient of \cref{eq:hashing_objective} through the model parameters; we therefore need to specify the data format, model architecture, and training process.
As a feature representation, we used the same one chosen in \cref{sec:goldstandard}, i.e.\ log-magnitude constant-Q spectrograms consisting of spectra with 12 bins per octave from MIDI note C2 (65.4 Hz) to B6 (987.8 Hz) computed every 46 milliseconds.
We utilized \texttt{pretty\char`_midi} \cite{raffel2014pretty_midi} to synthesize MIDI file and \texttt{librosa} \cite{mcfee2015librosa, mcfee2015librosa_scipy} to compute features.
Because utilizing a cosine distance for comparing spectra was consistently beneficial for alignment (see \cref{sec:optimizing}), we also $L^2$-normalized each feature vector.
In order to improve convergence, we also standardized the feature dimensions of all data utilizing statistics from the training set.

For efficiency, we trained the networks using minibatches of training examples; each minibatch consisted of 50 sequences obtained by choosing a random offset for each training sequence pair and cropping out the next 100 spectra.
For $\mathcal{N}$, we simply presented the network with length-100 subsequences chosen at random from different songs.
Each time the network had iterated over minibatches from the entire training set (one epoch), we repeated the random sampling process.
For optimization, we used RMSProp, a recently-proposed stochastic optimization technique \cite{tieleman2012lecture}.
After each 100 minibatches, we computed the loss over the validation set.
If the validation loss was less than $99\%$ of the previous lowest, we trained for 1000 more iterations (minibatches).

While the loss computed over the validation set is a reasonable indicator of network performance, its scale will vary depending on the $\alpha$ and $m$ regularization hyperparameters.
To obtain a more consistent metric, we also computed the distribution of distances between the hash vectors produced by the network for the pairs in $\mathcal{P}$ and those in $\mathcal{N}$.
We then used the Bhattacharya distance \cite{bhattacharyya1943measure} to compute the separation of these distributions to obtain a more direct measure of network performance.

In each modality, the hashing networks have the same architecture: A series of alternating convolutional and pooling layers followed by a series of fully-connected layers.
While the exact specification of this architecture will effect performance, the most impactful design choice for our problem setting is the extent to which the temporal axis is downsampled by pooling layers.
Following the common convention of pooling by 2, each pooling layer will result in sequence lengths being halved and the resulting DTW calculation being faster by up to a factor of 4.
However, too much downsampling may result in a loss of precision, particularly if the resulting sequences become extremely short.
We therefore chose to utilize a total of three pooling layers, which corresponds to downsampling by a factor of 8.
This resulted in the 30-second 7digital preview clips being mapped to length-80 hash sequences, which provided enough information for high precision while still achieving up to a factor of 64 speedup when computing DTW.

For the convolutional front-end of our networks, we experimented with a variety of architectures and chose the following:
Each of the three max-pooling layers were preceded by two convolutional layers, each with $3 \times 3$ filters (where the first dimension is time and the second is constant-Q frequency bin).
The use of a larger number of smaller convolutional filters is advocated by Simonyan and Zisserman \cite{simonyan2014very}, who argue that this structure improves the network's modeling capabilities without sacrificing receptive field size.
The convolutional layers in each of the three ``groups'' had 16, 32, and 64 filters, respectively.
We padded the input to each convolutional layer so that the output size was the same as the input size.
All of the max-pooling layers utilized $2 \times 2$ pooling with a stride of $2$.

After the convolutional layers, we utilized two dense layers with $2048$ units each, followed by the output layer.
The fully-connected layers were applied independently over the temporal axis to the output of the convolutional portion of the network.
In our networks, all layers except the output use rectifier nonlinearities; as in \cite{masci2014multimodal}, the output layer uses a hyperbolic tangent.
This choice in output nonlinearity allows us to obtain binary hash vectors by testing whether each output unit is greater or less than zero.
We chose 32 bits as the dimensionality of our embedded Hamming space, since 32 bit values are efficiently manipulated as unsigned integers.
For initial weight parameter values, as suggested in \cite{he2015delving}, we used normally-distributed random variables with a mean of zero and a standard deviation of $\sqrt{2/n_{in}}$, where $n_{in}$ is the number of inputs to each layer.
All bias values were initialized to $0$.
Our model was implemented using \texttt{theano} \cite{bastien2012theano,bergstra2010theano} and \texttt{lasagne} \cite{dieleman2015lasagne}.

For regularization, we found it benificial to add a squared $L^2$ penalty to the network's output prior to thresholding.
This effectively encourages the network to produce values near 0, which we believe improves convergence due to the fact that the gradient of the hyperbolic tangent nonlinearity vanishes away from 0.
This is in contrast to the intuition provided in \cite{masci2014multimodal} for using a hyperbolic tangent, where it is argued that saturating the nonlinearity will more closely simulate a binary embedding.
To control the amount of regularization, we multiplied this $L^2$ penalty by a scalar hyperparameter $\beta$ before adding it to \cref{eq:hashing_objective}.

To ensure good performance, we optimized all model hyperparameters using the Bayesian optimization framework proposed in \cite{snoek2012practical}.
We optimized over the convolutional/pooling layer architecture, the RMSProp learning rate and decay parameters, and the $\alpha$, $\beta$ and $m$ regularization parameters of the loss function and the output $L^2$ penalty.
As a hyperparameter optimization objective, we used the Bhattacharyya distance as described above.
The best performing network had the structure we described above, a learning rate of $0.001$ with an RMSProp decay parameter of $0.67$, $\alpha = 4.755$, and $m = 3$, and $\beta = 0.01$.
This hyperparameter configuration achieved a Bhattacharyya separation of $0.359$.

Beyond those described above, we also experimented with a number of other regularizers, model architectures, and optimization schemes.
For posterity, we report those here:
\begin{itemize}
\item We tried $2 \times 1$ pooling (i.e.\ not downsampling the frequency axis) as advocated in \cite{humphrey2012rethinking} but found it did not effect results despite being somewhat more computationally expensive.
\item We experimented with a larger $5 \times 12$ filter size in the first layer as used in \cite{raffel2015large} with single $3 \times 3$ convolutional in the second and third layers, but found this structure resulted in slightly worse performance despite having a similar receptive field.
\item We tested the use of dropout \cite{hinton2012improving} in the fully-connected layers, but it slightly decreased performance.
\item We tried adding a ``stress'' \cite{kruskal1964multidimensional} term to the loss function to encourage the networks' outputs to preserve the distances of their input, but found it hurt performance.
\item We also tried the hash entropy-encouraging loss term proposed in \cite[equation (3)]{yang2015supervised} but it did not significantly affect the resulting hash distribution.
\item In early experiments, we utilized fully-connected networks with no convolutional layers, but quickly achieved better results after including the convolutional front-end.
\item We experimented with a variety of architectures for the fully-connected portion of the network, including using 1, 2, or 3 hidden layers, with 256, 512, 1024, 2048, or 4096 layers; 2 layers with 2048 units was the largest configuration which could fit in memory and incidentally produced the best performance.
\item Apart from RMSProp, we also experimentied with Nesterov's accelerated gradient \cite{nesterov1983method} and Adam \cite{kingma2015adam} (see \cref{sec:sgd}) but found that RMSProp produced the best results.
\end{itemize}

\subsection{Adapting to Multimodal Data}
\label{sec:multimodal}

Our proposed system is structured to learn mappings from two different modalities thanks to the fact that two separate networks $f$ and $g$ are trained.
Utilizing the shared constant-Q spectrogram representation described in the previous section does not test the multimodal capabilities of our model because it allows MIDI files and audio recordings to be directly compared (as demonstrated in \cref{ch:dtw}).
We therefore also evaluated the use of a ``piano roll'' representation for the MIDI data.
A piano roll matrix $P$ is constructed such that $P[i, j] = v$, where $v$ is the total velocity of all notes on all instruments playing with pitch $i$ at the time corresponding to the $j^{th}$ feature vector.
This representation can be readily extracted from the MIDI file itself (i.e.\ without synthesis), and contains substantially less information than the CQT.
It is, however, still a ``time-frequency'' matrix, so it is not truly multimodal in the strictest sense.

For ease of comparison, we utilized the exact same system as outlined in \cref{sec:hashing_system} except that we replaced the constant-Q spectrograms of synthesized MIDI files with piano roll matrices, computed over the same timebase and range of semitones.
Specifically, after matching and aligning entries from the clean MIDI subset, we computed piano roll matries using the \texttt{get\char`_piano\char`_roll} method of the \texttt{pretty\char`_midi} library \cite{raffel2014pretty_midi}.
As with the constant-Q spectrograms, we also $L^2$ normalized each feature vector in the piano roll matrices and standardized the feature dimensions using statistics from the training set.
We then carried out a separate hyperparameter optimization, because we don't necessarily expect the same system design to perform best using both feature representations.
The best performance was attained with a learning rate of $0.00011$, an RMSProp decay parameter of $0.90$, $\alpha = 5.72$, and $m = 5$, and $\beta = 0.036$, producing a Bhattacharyya distance of $0.428$.

\subsection{Baseline}

In order to verify whether our hashing system is beneficial, we also evaluated a simple learning-free baseline method.
This approach is inspired by the ``piecewise aggregate approximation'' approach proposed in \cite{keogh2001dimensionality,yi2000fast}, which downsamples a sequence by computing the average of feature vectors within non-overlapping blocks of fixed length.
Given a constant-Q spectrogram, our baseline method first computes the averate spectrum over non-overlapping blocks of 8 spectra, which achieves the same downsampling ratio as our convolutional network models.
In order to produce binary vectors, we first computed the mean value in each feature dimension among all sequences in our training set, and then thresholded the downsampled sequences about this value.
More explicitly, given a sequence $X \in \mathbb{R}^{M \times D}$ which consists of $M$ $D$-dimensional feature vectors, we compute
\begin{equation}
\hat{X}[n, d] = \begin{dcases}
1, \frac{1}{G} \sum_{n = Gm}^{G(m + 1)} X[m, d] \ge \bar{X}[d]\\
0, \mathrm{otherwise}
\end{dcases}
\end{equation}
where $G$ is the downsampling factor ($8$ in our experiments), $\hat{X} \in \mathbb{H}^{\lfloor \frac{M}{G} \rfloor \times D}$ is the resulting downsampled hash sequence, and
\begin{equation}
\bar{X}[d] = \frac{1}{|\mathcal{S}|} \sum_{k = 1}^{|\mathcal{S}|} \sum_{m = 1}^{M_k} X_k[m, d]
\end{equation}
where $X_1, \ldots, X_{|\mathcal{S}|} \in \mathcal{S}$ is the training set of sequences with $X_k \in \mathbb{R}^{M_k \times D}$.
After thresholding our constant-Q spectrograms, this resulted in binary vectors in $\mathbb{H}^{48}$.
For more direct comparison, and to enable the use of exclusive-or and POPCNT to measure Hamming distance, we simply discarded the bottom and top eight entries in each of the resulting binary vectors so that they instead were embedded in $\mathbb{H}^{32}$.
Intuitively, the resulting downsampled hash sequences represent whether a given semitone had more or less energy than average over subsequent blocks of 8 frames (corresponding to 370 milliseconds).

\subsection{Results}

ismir2015large4

Add histogram plots.
Add cqt/cqt, dhs/dhs, distance matrices plot.
Add qualitative network behavior plots (deep dream).
Add percentage-below-rank plot.

\section{Discussion}
\label{sec:dhs_discussion}
