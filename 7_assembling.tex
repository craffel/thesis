\chapter{Assembling a Collection of MIDI Files Matched to the Million Song Dataset}
\label{ch:assembling}

Combining the results of \cref{ch:dtw,ch:dhs,ch:pse}, we now have all the ingredients required to efficiently match MIDI files to the Million Song Dataset.
Taken together, these three techniques present a cascade of matching methods, each of which achieves more precision at higher computational cost than the last.
This allows us to quickly discard the vast majority of the dataset before utilizing the expensive, but accurate, DTW operation of \cref{ch:dtw}.
While our main application in this thesis is MIDI-to-audio matching, all of these techniques are learning-based and data-adaptive, so we are optimistic they will be effective in other domains.

In this chapter, we finally utilize these techniques to match our collection of 178,561 MIDI files to the MSD.
In the following section, we discuss the practical implementation of this process, perform a small experiment to verify its performance, and give an overview of the results.
In \cref{sec:deriving}, we quantify the reliability of ground-truth annotations for content-based MIR extracted from this data and discuss further potential applications.
Finally, in \cref{sec:ideas} we wrap up with ideas for additional applications and improvements for our approach.

\section{Matching MIDI Files}

As an overview, our method for matching MIDI files to the MSD will proceed as follows:
Given a query MIDI file, we first synthesize it using \prettymidi{}'s \texttt{fluidsynth} method.
Then, we compute the log-magnitude, $L^2$-normalized, constant-Q spectrogram representation utilized throughout this thesis.
Using the pre-trained models from \cref{ch:dtw,ch:pse}, we compute the MIDI file's downsampled hash sequence and embedding.
We then compute the pairwise distance between the resulting embedding and all pre-computed embeddings of entries in the MSD, rank the entries according to their distance, and discard all but the top $N_{PSE}$.
We perform a similar pruning with the downsampled hash sequences, computing the DTW distance between the query and top $N_{PSE}$ MSD entries, ranking by distance, and discarding all but the top $N_{DHS}$.
Finally, we align the MIDI file to the remaining $N_{DHS}$ entries utilizing the system of \cref{sec:goldstandard}.

Before proceeding, we must first decide the pruning thresholds $N_{PSE}$ and $N_{DHS}$.
Choosing these thresholds directly trades off precision and computation, because setting larger thresholds will decrease the likelihood of discarding a valid match but will result in more comparisons being made downstream.
Based on the results of \cref{sec:ranking,sec:pse_results}, we chose $N_{PSE} = 100,000$ and $N_{DHS} = 250$.
This corresponds to pruning 90\% of the MSD in the first step, then pruning 99.75\% of the remaining entries, so that in the end the expensive DTW comparison is only performed on 0.025\% of the MSD.
According to our test-set results, this results in a false-reject rate of approximately 3.5\% for the embedded distance pruning and xxx\% for the downsampled hash sequences.
Assuming false rejects occur in each step independently, we can expect a combined false reject rate of up to xxx\%.

\subsection{Shortcuts}

Overall, we will follow the matching processes of \cref{sec:goldstandard,sec:hashing_system,sec:pse_specifics}, except for the following two changes made for efficiency:

First, we utilize an exact pruning method when comparing downsampled hash sequences.
Assuming we are computing the DTW distance as defined in \cref{sec:dtw} between two sequences, one of length $M$ and one of length $N$ with $M \le N$, then the normalized DTW distance will never be smaller than $T$ if there are not at least $M$ pairwise distances less than $T$.
Given this, while computing the pairwise distance matrix for downsampled hash sequences, we can (with a minor performance penalty) keep track of the number of entries which are smaller than $T$, where $T$ is set to the smallest normalized DTW distance which has been encountered so far.
If fewer than $M$ entries are less than $T$, we can skip computing the DTW alignment path because we can guarantee that the resulting normalized DTW distance will not be smaller than $T$.
In practice, this avoids the expensive operation of computing the DTW distance for many candidates whose distance matrices dictate that they cannot be the correct match.

Second, instead of utilizing the median pairwise distance for the CQT-to-CQT DTW additive penalty as suggested in \cref{sec:goldstandard}, we will use the mean distance.
This choice is largely made for performance reasons; computing the median of the $MN$ distances is at least $\mathcal{O}( NM\log(NM) )$ complexity whereas computing the mean is always $\mathcal{O}(NM)$.
We justify this decision based on the discussion in \cref{sec:optimizing} which suggests that small changes to the additive penalty will not affect alignment quality.
In practice, we found the mean and median pairwise distance to be extremely close, so this change likely has a negligible effect on the results aside from a small speed-up.

\subsection{Measuring Performance}

Prior to performing the matching of our full collection of MIDI files to the MSD, it is useful to get an idea of the expected accuracy of the process described above.
We therefore utilized this approach on the same evaluation task used in \cref{ch:dhs,ch:pse}, i.e.\ measuring the accuracy of matching our test set of known-correct MIDI-MSD pairs.
Specifically, we utilized the methods of \cref{ch:pse,ch:dhs} to prune all but 100,000 and 250 entries respectively and then computed the match confidence score for the remaining 250 entries using the method of \cref{sec:goldstandard}.
For evaluation, we then ranked the remaining 250 entries according to their resulting confidence scores.
If the correct match in the MSD was pruned incorrectly, we assigned a rank 1,000,000, because effectively this results in the correct matching being unrecoverable by ouer system.
For full details of this evaluation task, refer to \cref{sec:dhs_experiment}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/7-ranks.pdf}
  \caption[Percentage of the test set below a given rank]{Percentage of MIDI files in the test set whose correct match in the MSD ranked better than a given threshold, using the ``DHS'' method of \cref{ch:dhs}, ``PSE'' of \cref{ch:pse}, and their combination with the method of \cref{sec:goldstandard}.}
  \label{fig:combined_ranks}
\end{figure}

The proportion of pairs for which the correct entry matched above a given rank threshold is shown in \cref{fig:combined_ranks}.
For comparison, we also plot the curves obtained using the approaches of \cref{ch:dhs,ch:pse}.
Using the combined method, the correct match in the MSD had the highest confidence score 78.2\% of the time, and for 94.6\% of the test set the correct match ranked in the top 5.
Beyond this point, the performance is effectively constant - this is because for 94.7\% of the test set, the pruning steps discarded the correct match.
This is within our expected upper limit for the false reject rate of xxx\%.
That our high-precision approach only ranked the correct match first 78.2\% of the time is also expected, due to the fact that the MSD contains many duplicates and alternate versions of the songs \cite{bertin2012large}.

Of course, our approach's precision is only beneficial if it is also efficient.
On average, matching a MIDI file to the MSD using this cascade of techniques takes xxx.
Of this, on average xxx is spent pruning all but 100,000 entries using the embedding approach, xxx is spent pruning down to 250 entries using the downsampled hash sequence approach, and xxx is spent computing confidence scores for the remaining 250 entries.
The remaining xxx is the time required to compute all of the necessary representations for the MIDI file prior to matching.
Compared to the approximately 1.5 days it would take to match a single MIDI file to the MSD using the approach from \cref{sec:goldstandard} on its own, we consider the 5.4\% false reject rate a small price to pay for this increase in efficiency.
Of course, we could increase precision substantially by raising the pruning thresholds in exchange for a drop in efficiency, but we consider this to be a good trade-off.

\subsection{The MIDI Dataset}

How many matches?  Plot of proportion of min score per MIDI file.
Distribution format - original MIDI files, for when different audio is available and/or a better alignment scheme is available.

\section{Deriving Ground-Truth Information}
\label{sec:deriving}

Obvious: Train content-based MIR systems on derived ground-truth information.
We also have a very large of richly annotated MIDI files.
We can do things like look at transcription statistics for different genres; basically improve on the Serra paper.
We can also query by riff more accurately.
Use multiple MIDI transcriptions as different ground truth data views.
Learn to invert MIDI piano rolls.
Musical language model.

\subsection{Comparison to Hand-Annotated Data}
\label{sec:reliability}

In order to utilize many of these sources of information, we need to align, using the system of 3dtw.
How accurate is the result?
Test beats and key

\subsubsection{Key Experiment}

\subsubsection{Beat Experiment}

\subsubsection{Improving Alignment as a Proxy for Content-Based MIR}

Ground truth is already valuable; we can make it more reliable by making alignment better.

\section{Ideas and Improvements}
\label{sec:ideas}

Try improving DTW by using the same representation learning approach.
Try learning DTW end-to-end.
Better recurrent network learning for embedding.
Hierarchy of approaches.
Applying to other areas: Cover song ID.  Speaker ID.  Plagiarism detection.
