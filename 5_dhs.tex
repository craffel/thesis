\chapter{Learning an Efficient Representation for Dynamic Time Warping}
\label{ch:dhs}

The system developed in \cref{ch:dtw} produces a confidence score which proved to be extremely reliable at determining whether or not a MIDI file was a transcription of a given audio file.
The goal of this thesis is to match a large collection of 178,561 MIDI files to corresponding audio recordings.
We cannot rely on any source of metadata from these MIDI files (see \cref{sec:score_overview}), so we must instead utilize a {\em content-based} comparison scheme like the one proposed in \cref{ch:dtw}.
To maximize the chances of finding a matching audio recording for a given MIDI file, we need to use a large and comprehensive pool of audio files.
We use the 994,960 7digital preview clips \cite{schindler2012facilitating} corresponding to the Million Song Dataset (MSD) \cite{bertin2011million}, which consist of (typically) 30 second portions of recordings from the largest standard research corpus of popular music.
A complete search for matches could thus involve 994,960 alignments for each of our 178,561 MIDI files, resulting in 177,661,052,560 comparison.

The method proposed in \cref{ch:dtw} is much too inefficient to be used in this manner.
Under this approach, aligning a typical-length MIDI file to a single audio recording from the MSD takes on average about 132 milliseconds on an Intel Core i7-4930k processor when using an extremely efficient LLVM-compiled DTW routine and a parallelized distance matrix computation.
Matching our entire collection of MIDI files using this approach would therefore take approximately
\begin{equation}
\frac{(.132 \mathrm{\;seconds})(178{,}561 \mathrm{\;MIDI\;files})(994{,}960 \mathrm{\;audio\;files})}{(60 \mathrm{\;seconds})(60 \mathrm{\;minutes})(24 \mathrm{\;hours})(365 \mathrm{\;days})} \approx 745 \mathrm{\;years}
\end{equation}
Clearly, a more efficient approach is warranted.

In fact, the problem of finding the sequence in a large database (here, a collection of audio recordings) which is most similar under DTW distance to a query (here, a MIDI file) is common in the field of data mining \cite{berndt1994using}.
Unsurprisingly, a variety of methods have been proposed for speeding up this task to make it feasible for very large databases.
A pervasive framework is ``pruning'', where a computationally efficient operation is used to discard a large portion of the database and the remaining entries are compared using a more expensive, but highly accurate, method.
Under certain assumptions, there are methods which can achieve {\em exact} pruning (i.e.\ without ever erroneously discarding the correct match) of large-scale nearest-neighbor DTW search.
\cite{rakthanmanon2012searching} provides a thorough overview of these techniques and shows that for some problems they can allow exact search of databases with trillions of data points.
However, one of the core assumptions of these methods is that the query is always a subsequence of its correct match, and equivalently that the length of the aligned sequence is known a priori, which does not hold in the setting of MIDI-to-audio matching.
In fact, because the 7digital MP3s are truncated preview clips and MIDI files are usually complete transcriptions, for our problem we nearly always have the opposite case where the correct match is a subsequence of the query.
We therefore cannot leverage these pruning methods for our problem.

One of the reasons DTW-based search is so expensive is that it has $\mathcal{O}(MN)$ complexity, where $M$ and $N$ are the lengths of the two sequences being compared.
Clearly, then, DTW can be made quadratically more efficient by decreasing the number of elements in the sequences being compared.
For example, \cite{keogh2001dimensionality,yi2000fast} proposed downsampling sequences by computing their average over non-overlapping blocks, which for some tasks provided orders of magnitude speedup without dramatically affecting accuracy.
Similarly, \cite{salvador2007toward} propose an iterative procedure for making DTW more efficient which starts by finding the alignment path based on a low-resolution estimate of the sequences being compared, and then refines this estimate at higher and higher resolutions.
The appropriateness of downsampling sequences depends on the sequences being oversampled (i.e.\ having an unnecessarily high correlation between consecutive sequence elements).

Computing the DTW distance between two sequences also potentially involves computing the pairwise Euclidean distance between all of their elements.
In data mining tasks, entries in sequences are often one-dimensional, so the cost of this operation is usually not considered independent of the overall cost of DTW.
For the time-frequency representation we utilize in \cref{ch:dtw}, our sequences consist of 48-dimensional feature vectors.
As a result, in our problem setting computing the pairwise distance matrix is actually more expensive than finding the lowest-cost path through it because each entry in the distance matrix requires at least $D$ operations, where $D$ is the feature dimensionality.
Computing the distance matrix similarly receives quadratic speed gains when the sequences are downsampled, but also can be made faster by using a lower-dimensional or otherwise more efficient feature representation.

In addition, the ``raw'' representation in which data is initially provided may not produce the most effective measure of similarity under the Euclidian distance utilized by DTW.
More concretely, while the audio-to-synthesized MIDI CQT comparison utilized in \cref{ch:dtw} proved effective, it may be that we are able to transform the individual feature vectors to a space where similarity is better represented so that matching MIDI files and audio recordings have a smaller DTW distance.
Recently, the framework of ``representation learning'' has proven to be effective for this type of problem \cite{bengio2013representation}.
Representation learning utilizes prior knowledge, such as a collection of pairs of known-similar and dissimilar elements, to learn a mapping to a more effective transformed space.
This process is also capable of producing ``multimodal'' mappings, i.e.\ transformations which allow the comparison of heterogeneous data.

From the above discussion, it is clear that mapping very long sequences of high-dimensional feature vectors to shorter, lower-dimensional sequences such that similarity is preserved would provide substantial gains when comparing sequences with DTW.
Motivated by this possibility, in this chapter we propose a system with the following capabilities:
\begin{description}
\item[Maps to a Hamming space:] By replacing continuous-valued feature vectors with binary vectors in an embedded Hamming space, computing the distance between a pair of feature vectors simplifies to an exclusive-or followed by a single POPCNT SSE4.2 operation \cite{intel2007programming}, which substantially speeds up distance matrix calculation.
\item[Downsamples sequences:] Rather than creating a one-to-one correspondence between the original feature vectors and mapped binary vectors, groups of subsequent feature vectors are mapped to a single binary vector, giving a quadratic increase in efficiency.
\item[Preserves similarity:] The system is trained with an objective which seeks to produce a mapping where aligned feature vectors from matching sequences have a small Hamming distance in the embedded space, and non-matched feature vectors have a large distance.
\item[Learns its representation:] Our approach is entirely data-driven, which allows it to adapt to different problem settings including multimodal data, as we show in \cref{sec:multimodal}.
\end{description}

In the following section, we describe our model and training approach in detail.
In \cref{sec:dhs_experiment}, we test our system's accuracy on the task of matching MIDI files to the Million Song Dataset.
Finally, we discuss possibilities for improvement in \cref{sec:dhs_discussion}.

\section{Learning to Downsample and Hash}
\label{sec:hashing_model}

As a high-level overview, our system will learn a mapping from sequences of feature vectors to downsampled sequences of binary vectors in a Hamming space where similarity is preserved based on a training set of matched and aligned sequences.
This training set can be constructed by obtaining a collection of sequences in which matching pairs are known, and then using DTW to find the optimal alignment of feature vectors in matching sequences.
From this data, we extract groups of sequentially co-occurring feature vectors to construct $\mathcal{P}$, such that $(x, y) \in \mathcal{P}$ indicates that $x$ is a group of feature vectors which is aligned to the group $y$ from a matching sequence.
For example, in the context of MIDI to audio matching, $x$ will be subsequent constant-Q spectra of a synthesized MIDI which are aligned in time to $y$, consisting of co-occurring constant-Q spectra from a matching audio recording.
We then construct $\mathcal{N}$, a set of ``dissimilar'' pairs, by repeatedly randomly choosing two pairs $(x_1, y_1), (x_2, y_2) \in \mathcal{P}$ and swapping to obtain $(x_1, y_2), (x_2, y_1) \in \mathcal{N}$.

Given this training data, our goal is to map sequences of feature vectors to sequences of binary vectors in a Hamming space where groups of sequentially co-occurring vectors in $\mathcal{P}$ have a small distance and groups in $\mathcal{N}$ have a large distance.
Motivated by the multimodal hashing technique of \cite{masci2014multimodal}, we use the following objective function to measure the quality of the mapping:

\begin{equation}
\mathcal{L} = \frac{1}{|\mathcal{P}|} \sum_{(x, y) \in \mathcal{P}} \| f(x) - g(y) \|_2^2  + \frac{\alpha}{|\mathcal{N}|} \sum_{(a, b) \in \mathcal{N}} \max(0, m - \|f(a) - g(b) \|_2)^2
\label{eq:hashing_objective}
\end{equation}

where $f$ and $g$ are learned nonlinear functions, $\alpha$ is a parameter to control the importance of separating dissimilar items, and $m$ is a target separation of dissimilar pairs.
The first term in the loss function encourages the embedded distance between similar pairs to be small and the second term encourages dissimilar pairs to have a distance of at least $m$.
More specifically, if $x, y \in \mathbb{R}^{G \times D}$ where $G$ is the number of subsequent feature vectors which are grouped together (and therefore the downsampling ratio) and $D$ is the feature dimensionality, then $f(x), g(y) \in \mathbb{H}^E$ where $\mathbb{H}^E$ is the Hamming space of dimensionality $E$.
This system is then optimized over $\mathcal{P}$ and $\mathcal{N}$ to adjust the parameters of $f$ and $g$ such that \cref{eq:hashing_objective} is minimized.
The use of two different functions $f$ and $g$ allows for different mappings to be learned for different types of data; we expect this to be beneficial for our problem setting because synthesized MIDI CQTs and real-world audio CQTs may have different characteristics.
This also allows us to experiment with multimodal data, i.e.\ cases where $x$ and $y$ come from different, and potentially non-comparable, feature spaces.
A visualization of this process is shown in \cref{fig:hashing_schematic}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-hashing_schematic.pdf}
  \caption[Hashing groups of subsequent feature vectors]{Diagram visualizing how subsequent groups of feature vectors are mapped to binary vectors.
Matching sequences of feature vectors which have been aligned in time are shown at the top and bottom.
Co-occurring groups of feature vectors, highlighted in light boxes, are passed through two learned nonlinear functions $f$ and $g$ (visualized as neural network diagrams).
The functions then output a single hash vector each, which are ideally close together in the embedded Hamming space because they represent co-occurring feature vectors from matched and aligned sequences.
For groups of feature vectors from non-matching sequences, the networks are trained to output binary vectors with a large Hamming distance.}
  \label{fig:hashing_schematic}
\end{figure}

After minimizing \cref{eq:hashing_objective} on a training set, sequences of feature vectors in either modality can then be mapped to downsampled sequences of hash vectors using $f$ and $g$.
Once mapped, we can perform DTW much more efficiently to compute a distance between hash sequences, where the Euclidean distance calculation inherent in DTW has been replaced by Hamming distance.
More specifically, we expect a speedup by a factor of up to $G^2 D$, because we have effectively downsampled both sequences by a factor of $G$ and have replaced the $D$-operation Euclidean distance with a Hamming distance calculation.
In practice, the resulting binary vectors can be represented efficiently as unsigned integers, whose Hamming distance can be computed with an exclusive-or operation followed by a call to the POPCNT SSE4.2 instruction \cite{intel2007programming}.
The exclusive-or compares bit-by-bit whether the entries of the binary vectors are equal, and the POPCNT implements a ``population count'' or ``hamming weight'' calculation, which simply counts the number of nonzero bits in the result.
This process is visualized in \cref{fig:popcnt}.
Ideally, utilizing DTW to compare sequences which have been mapped to this efficient representation will effectively recover matching and non-matching sequences.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-popcnt.pdf}
  \caption[Computing Hamming distance with XOR and POPCNT]{Computing the Hamming distance between two binary vectors via an exclusive-or followed by a POPCNT instruction.
The exclusive-or of the top two vectors produces the bottom vector, which is $1$ where the input vectors differ and $0$ where they match.
The result of the exclusive-or is passed to the POPCNT SSE4.2 instruction, which simply counts the number of non-zero bits in its input.
These two steps therefore can produce the distance between two binary vectors in two machine instructions.}
  \label{fig:popcnt}
\end{figure}

Given our training procedure, we now need to choose appropriate models for the hashing functions $f$ and $g$.
Neural networks are an attractive choice, because they can in principle be optimized with respect to unusual loss functions like the one defined in \cref{eq:hashing_objective}.
In addition, the suite of tricks discussed in \cref{sec:tricks} can help ensure effective results.

In \cite{masci2014multimodal}, dense feed-forward neural networks are used for the learnable functions $f$ and $g$.
We could potentially utilize this type of network for our hashing and downsampling procedure by concatenating input $x, y \in \mathbb{R}^{G \times D}$ into single-dimensional vectors in $\mathbb{R}^{GD}$.
However, we argue that convolutional networks (discussed in \cref{sec:convolutional_networks}) are a better choice for sequential data for the following reasons:
Sequential data has at least one ordered (temporal) dimension, and in most natural data, the strongest dependencies along the temporal axis are localized.
This is exactly the kind of structure convolutional networks are designed to efficiently exploit.
Furthermore, the pooling layers which are a common ingredient in convolutional networks provide a natural method of downsampling the sequences.
They also allow the network's output a given point in time to depend on a large ``receptive field'' of the input.
In our problem setting, this means that a given hash vector produced by $f$ or $g$ will be affected not only by the corresponding group of $G$ feature vectors but also on a window of feature vectors surrounding it.
For the above reasons, we utilize convolutional networks for the rest of this chapter.

\section{Experiment: MIDI to MSD Matching}
\label{sec:dhs_experiment}

To determine the effectiveness of this approach on the task of matching MIDI files to the MSD, we evaluated its effectiveness on a smaller surrogate problem.
As an overview, this experiment proceeded as follows:
First, we assembled a collection of known-correct MIDI-MSD matches.
We then aligned these pairs using the method described in \cref{sec:goldstandard} and used a subset of the matched and aligned pairs to train our hashing model.
Finally, we utilized the trained model to produce hash sequences for the entire MSD and a held-out subset of MIDI files from our collection of known-correct matches.
Our approach was then evaluated based on how accurately it was able to recover the correct match in the MSD by nearest-neighbor DTW search utilizing the downsampled hash sequences.
To explore the ability of our approach to handle multimodal data, we repeated this experiment using piano roll matrices to represent MIDI files rather than constant-Q spectrograms.
We also compared our technique to a simple baseline, to ensure that our learning-based approach is beneficial.

\subsection{Preparing Data}
\label{sec:training_data}

For evaluation, we need a collection of ground-truth MIDI-audio pairs which are correctly matched.
To train our hashing model, we further require a collection of {\em aligned} MIDI and audio files, to supply the matching groups of feature vectors from each domain that will be used to train our model.
To obtain MIDI-audio pairs, we first separated a subset of MIDI files from our 178,561-file collection for which the directory name corresponded to the song's artist and the filename gave the song's title.
The resulting metadata needed additional canonicalization; for example, ``The Beatles'', ``Beatles, The'', ``Beatles'', and ``The Beatles John Paul Ringo George'' all appeared as artists.
To normalize these issues, we applied some manual text processing and resolved the artists and song titles against the Freebase \cite{bollacker2008freebase} and Echo Nest\footnote{\texttt{http://developer.echonest.com/docs/v4}} databases.
This resulted in a collection of 17,256 MIDI files for 10,325 unique songs, which we will refer to as the ``clean MIDI subset''.

We will leverage the clean MIDI subset in two ways: First, to obtain ground-truth pairings of MSD/MIDI matches, and second, to create training data for our hashing scheme.
The training data does not need to be restricted to the MSD, and using other sources to increase the training set size will likely improve our hashing performance, so we combined the MSD with three benchmark audio collections: CAL500 \cite{turnbull2007towards}, CAL10k \cite{tingle2010exploring}, and uspop2002 \cite{berenzweig2004large}.
To match these datasets to the clean MIDI subset, we used the Python search engine library \texttt{whoosh}\footnote{\texttt{https://github.com/dokipen/whoosh}} to perform a fuzzy matching of their metadata.
This resulted in 25,707 audio/MIDI file pairs corresponding to 5,249 unique songs.

Fuzzy metadata matching is not enough to ensure that we have MIDI and audio files with matching content; for instance, the metadata could be incorrect, the fuzzy text match could have failed, the MIDI could be a poor transcription (e.g., missing instruments or sections), and/or the MIDI and audio data could correspond to different versions of the song.
We therefore utilized the alignment technique developed in \cref{sec:goldstandard}, which was empirically shown to produce a confidence score which reliably reports whether the content of a MIDI file matches a given audio file.
Conveniently, utilizing an alignment system to verify matches also provides the aligned sequences we need for training our hashing system.
Based on the real-world results presented in \cref{sec:qualitative}, we decided to consider a match ``correct'' when its confidence score was above $0.5$; for the 500 alignments annotated in \cref{sec:qualitative}, this produced the correct label for all but 1 of the failed matches and for the majority of the correct matches.
Retaining all alignments with confidence scores higher than this threshold resulted in 10,094 successful matches.

For a fair evaluation, we exclude items used in training from the evaluation of our system, thus we split the successful alignments into three parts: $60\%$ to use as training data, $5\%$ as a validation set used for rapid evaluation of models as they are being trained, $10\%$ as a ``development set'' to tune the content-based matching system, and the remaining $25\%$ to use for final evaluation of our system.
Care was taken to split based on \textit{songs}, rather than by entry (since some songs appear multiple times).

\subsection{System Specifics}
\label{sec:hashing_system}

Training the hashing model involves presenting training examples and backpropagating the gradient of \cref{eq:hashing_objective} through the model parameters; we therefore need to specify the data format, model architecture, and training process.
As a feature representation, we used the same one chosen in \cref{sec:goldstandard}, i.e.\ log-magnitude constant-Q spectrograms consisting of spectra with 12 bins per octave from MIDI note C2 (65.4 Hz) to B6 (987.8 Hz) computed every 46.4 milliseconds.
We utilized \texttt{pretty\char`_midi} \cite{raffel2014pretty_midi} and \texttt{fluidsynth}\footnote{\texttt{http://fluidsynth.org}} to synthesize MIDI files and \texttt{librosa} \cite{mcfee2015librosa, mcfee2015librosa_scipy} to compute features.
Because utilizing a cosine distance for comparing spectra was consistently beneficial for alignment (see \cref{sec:optimizing}), we $L^2$-normalized each feature vector.
In order to improve convergence, we also standardized the feature dimensions of all data utilizing statistics from the training set.

For efficiency, we trained the networks using minibatches of training examples; each minibatch consisted of 50 sequences obtained by choosing a random offset for each training sequence pair and cropping out the next 100 spectra.
For $\mathcal{N}$, we simply presented the network with length-100 subsequences chosen at random from different songs.
Each time the network had iterated over minibatches from the entire training set (one epoch), we repeated the random sampling process.
For optimization, we used RMSProp \cite{tieleman2012lecture}, described in \cref{sec:rmsprop}.
After each 100 minibatches, we computed the loss over the validation set.
If the validation loss was less than $99\%$ of the previous lowest, we trained for 1000 more iterations (minibatches).

While the loss computed over the validation set is a reasonable indicator of network performance, its scale will vary depending on the $\alpha$ and $m$ regularization hyperparameters.
To obtain a more consistent metric, we also computed the distribution of distances between the hash vectors produced by the network for the pairs in $\mathcal{P}$ and those in $\mathcal{N}$.
We then used the Bhattacharya distance \cite{bhattacharyya1943measure} to compute the separation of these distributions to obtain a more direct measure of network performance.

In each modality, the hashing networks have the same architecture: A series of alternating convolutional and pooling layers followed by a series of fully-connected layers.
While the exact specification of this architecture will effect performance, the most impactful design choice for our problem setting is the extent to which the temporal axis is downsampled by pooling layers.
Following the common convention of pooling by 2, each pooling layer will result in sequence lengths being halved and the resulting DTW calculation being faster by up to a factor of 4.
However, too much downsampling may result in a loss of precision, particularly if the resulting sequences become extremely short.
We therefore chose to utilize a total of three pooling layers, which corresponds to downsampling by a factor of 8.
This resulted in the 30-second 7digital preview clips being mapped to length-80 hash sequences, which provided enough information for high precision while still achieving up to a factor of 64 speedup when computing DTW.

For the convolutional front-end of our networks, we experimented with a variety of architectures and chose the following:
Each of the three max-pooling layers were preceded by two convolutional layers, each with $3 \times 3$ filters (where the first dimension is time and the second is constant-Q frequency bin).
The use of a larger number of smaller convolutional filters is advocated by Simonyan and Zisserman \cite{simonyan2014very}, who argue that this structure improves the network's modeling capabilities without sacrificing receptive field size.
The convolutional layers in each of the three ``groups'' had 16, 32, and 64 filters, respectively.
We padded the input to each convolutional layer so that the output size was the same as the input size.
All of the max-pooling layers utilized $2 \times 2$ pooling with a stride of $2$.

After the convolutional layers, we utilized two dense layers with $2048$ units each, followed by the output layer.
The fully-connected layers were applied independently over the temporal axis to the output of the convolutional portion of the network.
In our networks, all layers except the output use rectifier nonlinearities; as in \cite{masci2014multimodal}, the output layer uses a hyperbolic tangent.
This choice in output nonlinearity allows us to obtain binary hash vectors by testing whether each output unit is greater or less than zero.
We chose 32 bits as the dimensionality of our embedded Hamming space, since 32 bit values are efficiently manipulated as unsigned integers.
For initial weight parameter values, as suggested in \cite{he2015delving}, we used normally-distributed random variables with a mean of zero and a standard deviation of $\sqrt{2/n_{in}}$, where $n_{in}$ is the number of inputs to each layer.
All bias values were initialized to $0$.
Our model was implemented using \texttt{theano} \cite{bastien2012theano,bergstra2010theano} and \texttt{lasagne} \cite{dieleman2015lasagne}.

For regularization, we found it beneficial to add a squared $L^2$ penalty to the network's output prior to thresholding.
This effectively encourages the network to produce values near 0, which we believe improves convergence due to the fact that the gradient of the hyperbolic tangent nonlinearity vanishes away from 0.
This is in contrast to the intuition provided in \cite{masci2014multimodal} for using a hyperbolic tangent, where it is argued that saturating the nonlinearity will more closely simulate a binary embedding.
To control the amount of regularization, we multiplied this $L^2$ penalty by a scalar hyperparameter $\beta$ before adding it to \cref{eq:hashing_objective}.

To ensure good performance, we optimized all model hyperparameters using the Bayesian optimization framework proposed in \cite{snoek2012practical}.
We optimized over the convolutional/pooling layer architecture, the RMSProp learning rate and decay parameters, and the $\alpha$, $\beta$ and $m$ regularization parameters of the loss function and the output $L^2$ penalty.
As a hyperparameter optimization objective, we used the Bhattacharyya distance as described above.
The best performing network had the structure we described above, a learning rate of $0.001$ with an RMSProp decay parameter of $0.67$, $\alpha = 4.76$, and $m = 3$, and $\beta = 0.01$.
This hyperparameter configuration achieved a Bhattacharyya separation of $0.359$.

Beyond those described above, we also experimented with a number of other regularizers, model architectures, and optimization schemes.
For posterity, we report those here:
\begin{itemize}
\item We tried $2 \times 1$ pooling (i.e.\ not downsampling the frequency axis) as advocated in \cite{humphrey2012rethinking} but found it did not effect results despite being somewhat more computationally expensive.
\item We experimented with a larger $5 \times 12$ filter size in the first layer as used in \cite{raffel2015large} with single $3 \times 3$ convolutional in the second and third layers, but found this structure resulted in slightly worse performance despite having a similar receptive field.
\item We tested the use of dropout \cite{hinton2012improving} in the fully-connected layers, but it slightly decreased performance.
\item We tried adding a ``stress'' \cite{kruskal1964multidimensional} term to the loss function to encourage the networks' outputs to preserve the distances of their input, but found it hurt performance.
\item We also tried the hash entropy-encouraging loss term proposed in \cite[equation (3)]{yang2015supervised} but it did not significantly affect the resulting hash distribution.
\item In early experiments, we utilized fully-connected networks with no convolutional layers, but quickly achieved better results after including the convolutional front-end.
\item We experimented with a variety of architectures for the fully-connected portion of the network, including using 1, 2, or 3 hidden layers, with 256, 512, 1024, 2048, or 4096 layers; 2 layers with 2048 units was the largest configuration which could fit in memory and incidentally produced the best performance.
\item Apart from RMSProp, we also experimentied with Nesterov's accelerated gradient \cite{nesterov1983method} and Adam \cite{kingma2015adam} (see \cref{sec:nesterov} and \cref{sec:adam}) but found that RMSProp produced the best results.
\end{itemize}

\subsection{Adapting to Multimodal Data}
\label{sec:multimodal}

Our proposed system is structured to learn mappings from two different modalities thanks to the fact that two separate networks $f$ and $g$ are trained.
Utilizing the shared constant-Q spectrogram representation described in the previous section does not test the multimodal capabilities of our model because it allows MIDI files and audio recordings to be directly compared (as demonstrated in \cref{ch:dtw}).
We therefore also evaluated the use of a ``piano roll'' representation for the MIDI data.
A piano roll matrix $P$ is constructed such that $P[i, j] = v$, where $v$ is the total velocity of all notes on all instruments playing with pitch $j$ at the time corresponding to the $i^{th}$ feature vector.
This representation can be readily extracted from the MIDI file itself (i.e.\ without synthesis), and contains substantially less information than the CQT.
It is, however, still a ``time-frequency'' matrix, so it is not truly multimodal in the strictest sense.

For ease of comparison, we utilized the exact same system as outlined in \cref{sec:hashing_system} except that we replaced the constant-Q spectrograms of synthesized MIDI files with piano roll matrices, computed over the same timebase and range of semitones.
Specifically, after matching and aligning entries from the clean MIDI subset, we computed piano roll matrices using the \texttt{get\char`_piano\char`_roll} method of the \texttt{pretty\char`_midi} library \cite{raffel2014pretty_midi}.
As with the constant-Q spectrograms, we also $L^2$ normalized each feature vector in the piano roll matrices and standardized the feature dimensions using statistics from the training set.
We then carried out a separate hyperparameter optimization, because we don't necessarily expect the same system design to perform best using both feature representations.
The best performance was attained with a learning rate of $0.00011$, an RMSProp decay parameter of $0.90$, $\alpha = 5.72$, and $m = 5$, and $\beta = 0.036$, producing a Bhattacharyya distance of $0.428$.

\subsection{Baseline}
\label{sec:baseline}

In order to verify whether our hashing system is beneficial, we also evaluated a simple learning-free baseline method.
This approach is inspired by the ``piecewise aggregate approximation'' approach proposed in \cite{keogh2001dimensionality,yi2000fast}, which downsamples a sequence by computing the average of feature vectors within non-overlapping blocks of fixed length.
Given a constant-Q spectrogram, our baseline method first computes the averate spectrum over non-overlapping blocks of 8 spectra, which achieves the same downsampling ratio as our convolutional network models.
In order to produce binary vectors, we first computed the mean value in each feature dimension among all sequences in our training set, and then thresholded the downsampled sequences about this value.
More explicitly, given a sequence $X \in \mathbb{R}^{M \times D}$ which consists of $M$ $D$-dimensional feature vectors, we compute
\begin{equation}
\hat{X}[n, d] = \begin{dcases}
1, \frac{1}{G} \sum_{n = Gm}^{G(m + 1)} X[m, d] \ge \bar{X}[d]\\
0, \mathrm{otherwise}
\end{dcases}
\end{equation}
where $G$ is the downsampling factor ($8$ in our experiments), $\hat{X} \in \mathbb{H}^{\lfloor \frac{M}{G} \rfloor \times D}$ is the resulting downsampled hash sequence, and
\begin{equation}
\bar{X}[d] = \frac{1}{|\mathcal{S}|} \sum_{k = 1}^{|\mathcal{S}|} \sum_{m = 1}^{M_k} X_k[m, d]
\end{equation}
where $X_1, \ldots, X_{|\mathcal{S}|} \in \mathcal{S}$ is the training set of sequences with $X_k \in \mathbb{R}^{M_k \times D}$.
After thresholding our constant-Q spectrograms, this resulted in binary vectors in $\mathbb{H}^{48}$.
For more direct comparison, and to enable the use of exclusive-or and POPCNT to measure Hamming distance, we simply discarded the bottom and top eight entries in each of the resulting binary vectors so that they instead were embedded in $\mathbb{H}^{32}$.
Intuitively, the resulting downsampled hash sequences represent whether a given semitone had more or less energy than average over subsequent blocks of 8 frames (corresponding to 371.5 milliseconds).
We will refer to this approach as ``thresholded piecewise aggregate approximation'' (TPAA).

\subsection{Matching MIDI Files to the MSD}
\label{sec:matching_process}

After training our hashing system as described above, the process of evaluating the effectiveness of matching MIDI files to the MSD proceeds as follows:
First, we precompute downsampled hash sequences for every 7digital preview clip and every MIDI file in the test and development sets for which we know a priori the correct match(es) in the MSD.
Then, we compute the DTW cost as described in \cref{sec:dtw} between every audio and MIDI hash sequence, except that the Euclidean distance of \cref{eq:dtw_objective} is replaced with Hamming distance.
We can evaluate the effectiveness of a given approach by ranking MSD entries according to their hash sequence DTW distance to a given MIDI file, and determining the rank of the correct match.

We tuned the parameters of the DTW cost calculation to optimize results over our ``development'' set of successfully aligned MIDI/MSD pairs.
We found it beneficial to use a smaller value of $g = 0.9$.
In \cref{ch:dtw}, we utilized the median pairwise distance for the ``non-diagonal move'' penalty for CQT-based distance matrices, but we found that this median was consistent across Hamming-distance matrices.
Using a fixed value for the additive non-diagonal move penalty $\phi$ avoids the median calculation, so we chose a fixed $\phi = 15$, the median distance we observed.
Similarly, we found that normalizing by the average distance value as defined in \cref{eq:dtw_mean_distance_normalization} did not help, so we skipped this step.

\subsection{Results}

\subsubsection{Qualitative}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-cqts_and_dhs.pdf}
  \caption[Spectrograms and downsampled hash sequences for a matching pair]{Example constant-Q spectrograms (top) and downsampled hash sequences (bottom) for a matched MSD entry (left) and MIDI transcription (right) of the song ``I Wanna Be Your Lover'' by Prince, which appeared in our development set.
The downsampled hash sequences were created using the trained model described in \cref{sec:hashing_system}.
As utilized in our experiments, the constant-Q spectrograms are log-magnitude and $L^2$-normalized.
For the spectrograms, the feature dimension corresponds to semitones; for the hash sequence, the Hamming space dimensions have no explicit meaning.
The spectrogram utilizes a timebase of 46.4 milliseconds per sequence step, which the hash sequence downsamples by a factor of 8 to 371.5 milliseconds.}
  \label{fig:cqts_and_dhs}
\end{figure}

Before diving into the empirical performance of our system, it is useful to get a qualitative idea of its behavior.
By way of example, we show our model's output for an example pair of matching sequences from our development set in \cref{fig:cqts_and_dhs}.
While the hash sequences are not directly interpretable, they do appear to match the overall structure of the constant-Q spectrograms.
The pairwise distance matrices and DTW path for these constant-Q spectrogram and downsampled hash sequence pairs can be seen in the first and second row of \cref{fig:distance_matrices} respectively.
Visualized in this manner, the hash sequence distance matrix clearly shows the structure of the song, including local repetitions as well as the two choruses at around 1/3 and 2/3 of the way through the song.
Furthermore, the hash sequence DTW operation recovered the same alignment path.
The distance matrix produced by the ``multimodal'' system, shown in the second row, has similar structure although the DTW alignment chose the first rather than second chorus.
At the bottom is the baseline TPAA distance matrix, which has less obvious structure than the other hash sequence representations.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-distance_matrices.pdf}
  \caption[Distance matrices utilizing different representations]{Distance matrices and DTW alignment paths (shown as dashed white lines) between the MSD entry and MIDI transcription shown in \cref{fig:cqts_and_dhs} utilizing different representations.
At the top is the standard synthesized MIDI CQT-to-audio CQT distance matrix, as utilized in \cref{ch:dtw}.
The second row shows the distance matrix for the downsampled hash sequences shown at the bottom of \cref{fig:cqts_and_dhs}.
Below that is the distance matrix for hash sequences created utilizing the multimodal system described in \cref{sec:multimodal}.
Finally, the bottom distance matrix was computed using the TPAA method defined in \cref{sec:baseline}.}
  \label{fig:distance_matrices}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-distributions.pdf}
  \caption[Distributions of matching and non-matching distances]{Distributions of distances for matching and non-matching sequence elements from our validation set for a variety of representations.
Real-valued (squared Euclidean) distance distributions for matching and non-matching pairs are shown in red and orange respectively; Hamming distance distributions are shown in blue and green.
From top left to bottom right: synthesized MIDI CQT-to-audio recording CQT, downsampled hash vectors using the method in \ref{sec:hashing_system}, MIDI piano roll-to-audio CQT, downsampled hash vectors using this ``multimodal'' data as described in \cref{sec:multimodal}, CQT-to-CQT hashing network output prior to thresholding, and the TPAA method described in \cref{sec:baseline}.}
  \label{fig:distributions}
\end{figure}

An intuitive surrogate for performance is the distribution of distances between matching and non-matching pairs.
Ideally, the distance between aligned feature vectors or embedded binary vectors in matched MIDI and audio files will be small, and will be large for non-matching pairs.
The extent to which the distributions of matching and non-matching distances overlap is measured by our objective, the Bhattacharyya coefficient, but it is also useful to inspect the distributions themselves before and after undergoing downsampling and hashing for the various techniques we are evaluating.
These distributions, for the raw CQT-to-CQT, piano roll-to-CQT, and each of the Hamming-space representations proposed in sections \ref{sec:hashing_system}, \ref{sec:multimodal}, and \ref{sec:baseline}, are shown in \cref{fig:distributions}.

The overlap between the matching and non-matching distributions of CQT-to-CQT distances in the top left shows that there is certainly some room for improvement in this representation.
Below this, the distributions for the piano roll-to-CQT distances show that comparing these two representations is substantially less effective, although there is some trend towards smaller distances for matching feature vectors.
At the bottom left, the distributions for raw (pre-thresholding) distances are shown.
This plot gives a clear picture that representation learning is helpful, because there is virtually no overlap between matching and non-matching distances, and furthermore the Euclidean distance in the embedded space for matching sequence elements is generally very small.
The effect of thresholding this representation is shown in the top right; thresholding clearly limits some of the discriminative power of the representation, but nevertheless there is much less overlap in the distributions than there was for the ``raw'' CQT feature representation.
Below, the Hamming distance distributions are shown for the model which learned to embed piano roll vectors; the separation of the distributions almost matches the CQT-to-CQT-based model, showing that our system is adaptable to differing input representations.
Finally, in the bottom right, we show the distributions for our baseline TPAA method.
Clearly, there is substantially move overlap between matching and non-matching embeddings in this representation.

\subsubsection{Ranking Metrics}

To evaluate our system using the known MIDI-audio pairs of our test set, we ranked MSD entries according to their hash sequence DTW distance to a given MIDI file, and determined the rank of the correct match.
Because some MIDI files were matched to multiple MSD entries due to duplicates in the MSD, we made the forgiving choice of utilizing the best rank among all correct matches for a given MIDI file.
We also ignored all resulting hash sequences with less than 30 sequence elements; these generally corresponded to corrupt MP3 files and occasionally confounded the results.
The mean reciprocal rank (MRR) across the test set for the three methods we evaluated is shown in the first row of \cref{tab:ranking_metrics}.
This metric makes it clear that our learning-based system outperforms our baseline, producing a MRR of $0.722$ compared to $0.270$.
We also observe a modest, but significant, drop in performance to an MRR of $0.580$ when using piano rolls instead of constant-Q spectrograms of MIDI syntheses.

\begin{table}
  \begin{center}
    \begin{tabular}{l l l l}
      \toprule
      Metric & \textbf{DHS, CQT} & \textbf{DHS, Piano roll} & \textbf{TPAA} \\
      \midrule
      Mean Reciprocal Rank & 0.722 & 0.580 & 0.270 \\
      95\% Rank Threshold & 167 & 518 & 25,944 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Metrics measuring the performance of the different approaches we tried for creating downsampled hash sequences.
The first row shows the mean reciprocal rank of the correct match across all entries in our test set.
The second shows the threshold below which 95\% had a smaller rank.}
  \label{tab:ranking_metrics}
\end{table}

While the MRR provides a compact measure of performance, we believe a more intuitive empirical measure is the proportion of MIDI files in the test set whose correct match ranked under a certain threshold.
We plot this proportion against the rank threshold in \cref{fig:ranks}.
A high-performance metric will yield a curve which approaches 100\% very quickly, meaning that the majority of the correct matches in the test set were ranked highly.
From a high level, then, we can see that our proposed method substantially outperforms the baseline, due to the fact a much larger proportion of the test set had high ranks.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/5-ranks.pdf}
  \caption[Percentage of the test set below a given rank]{Percentage MIDI files in the test set whose correct match in the MSD fell below a given rank, using each of the methods we evaluated.}
  \label{fig:ranks}
\end{figure}

Nevertheless, using our learning-based approach for creating downsampled hash sequences resulted in only about 64\% of the matches in our test set having the smallest DTW distance (i.e.\ having rank 1).
This implies that we can't rely on the correct entry appearing as the top match among all the MSD tracks in practice.
Some degree of inaccuracy can be attributed to the fact that our hashing model is not perfect (as shown in \cref{fig:distributions}) and that the MSD is very large, containing many possible decoys.
In a relatively small proportion of cases, the MIDI hash sequence ended up being very similar to many MSD hash sequences, pushing down the rank of the correct entry.

Given that we cannot reliably assume the top hit is the correct MSD entry, it is more realistic to look at our system as a pruning technique; that is, it can be used to discard MSD entries which we can be reasonably confident do not match a given MIDI file.
We therefore propose that a useful metric is to find the rank threshold below which we can be highly confident that the correct match appears, which we report for our different methods in the second row of \cref{tab:ranking_metrics}.
Specifically, we computed the threshold $N$ at which 95\% of the correct matches in the test set had a rank better than $N$.
Given this high-confidence rank threshold $N$, we can compute the hash sequence DTW distance between a query MIDI and all MSD entries, discard all entries in the MSD except those with the $N$-smallest distances, and then perform the more precise and more expensive method of \cref{ch:dtw} to find the best match in the remaining candidates.
Inspecting the resulting values of $N$ for the methods we evaluated, we see that we can discard the vast majority (all but 167 entries, or over 99.98\%) of the MSD with high confidence using the CQT hashing technique of \cref{sec:hashing_system}.
Once again, utilizing piano roll matrices incurs a modest decrease in precision, and the baseline TPAA method has substantially worse performance.

\section{Discussion}
\label{sec:dhs_discussion}

Pruning methods are valuable only when they are substantially faster than performing the original computation; fortunately, our approach is orders of magnitude faster:
On an same Intel Core i7-4930k processor, for the median hash sequence lengths, calculating a Hamming distance matrix between downsampled hash sequences is about xxx times faster than computing the CQT cosine distance matrix (xxx microseconds vs. xxx milliseconds) and computing the DTW score is about xxx times faster (xxx microseconds vs. xx microseconds).
These speedups can be attributed to the fact that computing an exclusive-or and a POPCNT is much more efficient than computing the cosine distance between two vectors and that, thanks to downsampling, our hash-based distance matrices have $\frac{1}{64}$ of the entries of the CQT-based ones.
In summary, a straightforward way to describe the success of our system is to observe that we can, with high confidence, discard over xxx\% of the entries of the MSD by performing a calculation that takes about as much time as matching against xxx\% of the MSD.
In addition, our learning-based approach provides a huge improvement over simply thresholding the raw feature values.

We identify a variety of potential avenues for improving our approach.
First, our simple method of randomly swapping matching pairs to construct our dissimilar pair set $\mathcal{N}$ may not be the most effective approach.
Preferentially choosing pairs on which our model is having difficulty, as done in \cite{schroff2015facenet}, or using a curriculum learning approach which presents more and more difficult examples to the network over time \cite{bengio2009curriculum} could prove beneficial.
Furthermore, we are interested in trying the various other loss functions which have been proposed for similarity-preserving embedding, such as the ``coscos'' loss from \cite{kamper2016deep} or the $k$-means style loss from \cite{hershey2016deep}.

Despite our system's efficiency, we estimate that comparing each of the MIDI files in our 178,561 file collection to the entire the MSD would still take xxx.
There is therefore room for improving the efficiency of our technique.
The main bottleneck is that it still requires DTW to compare hash sequences; if we could avoid this quadratic-complexity operation we could potentially develop an even faster pruning method.
We explore this possibility in the following chapter.
