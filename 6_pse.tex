\chapter{Pruning Subsequence Search by Embedding Sequences}
\label{ch:pse}

The DTW-based approaches for computing sequence similarity we have discussed so far have quadratic complexity in the sequence length because they involve multiple pairwise operations between each element in the sequences being compared.
While the downsampling and binary embedding approach proposed in \cref{ch:dhs} can make DTW much more efficient by lowering the number of sequence elements and making the pairwise comparisons faster, it is nevertheless still quadratic in complexity.
When datapoints can be represented as single fixed-length vectors, rather than as sequences of vectors, comparing two datapoints is effectively linear-time in the vector dimensionality.
This makes the task of matching a query to its most similar entry in a database extremely efficient.
An obvious question, then, is whether we can avoid the operation of comparing sequences for the majority of the database by comparing fixed-length vector surrogates for the sequences whose distance approximates their similarity.

Motivated by this question, we propose a learning-based system for mapping variable-length sequences of feature vectors to a fixed-length embedded space where sequence similarity is approximated by Euclidean distance.
This allows us to pre-compute embeddings for all entries in a large database of sequences, and rapidly estimate similarity by embedding a query sequence and computing its Euclidean distance to all of the database embeddings.
As a result, this operation does not become less efficient for longer or higher-dimensional sequences, i.e.\ it is effectively invariant to sequence length and feature vector dimensionality.
Our approach utilizes a novel attention-based neural network model which can adapt to any problem setting according to the training data provided.

In the next section we discuss embedding and attention-based neural networks, which we use to motivate our proposed model.
In \cref{sec:pse_experiment}, we evaluate the effectiveness of our approach on our exemplary task of matching MIDI files to entries in the Million Song Dataset.
Finally, we discuss results and possibilities for improvement in \cref{sec:pse_discussion}.

\section{Embedding Sequences with Attention}
\label{sec:pse_model}

\subsection{Embedding}

The idea behind ``embedding'' is attractive: Given data in a representation which is either inefficient or does not readily reveal factors of interest, can we produce a mapping to fixed-length vectors that possess these desired properties?
Because many simple machine learning methods are particularly effective given data where Euclidean distance corresponds to some notion of similarity, embedding can make data more amenable to learning and comparison.

As a concrete example, in \cite{schroff2015facenet}, a neural network architecture is used to embed images of faces such that images of the same person have small pairwise distances in the embedded space and images of different people have large distances.
The authors were able to achieve state-of-the-art results on the ``labeled faces the wild'' dataset by simply thresholding the resulting distances to denote ``same person'' or ``different person''.
In addition, the embedded representation produced qualitatively useful results when performing unsupervised clustering using $k$-means.
Alternatively, \cite{yang2015supervised} proposed a supervised method for learning Hamming-space embeddings for images which reflect salient characteristics of the images being embedded.
This allows for rapid ``semantic'' nearest neighbor queries to be made over large image databases.

For text, a striking application is word2vec, which is a family of models for mapping words to a Euclidean space with desirable properties \cite{mikolov2013distributed}.
For example, the difference between ``China'' and ``Beijing'' in the embedded space is roughly equivalent to the difference between ``Rome'' and ``Italy''.
These embeddings are typically learned in a unsupervised manner based on a word's context.

Most relevant to this thesis, embedding has been applied to the task of large-scale DTW search: \cite{papapetrou2011embedding} proposes a technique which maps variable-length sequences of feature vectors to a fixed-length embedded space where DTW distance is approximated by Euclidean distance.
The embedding is constructed by pre-computing the DTW distance between each sequence in the database and a small collection of ``reference'' sequences that are chosen to optimize retrieval accuracy.
Then, to match a query sequence to the database, its DTW distance is computed against the reference sequences, and the resulting vector of DTW distances is matched against the database of embeddings using a standard Euclidean distance-based nearest-neighbor search.
The resulting algorithm is approximate, but provided state-of-the-art results both in terms of accuracy and speed.
This technique relies on the assumption that the query is a subsequence of its correct match, and the training procedure involves repeatedly aligning a training set of sequences to the database.
These restrictions make it invalid in some cases, such as our MIDI-to-MSD matching problem where the correct database entry may be a subsequence of a given query and where it is computationally unreasonable to repeatedly align sequences to the entire database in the training process.

The idea of embedding has also been utilized in other sequential data problems.
For fixed-length sequences, the same type of feed-forward model used for fixed-size input can be used.
For example, \cite{kamper2016deep,bengio2014word} learn to embed spoken utterances of a fixed length in a Euclidean space where utterances of the same word are close together.

For variable-length sequences, it is common to use a model with some form of temporal awareness.
A natural example are recurrent networks (see \cref{sec:recurrent_networks}) whose hidden state can be treated as a learned embedding of the input sequence up to a given point in time.
Notably, in \cite{sutskever2014sequence}, a recurrent network was trained to map a sequence of words in a source language to a fixed-length vector, which was then used to generate a sequence of words in a target language, resulting in a quality translation of the original sentence.
The resulting source-sentence embeddings encoded high-level linguistic characteristics, such as invariance to word order in sentences with the same meaning.
In the audio domain, \cite{hershey2016deep} propose training a recurrent network to embed time-frequency ``pixels'' in spectrograms of acoustic mixtures so that sources can be separated using clustering techniques.

\subsection{Attention}

While recurrent networks have proven to be an effective method for embedding variable-length sequences, they have difficulty in modeling long-term dependencies when trained with backpropagation through time \cite{bengio1994learning,hochreiter1997long,pascanu2013difficulty} (see \cref{sec:recurrent_networks} for a discussion).
In the embedding setting, this may result in the end of the sequence having a greater impact on the embedding than the beginning.
A recent technique for mitigating this issue has been dubbed ``attention'' \cite{bahdanau2014neural}.
Conceptually, an attention-based model includes a mechanism which learns to assign a weight to each sequence step based on the current and previous states.
When used with recurrent networks, the addition of attention has proven effective in a wide variety of domains, including machine translation \citep{bahdanau2014neural}, image captioning \citep{xu2015show}, and speech recognition \cite{chan2015listen,bahdanau2015end}; see \cite{cho2015describing} for an overview.
Attention can also be seen as analogous to the ``soft addressing'' mechanisms of the recently proposed Neural Turing Machine \cite{graves2014neural} and End-To-End Memory Network \cite{sukhbaatar2015end} models.

More specifically, following the definition from \cite{bahdanau2014neural}, given a length-$T$ sequence $h$, attention-based models compute a series of ``context'' vectors $c_t$ as the weighted mean of the sequence $h$ by
\begin{equation}
c_t = \sum_{j = 1}^T \omega_t[j] h[j]
\end{equation}
where $\omega_t \in \mathbb{R}^T$ is a vector of weights computed at each time step $t$, with an entry for each sequence step in $h$.
These context vectors are then used to compute a new sequence $s$, where $s_t$ depends on $s_{t - 1}$, $c_t$ and the model's output at $t - 1$.
The weightings $\omega_t[j]$ are computed by
\begin{align}
e_t[j] &= a(s_{t - 1}, h[j])\\
\omega_t[j] &= \frac{\exp(e_t[j])}{\sum_{k = 1}^T \exp(e_t[k])}
\end{align}
where $a$ is a learned function which can be thought of as computing a scalar importance value for $h[j]$ given the value of $h[j]$ and the previous state $s_{t - 1}$.
This formulation allows the new sequence $s$ to have more direct access to the entire sequence $h$.

\subsection{Feed-Forward Attention}
\label{sec:ffattention}

While previous work on attention-based models has focused on recurrent networks, in the present work we will use feed-forward networks for the following reasons:
First, even with attention, recurrent networks have difficulty modeling very long-term dependencies.
The constant-Q spectrogram representation we have been using can result in sequences with thousands of timesteps, which is prohibitively long even for sophisticated models such as long short-term memory networks \cite{hochreiter1997long}.
Second, feed-forward networks are much more efficient to train and evaluate than recurrent networks because their operations can be completely parallelized, whereas recurrent networks must evaluate each time step sequentially.
Finally, attention provides its own form of temporal modeling because it integrates over the entire sequence.

We therefore propose a ``feed-forward'' variant of attention, which computes a weighting for each sequence step independent of all other steps as follows:
\begin{align}
e[t] &= a(h[t])\\
\omega[t] &= \frac{\exp(e[t])}{\sum_{k = 1}^T \exp(e[k])}\\
\label{eq:ffattention}
c &= \sum_{t = 1}^T \omega[t] h[t]
\end{align}
Conceptually, this attention mechanism can be thought of as using the learned nonlinear function $a$ to compute scalar ``importance'' values for each sequence step in $h$, which are then normalized using the softmax function and used to compute a weighted mean $c$ over all sequence elements in $h$.
Clearly, this mechanism will produce a single fixed-length vector regardless of the length of the sequence, which is what is necessary in sequence embedding.
A schematic of our feed-forward attention mechanism is shown in \cref{fig:attention_schematic}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-attention_schematic.pdf}
  \caption[Feed-forward attention mechanism]{Diagram visualizing our feed-forward attention mechanism.
Given a length-$T$ input sequence $h$, the learned nonlinear function $a$ computes a weighting $\omega[t]$ for each sequence element $h[t]$.
The resulting probability vector $\omega \in \mathbb{R}^T$ is used to compute a weighted average $c$ of $h$.}
  \label{fig:attention_schematic}
\end{figure}

The main difference between this formulation and the one proposed in \cite{bahdanau2014neural, cho2015describing} (stated in the previous section) is that here we are only producing a single weighting $\omega$ and context vector $c$ rather than a different $\omega_t$ and context vector $c_t$ at each time step.
This is primarily because our goal in embedding is to produce a single vector for the entire sequence whereas previous applications of attention have mainly focused on sequence-to-sequence transduction.
As a result, \cref{eq:ffattention} does not contain any terms for a previous attention vector or a previous hidden state, because in the present contexts these quantities do not exist.
The main disadvantage to using attention in this way is that it ignores temporal order by computing an average over the entire sequence.
However, computing an order-agnostic integration of sequence has in fact been shown to be highly effective in the domain of music information retrieval.
A common technique is to summarize a sequence of feature vectors by the mean, variance, and occasionally covariance of feature dimensions, which has been used for genre classification \cite{tzanetakis2002musical}, instrument classification \cite{deng2008study}, artist recognition \cite{mandel2005song}, and similarity rating prediction \cite{foster2014sequential}.

\section{Experiment: MIDI to MSD Matching}
\label{sec:pse_experiment}

Recall that the goal of this chapter is to prune large-scale sequence similarity searches by embedding sequences in a fixed-dimensional space where Euclidean distance approximates sequence similarity.
Towards this end, we will utilize embeddings as follows:
First, given a training set of matching sequences, we will train a feed-forward network with attention to embed sequences in a Euclidean space where matching pairs have a small distance and non-matching pairs have a large distance.
Then, we will utilize this network to pre-compute embeddings for all sequences in a database.
In order to match a query sequence, we will compute the distance between its embedding and all pre-computed embeddings from the database.
Before proceeding, then, we need to specify the structure of our embedding model, our loss function, and our evaluation technique.
In fact, this problem setting is remarkably similar to the previous chapter, and we will evaluate it on the same task and data.
The primary difference is that rather than embedding groups of aligned feature vectors in a shared Hamming space, we are embedding entire unaligned sequences as fixed-length vectors.

For our loss function, we can actually re-use \cref{eq:hashing_objective} exactly, which we re-print here for convenience:
\begin{equation}
\mathcal{L} = \frac{1}{|\mathcal{P}|} \sum_{(x, y) \in \mathcal{P}} \| f(x) - g(y) \|_2^2  + \frac{\alpha}{|\mathcal{N}|} \sum_{(a, b) \in \mathcal{N}} \max(0, m - \|f(a) - g(b) \|_2)^2
\label{eq:embedding_objective}
\end{equation}
where $\mathcal{P}$ and $\mathcal{N}$ are collections of matching and non-matching sequences respectively, $f$ and $g$ are learned functions, and $m$ and $\alpha$ are hyperparameters controlling the importance of non-matching pairs having a large embedded distance.
In the present context, the learned embedding functions $f$ and $g$ will produce a single vector, and their input will be entire unaligned sequences rather than short groups of aligned feature vectors.
In order to produce a single vector from an entire sequence, we will utilize the feed-forward attention mechanism described in the previous section.
Specifically, we will utilize two different embedding networks for $f$ and $g$, each of which will include a feed-forward attention mechanism.
For the attention mechanisms' learned adaptive weighting function $a$, we use the standard architecture of a small neural network with one hidden layer:
\begin{equation}
a(x) = v^\top\tanh(Wx + b)
\label{eq:weighting_function}
\end{equation}
where $x \in \mathbb{R}^D$ is a single feature vector from the input sequence and $W \in \mathbb{R}^{D \times D}$, $b \in \mathbb{R}^D$, and $v \in \mathbb{R}^D$ are parameters.

\subsection{Experiment Specifics}
\label{sec:pse_specifics}

As training data, we use the same collection assembled in \cref{sec:training_data}.
However, we will solely use the matches as ground-truth, {\em not} the alignments, because our system needs to be able to produce similarity-preserving embeddings regardless of whether the input sequences are aligned.
We use the same feature representation, i.e.\ log-magnitude constant-Q spectrograms consisting of spectra with 12 bins per octave from MIDI note C2 (65.4 Hz) to B6 (987.8 Hz) computed every 46.4 milliseconds.
We also use the same exact training, validation, development, and test splits, and utilize the statistics from the training split to standardize feature vectors.

For our networks, we fed the sequences first into a convolutional layer with 32 $3 \times 3$ filters followed by a $2 \times 2$ max-pooling layer, followed by the attention mechanism of \cref{eq:ffattention} with the weighting function of \cref{eq:weighting_function}, followed by two fully-connected layers with 2048 units each, followed by a 128-dimensional output layer.
We utilized rectified linear nonlinearities throughout, except on the final output layer and the weighting function which both used a hyperbolic tangent nonlinearity.
All weights were initialized using He et.\ al.'s method \cite{he2015delving} and all biases were initialized to zero.
The networks were optimized with respect to \cref{eq:embedding_objective} using RMSProp with mini-batches of 20 similar and dissimilar pairs.
As in \cref{ch:dhs}, we assembled $\mathcal{N}$ by randomly choosing non-matching pairs which were re-sampled every time we had iterated over the training set.
Minibatches were constructed by randomly choosing matching and non-matching pairs of sequences and cropping them to the minimum length within the minibatch.
Every 200 minibatches, we computed the mean loss on the held-out validation set; when it was less than .99 times the previous lowest validation loss, we trained for 1000 more minibatches.
We implemented our model using \texttt{lasagne} \cite{dieleman2015lasagne}, which is built on \texttt{theano} \cite{bergstra2010theano,bastien2012theano,al-rfou2016theano}.
A visualization of our model structure is shown in \cref{fig:model_schematic}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-model_schematic.pdf}
  \caption[Structure of our feed-forward attention network]{Visualization of the structure of our feed-forward network with attention.
The network first processed the input with a convolutional and pooling layer, followed by our proposed feed-forward attention mechanism which integrates the entire sequence to a single fixed-length vector, followed by a series of fully-connected layers which produce the embedding.}
  \label{fig:model_schematic}
\end{figure}

To choose hyperparameter settings, we once again used the Bayesian optimization framework proposed in \cite{snoek2012practical}.
We optimized the learning rate and RMSProp decay parameter, and the loss parameters $\alpha$ and $m$.
As an objective, we computed the Bhattacharyya coefficient \cite{bhattacharyya1943measure} between the distributions of embedded distances between matching and non-matching pairs in the validation set.
The best-performing network configuration achieved a Bhattacharyya coefficient of $0.393$ and used a learning rate of $5\cdot10^{-6}$, an RMSProp decay of $0.999$, $\alpha = 3.342$ and $m = 1.055$.

Before arriving at this system design, we also tried:
\begin{itemize}
\item Using more or fewer convolutional/pooling layers at the input of the network, including using no convolutional and pooling layers at all, a $5 \times 12$ convolutional layer followed by a variable number of $2 \times 2$ max-pooling layers and $3 \times 3$ convolutional layers, or using the same front-end as in \cref{ch:dhs}.
\item Using $2 \times 1$ pooling layers as in \cref{ch:dhs} and \cite{humphrey2012rethinking}; we found $2 \times 2$ max-pooling worked better for embedding networks.
\item Using up to four ``parallel'' attention mechanisms, whose output was concatenated before being processed by the fully-connected layer; this produced no performance gains.
\item The output $L^2$ penalty from \cref{ch:dhs} as well as the hash entropy-encouraging loss term proposed in \cite[equation (3)]{yang2015supervised}, but neither were beneficial.
\item Using dropout in the fully-connected layers, which hurt performance.
\item Using more or fewer fully-connected layers of different sizes; two $2048$-unit layers produced the best results.
\item For optimization, Nesterov's accelerated gradient \cite{nesterov1983method} and Adam \cite{kingma2015adam} (see \cref{sec:nesterov} and \cref{sec:adam}) but found that RMSProp was most effective.
\end{itemize}

\subsection{Baseline Method}
\label{sec:embedding_baseline}

As in \cref{ch:dhs}, we also evaluated a simple learning-free baseline method to make sure that our approach was beneficial.
For our baseline, we followed the tradition of \cite{tzanetakis2002musical, deng2008study, mandel2005song, foster2014sequential} and others by summarizing spectrograms by their mean and standard deviation across feature dimensions and concatenating the resulting statistics into a fixed-length embedding.
Specifically, given a sequence $X \in \mathbb{R}^{M \times D}$ which consists of $M$ $D$-dimensional feature vectors, we compute
\begin{align}
\mu_X[d] &= \frac{1}{M} \sum_{m = 1}^M X[m, d]\\
\sigma_X[d] &= \sqrt{\frac{1}{M} \sum_{m = 1}^M (X[m, d] - \mu_X[d])^2}\\
e &= \begin{bmatrix}
\mu_X\\
\sigma_X
\end{bmatrix}
\end{align}
where $e \in \mathbb{R}^{2D}$ is the resulting embedding.
We compute this statistics-based embedding over the same log-magnitude, $L^2$ normalized spectrograms used throughout this thesis.
This resulted in a 96-entry embedded vector for each spectrogram.

\subsection{Results}
\label{sec:pse_results}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-embeddings.pdf}
  \caption[Example embeddings for two pairs of sequences]{Example embeddings for two pairs of matching sequences produced by our feed-forward attention model after training.
The first two rows display pairs of matching constant-Q spectrograms, with synthesized MIDI on the left and MSD entry on the right.
For both pairs, the right sequence matches a subsequence of the left.
The last two rows show their resulting embeddings.
Dashed arrows denote which embedding corresponds to which sequence.}
  \label{fig:embeddings}
\end{figure}

Some qualitative intuition about our system's behavior can be gained by inspecting the embeddings resulting from matching and non-matching pairs of sequences.
Embeddings for two different matching sequence pairs from the validation set can be seen in \cref{fig:embeddings}.
Among the four embeddings displayed, the pairs of matched embeddings resulted in squared Euclidean distances of $0.0189$ and $0.00738$, while non-matching pairs had distances of $0.300$ and $0.228$.
While these distances indicate the appropriate behavior, i.e.\ matching pairs result in a much smaller embedded distance, a more complete picture can be obtain by looking at the distributions of distances between matching and non-matching pairs in our validation set.
We visualize these distributions for our proposed feed-forward attention system and for the baseline feature statistics-based method in \cref{fig:embedding_distributions}.
From this figure, we can see that our proposed approach produces embeddings which better differentiate between matching and non-matching pairs.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-distributions.pdf}
  \caption[Distributions of matching and non-matching distances]{Distributions of embedded distances for matching (blue) and non-matching (green) sequences from our validation set.
At the top are the distances produced by our proposed feed-forward attention model after training; at the bottom are distances based on the baseline proposed in \cref{sec:embedding_baseline}.
Note that the x-axes are log-scaled.}
  \label{fig:embedding_distributions}
\end{figure}

Because our proposed feed-forward attention system is a novel architecture, we are interested in getting some insight into its learned behavior.
One way to accomplish this is to look at the attention weighting values $\omega$ over the course of an input sequence.
We visualize this for our MIDI spectrogram embedding network in \cref{fig:attention}.
One clear pattern from this visualization is that the attention mechanism tends to downweight the portion of the song with strong low-frequency transients (frames 350 to 800).
In addition, in one region of mostly high attention (frames 100 to 350) there are two regions where the attention drops; these regions also have a sudden burst of energy in a low-frequency bin.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-attention.pdf}
  \caption[Example attention weighting]{Example attention weighting values $\omega$ (bottom) produced by our MIDI embedding network for a synthesized MIDI spectrogram (top) from our validation set.
The raw attention values are shown in light green; because they are noisy, we also display a median filtered version (with a window size of 15 frames) in blue.}
  \label{fig:attention}
\end{figure}

Another generally applicable visualization technique is DeepDream \cite{mordvintsev2015inceptionism}, which refers to the technique of optimizing the {\em input} of a pre-trained model, rather than the parameters, in order to minimize an objective.
We utilized this technique to see what kind of inputs the attention mechanism in each of our networks weights very highly.
To achieve this, we optimized a randomly-initialized input to minimize the entropy of the attention weights $\omega$.
This effectively tells the network to generate an input for which a small region is given the maximum attention and the rest of the input is ignored.
As advocated in \cite{mordvintsev2015inceptionism}, we initialized with zero-mean, unit-variance Gaussian noise and utilized standard gradient descent with gradients normalized to have unit norm.
In addition, we enforced the constraint that the input feature vectors stay $L^2$-normalized.
We plot 40 frames of the resulting input surrounding the maximal-attention region for both of our networks in \cref{fig:attention_dream}.
Most apparently, there is a clear lack of high-frequency content in the high-attention regions.
In addition, the regions of low attention appear to be basically noise.
This suggests that our attention mechanism may be most interested in steady-state regions with little high-frequency content.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-attention_dream.pdf}
  \caption[Generated maximal attention inputs]{Inputs generated by minimizing the entropy of the resulting attention weights $\omega$ produced by our trained MIDI embedding (left) and audio embedding (right) networks.
Shown are 40 frames of each generated input, centered around the region of maximal attention.}
  \label{fig:attention_dream}
\end{figure}

For empirical evaluation, we follow the procedure of \cref{sec:matching_process} exactly, except that rather than comparing sequences using downsampled hash sequences, we will be embedding them and computing the distance between their embedded representations.
More specifically, we first computed embeddings of constant-Q spectrograms for all of the short (30 to 60 second) preview recordings of each entry in the MSD provided by 7digital \cite{schindler2012facilitating}.
Then, we computed a constant-Q spectrogram and its embedding for each MIDI file in the test set.
To determine performance, we computed the Euclidean distance between the embedding of each MIDI file and the embeddings of every entry in the MSD and determined how many entries had a distance that was smaller than the correct match.
We carried out this procedure both for our proposed attention-based embedding technique (referred to as ``PSE'', for \textbf{p}airwise \textbf{s}equence \textbf{e}mbedding) and for the feature statistics-based baseline approach described in \cref{sec:embedding_baseline} (referred to as ``statistics'').

We plot the resulting proportion of MIDI files in the test set whose correct match ranked under a certain threshold in \cref{fig:embedding_ranks}.
To borrow the evaluation from \cref{ch:dhs}, we also computed the threshold $N$ at which 95\% of the correct matches in the test set had a rank better than $N$.
For PSE, $N$ can be set to 70,445, while the baseline would require 242,573.
This means that we can used our proposed embedding method to prune over 90\% of the database with high confidence, whereas the simple learning-free baseline would only allow us to discard about 75\%.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/6-ranks.pdf}
  \caption[Percentage of the test set below a given rank]{Percentage MIDI files in the test set whose correct match in the MSD fell below a given rank, using the attention-based PSE method and our baseline statistics method described in \cref{sec:embedding_baseline}.}
  \label{fig:embedding_ranks}
\end{figure}

\section{Discussion}
\label{sec:pse_discussion}

Comparing to \cref{fig:ranks}, our embedding approach is clearly much less precise than any of the hash sequence-based methods.
This is to be expected due to the fact that we are representing sequences of potentially thousands of 48-dimensional vectors as a single 128-dimensional vector.
Fortunately, it is also much faster: Computing the million embedding distance calculations required to compare a MIDI file to the entire MSD only takes about 400 milliseconds (i.e.\ about 400 nanoseconds per MIDI-to-MSD entry comparison) on our Intel Core i7-4930k CPU.
This means that we can discard over 90\% of the MSD with high confidence with an operation which is over 1,000 times faster than the downsampled hash sequence-based methods of \cref{ch:dhs} and is over 600,000 times faster than a brute-force approach using the DTW system from \cref{ch:dtw}.

To improve performance of our proposed method we are interested in investigating the same ideas discussed in \cref{sec:dhs_discussion}, i.e.\ different loss functions and methods of sampling negative examples.
In addition, as more and more methods are developed for making recurrent networks able to handle very long-term dependencies (some of which are mentioned in \cref{sec:recurrent_networks}), we are interested in testing whether recurrence could become applicable to our problem.

In summary, we have proposed a novel feed-forward attention mechanism which is capable of integrating arbitrary-length sequences to individual vectors in a fixed-dimensional embedded space where sequence similarity is approximated as Euclidean distance.
We showed that using this model for pruning large-scale similarity search produces a system which can extremely efficiently rule out a large proportion of the database.
Utilizing this technique alongside the systems proposed in \cref{ch:dhs} and \cref{ch:dtw} provides a ``cascade'' of techniques where we are able to discard more and more of the database with more and more expensive operations until we obtain a high-confidence match.
In the following chapter, we put this approach to use on the task of identifying matches in the MSD for our entire 178,561 MIDI file collection.
